{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[],"collapsed_sections":["YeIEamAx1OQV","Qu7gqT0s2itO"],"authorship_tag":"ABX9TyOOBoMCgJorvpaqnmfUoR+K"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **download dataset**","metadata":{"id":"YeIEamAx1OQV"}},{"cell_type":"code","source":"!kaggle datasets download bryanpark/sudoku","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kEfzkAns0Saz","executionInfo":{"status":"ok","timestamp":1735486074735,"user_tz":-120,"elapsed":3401,"user":{"displayName":"marawan attya (320210295)","userId":"01792541721127694871"}},"outputId":"7c810eae-a85f-494e-c11d-e30e9131c7fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/bryanpark/sudoku\n","License(s): CC0-1.0\n","Downloading sudoku.zip to /content\n"," 93% 63.0M/68.1M [00:00<00:00, 129MB/s]\n","100% 68.1M/68.1M [00:00<00:00, 123MB/s]\n"]}],"execution_count":null},{"cell_type":"code","source":"!unzip sudoku.zip","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQscFaXy0c6v","executionInfo":{"status":"ok","timestamp":1735486081309,"user_tz":-120,"elapsed":6581,"user":{"displayName":"marawan attya (320210295)","userId":"01792541721127694871"}},"outputId":"2cec6273-6330-4f52-f39e-b0a342d277a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  sudoku.zip\n","  inflating: sudoku.csv              \n"]}],"execution_count":null},{"cell_type":"code","source":"!pip install stable-baselines3[extra]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QWZPtifiOqPV","executionInfo":{"status":"ok","timestamp":1735486094311,"user_tz":-120,"elapsed":13007,"user":{"displayName":"marawan attya (320210295)","userId":"01792541721127694871"}},"outputId":"70dd160d-97c5-442e-9e08-d13095f04d1c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting stable-baselines3[extra]\n","  Downloading stable_baselines3-2.4.0-py3-none-any.whl.metadata (4.5 kB)\n","Collecting gymnasium<1.1.0,>=0.29.1 (from stable-baselines3[extra])\n","  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: numpy<2.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.26.4)\n","Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.1+cu121)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.1.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.8.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.10.0.84)\n","Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.6.1)\n","Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.17.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.67.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.9.4)\n","Collecting ale-py>=0.9.0 (from stable-baselines3[extra])\n","  Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (11.0.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from ale-py>=0.9.0->stable-baselines3[extra]) (4.12.2)\n","Collecting farama-notifications>=0.0.1 (from gymnasium<1.1.0,>=0.29.1->stable-baselines3[extra])\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n","Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.68.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.2)\n","Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (4.25.5)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.1.0)\n","Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.16.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.55.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.2)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.18.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n","Downloading ale_py-0.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading stable_baselines3-2.4.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium, ale-py, stable-baselines3\n","Successfully installed ale-py-0.10.1 farama-notifications-0.0.4 gymnasium-1.0.0 stable-baselines3-2.4.0\n"]}],"execution_count":null},{"cell_type":"markdown","source":"# **PPO model**","metadata":{"id":"sBDSk1Vz1Stu"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gymnasium as gym\nfrom gymnasium import spaces\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nimport torch.nn as nn\nfrom torch.cuda import is_available\nimport gc\n\nclass SudokuEnv(gym.Env):\n    def __init__(self, data_path, batch_size=1000):\n        super(SudokuEnv, self).__init__()\n        self.data_path = data_path\n        self.batch_size = batch_size\n        self.current_batch_index = 0\n\n        # Load data in chunks\n        self.data_iterator = pd.read_csv(\n            self.data_path,\n            chunksize=self.batch_size\n        )\n        self.current_batch = next(self.data_iterator)\n\n        # Convert current batch to numpy arrays\n        self.puzzles = np.array([list(map(int, puzzle))\n                                for puzzle in self.current_batch['quizzes']])\n        self.solutions = np.array([list(map(int, solution))\n                                 for solution in self.current_batch['solutions']])\n\n        # Simplified observation space\n        self.observation_space = spaces.Box(\n            low=0, high=9, shape=(9, 9), dtype=np.float32\n        )\n        self.action_space = spaces.Discrete(81)\n\n        self.current_puzzle = None\n        self.current_solution = None\n        self.steps = 0\n        self.max_steps = 100\n\n    def _load_next_batch(self):\n        try:\n            self.current_batch = next(self.data_iterator)\n            self.puzzles = np.array([list(map(int, puzzle))\n                                   for puzzle in self.current_batch['quizzes']])\n            self.solutions = np.array([list(map(int, solution))\n                                    for solution in self.current_batch['solutions']])\n            self.current_batch_index = 0\n        except StopIteration:\n            # Reset iterator if we've gone through all chunks\n            self.data_iterator = pd.read_csv(\n                self.data_path,\n                chunksize=self.batch_size\n            )\n            self.current_batch = next(self.data_iterator)\n            self.puzzles = np.array([list(map(int, puzzle))\n                                   for puzzle in self.current_batch['quizzes']])\n            self.solutions = np.array([list(map(int, solution))\n                                    for solution in self.current_batch['solutions']])\n            self.current_batch_index = 0\n\n        # Force garbage collection\n        gc.collect()\n\n    def reset(self, seed=None):\n        super().reset(seed=seed)\n\n        # Load next batch if we've used all puzzles in current batch\n        if self.current_batch_index >= len(self.puzzles):\n            self._load_next_batch()\n\n        self.current_puzzle = self.puzzles[self.current_batch_index].reshape(9, 9).copy()\n        self.current_solution = self.solutions[self.current_batch_index].reshape(9, 9)\n        self.current_batch_index += 1\n        self.steps = 0\n\n        return self.current_puzzle.astype(np.float32), {}\n\n    def _is_valid_move(self, row, col, num):\n        # Check if cell is empty\n        if self.current_puzzle[row, col] != 0:\n            return False\n\n        # Check row\n        if num in self.current_puzzle[row]:\n            return False\n\n        # Check column\n        if num in self.current_puzzle[:, col]:\n            return False\n\n        # Check 3x3 box\n        box_row, box_col = 3 * (row // 3), 3 * (col // 3)\n        box = self.current_puzzle[box_row:box_row+3, box_col:box_col+3]\n        if num in box:\n            return False\n\n        return True\n\n    def step(self, action):\n        row, col = divmod(action, 9)\n        number = self.current_solution[row, col]  # Use solution number\n        self.steps += 1\n\n        # Check if move is valid\n        if not self._is_valid_move(row, col, number):\n            return self.current_puzzle.astype(np.float32), -1, True, False, {}\n\n        # Apply move\n        self.current_puzzle[row, col] = number\n\n        # Calculate reward\n        reward = 1 if number == self.current_solution[row, col] else -1\n\n        # Check if puzzle is solved or max steps reached\n        done = np.array_equal(self.current_puzzle, self.current_solution)\n        truncated = self.steps >= self.max_steps\n\n        if done:\n            reward += 10\n\n        return self.current_puzzle.astype(np.float32), reward, done, truncated, {}\n\n\nclass Network(nn.Module):\n    def __init__(self):\n        super(Network, self).__init__()\n        self.shared_net = nn.Sequential(\n            nn.Linear(81, 128),  # 9x9 flattened board\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU()\n        )\n\n        self.policy_net = nn.Sequential(\n            nn.Linear(128, 81)  # Output for each cell\n        )\n\n        self.value_net = nn.Sequential(\n            nn.Linear(128, 1)\n        )\n\ndef train_model(env_fn, total_timesteps=100_000):\n    # Create environment\n    env = DummyVecEnv([env_fn])\n\n    # Initialize model with network\n    policy_kwargs = dict(\n        net_arch=[64, 64]\n    )\n\n    # Use GPU if available\n    device = 'cpu'\n\n    model = PPO(\n        \"MlpPolicy\",\n        env,\n        verbose=1,\n        learning_rate=3e-4,\n        n_steps=1024,  # Smaller batch size\n        batch_size=64,\n        n_epochs=5,\n        gamma=0.99,\n        policy_kwargs=policy_kwargs,\n        device=device\n    )\n\n    # Train in smaller chunks\n    chunk_size = 10_000\n    for i in range(0, total_timesteps, chunk_size):\n        current_chunk = min(chunk_size, total_timesteps - i)\n        model.learn(total_timesteps=current_chunk)\n\n        # Force garbage collection\n        gc.collect()\n\n        # Save checkpoint\n        if (i + 1) % 50_000 == 0:\n            model.save(f\"ppo_sudoku_checkpoint_{i+1}\")\n\n    return model\n\ndef evaluate_model(model, env_fn, n_episodes=10):\n    \"\"\"\n    Evaluate the model's performance while maintaining memory efficiency.\n\n    Args:\n        model: Trained PPO model\n        env_fn: Function that creates a new environment\n        n_episodes: Number of episodes to evaluate\n\n    Returns:\n        success_rate: Percentage of successfully solved puzzles\n        avg_reward: Average reward across all episodes\n    \"\"\"\n    # Create a fresh environment for evaluation\n    eval_env = env_fn()\n\n    successes = 0\n    total_reward = 0\n\n    for episode in range(n_episodes):\n        obs, _ = eval_env.reset()\n        episode_reward = 0\n        done = False\n        truncated = False\n\n        while not (done or truncated):\n            # Get model's action (using deterministic policy for evaluation)\n            action, _ = model.predict(obs, deterministic=True)\n\n            # Take step in environment\n            obs, reward, done, truncated, _ = eval_env.step(action)\n            episode_reward += reward\n\n        # Count as success if puzzle was solved (not just truncated)\n        success = done and not truncated and episode_reward > 0\n        successes += success\n        total_reward += episode_reward\n\n        print(f\"Episode {episode + 1}/{n_episodes}: \"\n              f\"Success = {success}, Reward = {episode_reward:.2f}\")\n\n        # Force garbage collection after each episode\n        gc.collect()\n\n    success_rate = (successes / n_episodes) * 100\n    avg_reward = total_reward / n_episodes\n\n    return success_rate, avg_reward\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Create environment factory function\n    def make_env():\n        return SudokuEnv('/content/sudoku.csv', batch_size=1000)\n\n    # Train model\n    model = train_model(make_env)\n\n    # Evaluate model\n    print(\"\\nEvaluating model performance...\")\n    success_rate, avg_reward = evaluate_model(model, make_env)\n    print(f\"\\nFinal Results:\")\n    print(f\"Success Rate: {success_rate:.2f}%\")\n    print(f\"Average Reward: {avg_reward:.2f}\")\n\n    # Save final model\n    model.save(\"ppo_sudoku_final\")\n    print(\"\\nModel saved as 'ppo_sudoku_final'\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WofxYQbwB4z6","executionInfo":{"status":"ok","timestamp":1735487105329,"user_tz":-120,"elapsed":160511,"user":{"displayName":"marawan attya (320210295)","userId":"01792541721127694871"}},"outputId":"e82f7ee3-6cad-4fcd-f07a-25e9676f74df"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","-----------------------------\n","| time/              |      |\n","|    fps             | 1114 |\n","|    iterations      | 1    |\n","|    time_elapsed    | 0    |\n","|    total_timesteps | 1024 |\n","-----------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 914         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 2           |\n","|    total_timesteps      | 2048        |\n","| train/                  |             |\n","|    approx_kl            | 0.005044965 |\n","|    clip_fraction        | 0.0107      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.39       |\n","|    explained_variance   | -0.169      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.12        |\n","|    n_updates            | 5           |\n","|    policy_gradient_loss | -0.0345     |\n","|    value_loss           | 2.03        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 745         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 4           |\n","|    total_timesteps      | 3072        |\n","| train/                  |             |\n","|    approx_kl            | 0.004112209 |\n","|    clip_fraction        | 0.00586     |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.39       |\n","|    explained_variance   | -0.149      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.842       |\n","|    n_updates            | 10          |\n","|    policy_gradient_loss | -0.0303     |\n","|    value_loss           | 2.08        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 703         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.004475853 |\n","|    clip_fraction        | 0.00977     |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.39       |\n","|    explained_variance   | -0.0805     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.784       |\n","|    n_updates            | 15          |\n","|    policy_gradient_loss | -0.0341     |\n","|    value_loss           | 2           |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 622         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 5120        |\n","| train/                  |             |\n","|    approx_kl            | 0.003834047 |\n","|    clip_fraction        | 0.00684     |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.39       |\n","|    explained_variance   | 0.0099      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.935       |\n","|    n_updates            | 20          |\n","|    policy_gradient_loss | -0.0316     |\n","|    value_loss           | 2.4         |\n","-----------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 649          |\n","|    iterations           | 6            |\n","|    time_elapsed         | 9            |\n","|    total_timesteps      | 6144         |\n","| train/                  |              |\n","|    approx_kl            | 0.0043974365 |\n","|    clip_fraction        | 0.0115       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.38        |\n","|    explained_variance   | -0.0749      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.44         |\n","|    n_updates            | 25           |\n","|    policy_gradient_loss | -0.0333      |\n","|    value_loss           | 2.54         |\n","------------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 651         |\n","|    iterations           | 7           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 7168        |\n","| train/                  |             |\n","|    approx_kl            | 0.004753217 |\n","|    clip_fraction        | 0.0158      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.38       |\n","|    explained_variance   | -0.0545     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.978       |\n","|    n_updates            | 30          |\n","|    policy_gradient_loss | -0.037      |\n","|    value_loss           | 2.06        |\n","-----------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 670          |\n","|    iterations           | 8            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 8192         |\n","| train/                  |              |\n","|    approx_kl            | 0.0055465093 |\n","|    clip_fraction        | 0.0189       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.38        |\n","|    explained_variance   | -0.0749      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.872        |\n","|    n_updates            | 35           |\n","|    policy_gradient_loss | -0.0393      |\n","|    value_loss           | 1.96         |\n","------------------------------------------\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 686        |\n","|    iterations           | 9          |\n","|    time_elapsed         | 13         |\n","|    total_timesteps      | 9216       |\n","| train/                  |            |\n","|    approx_kl            | 0.00478568 |\n","|    clip_fraction        | 0.0133     |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.38      |\n","|    explained_variance   | -0.0788    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.42       |\n","|    n_updates            | 40         |\n","|    policy_gradient_loss | -0.0341    |\n","|    value_loss           | 2.62       |\n","----------------------------------------\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 681        |\n","|    iterations           | 10         |\n","|    time_elapsed         | 15         |\n","|    total_timesteps      | 10240      |\n","| train/                  |            |\n","|    approx_kl            | 0.00478262 |\n","|    clip_fraction        | 0.0158     |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.38      |\n","|    explained_variance   | -0.0596    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.02       |\n","|    n_updates            | 45         |\n","|    policy_gradient_loss | -0.0364    |\n","|    value_loss           | 2.45       |\n","----------------------------------------\n","-----------------------------\n","| time/              |      |\n","|    fps             | 1151 |\n","|    iterations      | 1    |\n","|    time_elapsed    | 0    |\n","|    total_timesteps | 1024 |\n","-----------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 819          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 2            |\n","|    total_timesteps      | 2048         |\n","| train/                  |              |\n","|    approx_kl            | 0.0050544553 |\n","|    clip_fraction        | 0.017        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.37        |\n","|    explained_variance   | -0.0655      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.5          |\n","|    n_updates            | 55           |\n","|    policy_gradient_loss | -0.0365      |\n","|    value_loss           | 2.56         |\n","------------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 743          |\n","|    iterations           | 3            |\n","|    time_elapsed         | 4            |\n","|    total_timesteps      | 3072         |\n","| train/                  |              |\n","|    approx_kl            | 0.0062264083 |\n","|    clip_fraction        | 0.023        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.37        |\n","|    explained_variance   | -0.0557      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.512        |\n","|    n_updates            | 60           |\n","|    policy_gradient_loss | -0.0396      |\n","|    value_loss           | 2.02         |\n","------------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 636         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 6           |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.005426985 |\n","|    clip_fraction        | 0.0215      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.37       |\n","|    explained_variance   | -0.0699     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.1         |\n","|    n_updates            | 65          |\n","|    policy_gradient_loss | -0.0384     |\n","|    value_loss           | 2.23        |\n","-----------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 650          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 7            |\n","|    total_timesteps      | 5120         |\n","| train/                  |              |\n","|    approx_kl            | 0.0060967477 |\n","|    clip_fraction        | 0.0238       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.36        |\n","|    explained_variance   | -0.054       |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.756        |\n","|    n_updates            | 70           |\n","|    policy_gradient_loss | -0.0408      |\n","|    value_loss           | 2.34         |\n","------------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 651          |\n","|    iterations           | 6            |\n","|    time_elapsed         | 9            |\n","|    total_timesteps      | 6144         |\n","| train/                  |              |\n","|    approx_kl            | 0.0059176306 |\n","|    clip_fraction        | 0.0242       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.36        |\n","|    explained_variance   | -0.0828      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.91         |\n","|    n_updates            | 75           |\n","|    policy_gradient_loss | -0.0414      |\n","|    value_loss           | 2.34         |\n","------------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 672         |\n","|    iterations           | 7           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 7168        |\n","| train/                  |             |\n","|    approx_kl            | 0.006019933 |\n","|    clip_fraction        | 0.024       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.36       |\n","|    explained_variance   | -0.0412     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.805       |\n","|    n_updates            | 80          |\n","|    policy_gradient_loss | -0.0421     |\n","|    value_loss           | 2.27        |\n","-----------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 686          |\n","|    iterations           | 8            |\n","|    time_elapsed         | 11           |\n","|    total_timesteps      | 8192         |\n","| train/                  |              |\n","|    approx_kl            | 0.0058921133 |\n","|    clip_fraction        | 0.0236       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.36        |\n","|    explained_variance   | -0.0649      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.35         |\n","|    n_updates            | 85           |\n","|    policy_gradient_loss | -0.0413      |\n","|    value_loss           | 2.59         |\n","------------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 681          |\n","|    iterations           | 9            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 9216         |\n","| train/                  |              |\n","|    approx_kl            | 0.0068700104 |\n","|    clip_fraction        | 0.0332       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.35        |\n","|    explained_variance   | -0.0467      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.27         |\n","|    n_updates            | 90           |\n","|    policy_gradient_loss | -0.0448      |\n","|    value_loss           | 2.37         |\n","------------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 692          |\n","|    iterations           | 10           |\n","|    time_elapsed         | 14           |\n","|    total_timesteps      | 10240        |\n","| train/                  |              |\n","|    approx_kl            | 0.0054834047 |\n","|    clip_fraction        | 0.0182       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.35        |\n","|    explained_variance   | -0.0548      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.14         |\n","|    n_updates            | 95           |\n","|    policy_gradient_loss | -0.0383      |\n","|    value_loss           | 2.74         |\n","------------------------------------------\n","-----------------------------\n","| time/              |      |\n","|    fps             | 806  |\n","|    iterations      | 1    |\n","|    time_elapsed    | 1    |\n","|    total_timesteps | 1024 |\n","-----------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 719          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 2            |\n","|    total_timesteps      | 2048         |\n","| train/                  |              |\n","|    approx_kl            | 0.0056679007 |\n","|    clip_fraction        | 0.0234       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.35        |\n","|    explained_variance   | -0.0153      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.959        |\n","|    n_updates            | 105          |\n","|    policy_gradient_loss | -0.0389      |\n","|    value_loss           | 2.98         |\n","------------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 663         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 4           |\n","|    total_timesteps      | 3072        |\n","| train/                  |             |\n","|    approx_kl            | 0.007377118 |\n","|    clip_fraction        | 0.0365      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.35       |\n","|    explained_variance   | -0.0138     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.08        |\n","|    n_updates            | 110         |\n","|    policy_gradient_loss | -0.0462     |\n","|    value_loss           | 2.78        |\n","-----------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 621          |\n","|    iterations           | 4            |\n","|    time_elapsed         | 6            |\n","|    total_timesteps      | 4096         |\n","| train/                  |              |\n","|    approx_kl            | 0.0072130477 |\n","|    clip_fraction        | 0.0336       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.35        |\n","|    explained_variance   | -0.0681      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.06         |\n","|    n_updates            | 115          |\n","|    policy_gradient_loss | -0.0458      |\n","|    value_loss           | 2.56         |\n","------------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 650          |\n","|    iterations           | 5            |\n","|    time_elapsed         | 7            |\n","|    total_timesteps      | 5120         |\n","| train/                  |              |\n","|    approx_kl            | 0.0073662596 |\n","|    clip_fraction        | 0.0359       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.34        |\n","|    explained_variance   | -0.0766      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.972        |\n","|    n_updates            | 120          |\n","|    policy_gradient_loss | -0.0476      |\n","|    value_loss           | 2.42         |\n","------------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 651         |\n","|    iterations           | 6           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.007259435 |\n","|    clip_fraction        | 0.0367      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.34       |\n","|    explained_variance   | -0.047      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.78        |\n","|    n_updates            | 125         |\n","|    policy_gradient_loss | -0.0444     |\n","|    value_loss           | 2.92        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 668         |\n","|    iterations           | 7           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 7168        |\n","| train/                  |             |\n","|    approx_kl            | 0.007959707 |\n","|    clip_fraction        | 0.0404      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.34       |\n","|    explained_variance   | -0.0544     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.901       |\n","|    n_updates            | 130         |\n","|    policy_gradient_loss | -0.0468     |\n","|    value_loss           | 2.65        |\n","-----------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 680          |\n","|    iterations           | 8            |\n","|    time_elapsed         | 12           |\n","|    total_timesteps      | 8192         |\n","| train/                  |              |\n","|    approx_kl            | 0.0075208833 |\n","|    clip_fraction        | 0.0373       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.34        |\n","|    explained_variance   | -0.0119      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.02         |\n","|    n_updates            | 135          |\n","|    policy_gradient_loss | -0.0452      |\n","|    value_loss           | 3.07         |\n","------------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 676          |\n","|    iterations           | 9            |\n","|    time_elapsed         | 13           |\n","|    total_timesteps      | 9216         |\n","| train/                  |              |\n","|    approx_kl            | 0.0078998655 |\n","|    clip_fraction        | 0.04         |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.34        |\n","|    explained_variance   | -0.0356      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.02         |\n","|    n_updates            | 140          |\n","|    policy_gradient_loss | -0.0485      |\n","|    value_loss           | 2.74         |\n","------------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 687         |\n","|    iterations           | 10          |\n","|    time_elapsed         | 14          |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.007440219 |\n","|    clip_fraction        | 0.033       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.34       |\n","|    explained_variance   | -0.0676     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.991       |\n","|    n_updates            | 145         |\n","|    policy_gradient_loss | -0.046      |\n","|    value_loss           | 2.81        |\n","-----------------------------------------\n","-----------------------------\n","| time/              |      |\n","|    fps             | 874  |\n","|    iterations      | 1    |\n","|    time_elapsed    | 1    |\n","|    total_timesteps | 1024 |\n","-----------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 602          |\n","|    iterations           | 2            |\n","|    time_elapsed         | 3            |\n","|    total_timesteps      | 2048         |\n","| train/                  |              |\n","|    approx_kl            | 0.0075027347 |\n","|    clip_fraction        | 0.0389       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.33        |\n","|    explained_variance   | -0.0941      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.44         |\n","|    n_updates            | 155          |\n","|    policy_gradient_loss | -0.0476      |\n","|    value_loss           | 2.93         |\n","------------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 591         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 3072        |\n","| train/                  |             |\n","|    approx_kl            | 0.009937219 |\n","|    clip_fraction        | 0.0563      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.33       |\n","|    explained_variance   | -0.0543     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.593       |\n","|    n_updates            | 160         |\n","|    policy_gradient_loss | -0.0527     |\n","|    value_loss           | 2.23        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 604         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 6           |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.009246992 |\n","|    clip_fraction        | 0.0574      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.33       |\n","|    explained_variance   | -0.00446    |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.893       |\n","|    n_updates            | 165         |\n","|    policy_gradient_loss | -0.0487     |\n","|    value_loss           | 3.34        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 636         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 5120        |\n","| train/                  |             |\n","|    approx_kl            | 0.008447083 |\n","|    clip_fraction        | 0.049       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.32       |\n","|    explained_variance   | -0.0503     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.2         |\n","|    n_updates            | 170         |\n","|    policy_gradient_loss | -0.0518     |\n","|    value_loss           | 2.68        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 660         |\n","|    iterations           | 6           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.009063095 |\n","|    clip_fraction        | 0.0539      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.32       |\n","|    explained_variance   | 0.00106     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.25        |\n","|    n_updates            | 175         |\n","|    policy_gradient_loss | -0.0523     |\n","|    value_loss           | 2.86        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 659         |\n","|    iterations           | 7           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 7168        |\n","| train/                  |             |\n","|    approx_kl            | 0.009454693 |\n","|    clip_fraction        | 0.0514      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.32       |\n","|    explained_variance   | -0.0308     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.916       |\n","|    n_updates            | 180         |\n","|    policy_gradient_loss | -0.0518     |\n","|    value_loss           | 2.44        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 675         |\n","|    iterations           | 8           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 8192        |\n","| train/                  |             |\n","|    approx_kl            | 0.007991603 |\n","|    clip_fraction        | 0.0441      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.31       |\n","|    explained_variance   | -0.0293     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.2         |\n","|    n_updates            | 185         |\n","|    policy_gradient_loss | -0.0462     |\n","|    value_loss           | 3.36        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 688         |\n","|    iterations           | 9           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 9216        |\n","| train/                  |             |\n","|    approx_kl            | 0.009271001 |\n","|    clip_fraction        | 0.0582      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.31       |\n","|    explained_variance   | -0.0416     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.842       |\n","|    n_updates            | 190         |\n","|    policy_gradient_loss | -0.0507     |\n","|    value_loss           | 2.59        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 681         |\n","|    iterations           | 10          |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.008558523 |\n","|    clip_fraction        | 0.0518      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.31       |\n","|    explained_variance   | -0.0467     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.07        |\n","|    n_updates            | 195         |\n","|    policy_gradient_loss | -0.0511     |\n","|    value_loss           | 2.91        |\n","-----------------------------------------\n","-----------------------------\n","| time/              |      |\n","|    fps             | 827  |\n","|    iterations      | 1    |\n","|    time_elapsed    | 1    |\n","|    total_timesteps | 1024 |\n","-----------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 586         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 3           |\n","|    total_timesteps      | 2048        |\n","| train/                  |             |\n","|    approx_kl            | 0.009722921 |\n","|    clip_fraction        | 0.0586      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.3        |\n","|    explained_variance   | -0.0215     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.03        |\n","|    n_updates            | 205         |\n","|    policy_gradient_loss | -0.0527     |\n","|    value_loss           | 2.75        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 647         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 4           |\n","|    total_timesteps      | 3072        |\n","| train/                  |             |\n","|    approx_kl            | 0.009801628 |\n","|    clip_fraction        | 0.0566      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.3        |\n","|    explained_variance   | -0.0365     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.55        |\n","|    n_updates            | 210         |\n","|    policy_gradient_loss | -0.053      |\n","|    value_loss           | 2.86        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 684         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.010699566 |\n","|    clip_fraction        | 0.0699      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.29       |\n","|    explained_variance   | -0.0411     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.794       |\n","|    n_updates            | 215         |\n","|    policy_gradient_loss | -0.0577     |\n","|    value_loss           | 2.26        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 676         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 7           |\n","|    total_timesteps      | 5120        |\n","| train/                  |             |\n","|    approx_kl            | 0.010054806 |\n","|    clip_fraction        | 0.0637      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.29       |\n","|    explained_variance   | -0.031      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.11        |\n","|    n_updates            | 220         |\n","|    policy_gradient_loss | -0.0515     |\n","|    value_loss           | 3.1         |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 684         |\n","|    iterations           | 6           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.009386652 |\n","|    clip_fraction        | 0.0598      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.29       |\n","|    explained_variance   | -0.012      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.56        |\n","|    n_updates            | 225         |\n","|    policy_gradient_loss | -0.0513     |\n","|    value_loss           | 3.55        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 698         |\n","|    iterations           | 7           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 7168        |\n","| train/                  |             |\n","|    approx_kl            | 0.008692866 |\n","|    clip_fraction        | 0.05        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.28       |\n","|    explained_variance   | -0.031      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.28        |\n","|    n_updates            | 230         |\n","|    policy_gradient_loss | -0.0493     |\n","|    value_loss           | 3.87        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 683         |\n","|    iterations           | 8           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 8192        |\n","| train/                  |             |\n","|    approx_kl            | 0.010205606 |\n","|    clip_fraction        | 0.0645      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.27       |\n","|    explained_variance   | -0.105      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.13        |\n","|    n_updates            | 235         |\n","|    policy_gradient_loss | -0.0549     |\n","|    value_loss           | 3.34        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 675         |\n","|    iterations           | 9           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 9216        |\n","| train/                  |             |\n","|    approx_kl            | 0.010670802 |\n","|    clip_fraction        | 0.0709      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.27       |\n","|    explained_variance   | -0.0449     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.21        |\n","|    n_updates            | 240         |\n","|    policy_gradient_loss | -0.0535     |\n","|    value_loss           | 3.15        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 647         |\n","|    iterations           | 10          |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.011593485 |\n","|    clip_fraction        | 0.0865      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.27       |\n","|    explained_variance   | -0.0168     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.52        |\n","|    n_updates            | 245         |\n","|    policy_gradient_loss | -0.0582     |\n","|    value_loss           | 2.89        |\n","-----------------------------------------\n","-----------------------------\n","| time/              |      |\n","|    fps             | 923  |\n","|    iterations      | 1    |\n","|    time_elapsed    | 1    |\n","|    total_timesteps | 1024 |\n","-----------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 845         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 2           |\n","|    total_timesteps      | 2048        |\n","| train/                  |             |\n","|    approx_kl            | 0.011458702 |\n","|    clip_fraction        | 0.0801      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.26       |\n","|    explained_variance   | -0.0266     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.35        |\n","|    n_updates            | 255         |\n","|    policy_gradient_loss | -0.057      |\n","|    value_loss           | 2.9         |\n","-----------------------------------------\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 757        |\n","|    iterations           | 3          |\n","|    time_elapsed         | 4          |\n","|    total_timesteps      | 3072       |\n","| train/                  |            |\n","|    approx_kl            | 0.01253007 |\n","|    clip_fraction        | 0.0846     |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.25      |\n","|    explained_variance   | -0.0284    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.11       |\n","|    n_updates            | 260        |\n","|    policy_gradient_loss | -0.0583    |\n","|    value_loss           | 2.79       |\n","----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 770         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.010741679 |\n","|    clip_fraction        | 0.0732      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.25       |\n","|    explained_variance   | -0.0393     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.83        |\n","|    n_updates            | 265         |\n","|    policy_gradient_loss | -0.0524     |\n","|    value_loss           | 3.84        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 754         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 6           |\n","|    total_timesteps      | 5120        |\n","| train/                  |             |\n","|    approx_kl            | 0.010495055 |\n","|    clip_fraction        | 0.0674      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.25       |\n","|    explained_variance   | -0.0591     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.06        |\n","|    n_updates            | 270         |\n","|    policy_gradient_loss | -0.0525     |\n","|    value_loss           | 4.14        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 733         |\n","|    iterations           | 6           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.012138452 |\n","|    clip_fraction        | 0.0828      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.24       |\n","|    explained_variance   | -0.011      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.2         |\n","|    n_updates            | 275         |\n","|    policy_gradient_loss | -0.0572     |\n","|    value_loss           | 3.27        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 747         |\n","|    iterations           | 7           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 7168        |\n","| train/                  |             |\n","|    approx_kl            | 0.011278532 |\n","|    clip_fraction        | 0.0773      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.23       |\n","|    explained_variance   | -0.000147   |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.64        |\n","|    n_updates            | 280         |\n","|    policy_gradient_loss | -0.0573     |\n","|    value_loss           | 3.48        |\n","-----------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 751          |\n","|    iterations           | 8            |\n","|    time_elapsed         | 10           |\n","|    total_timesteps      | 8192         |\n","| train/                  |              |\n","|    approx_kl            | 0.0119160935 |\n","|    clip_fraction        | 0.0871       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.23        |\n","|    explained_variance   | -0.0225      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.3          |\n","|    n_updates            | 285          |\n","|    policy_gradient_loss | -0.0617      |\n","|    value_loss           | 3.09         |\n","------------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 709         |\n","|    iterations           | 9           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 9216        |\n","| train/                  |             |\n","|    approx_kl            | 0.013069479 |\n","|    clip_fraction        | 0.0889      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.23       |\n","|    explained_variance   | 0.0136      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.13        |\n","|    n_updates            | 290         |\n","|    policy_gradient_loss | -0.0605     |\n","|    value_loss           | 2.62        |\n","-----------------------------------------\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 694          |\n","|    iterations           | 10           |\n","|    time_elapsed         | 14           |\n","|    total_timesteps      | 10240        |\n","| train/                  |              |\n","|    approx_kl            | 0.0112998765 |\n","|    clip_fraction        | 0.0752       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -4.22        |\n","|    explained_variance   | -0.0487      |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 0.945        |\n","|    n_updates            | 295          |\n","|    policy_gradient_loss | -0.0535      |\n","|    value_loss           | 3.48         |\n","------------------------------------------\n","-----------------------------\n","| time/              |      |\n","|    fps             | 1179 |\n","|    iterations      | 1    |\n","|    time_elapsed    | 0    |\n","|    total_timesteps | 1024 |\n","-----------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 818         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 2           |\n","|    total_timesteps      | 2048        |\n","| train/                  |             |\n","|    approx_kl            | 0.012945281 |\n","|    clip_fraction        | 0.0975      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.21       |\n","|    explained_variance   | -0.0411     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.37        |\n","|    n_updates            | 305         |\n","|    policy_gradient_loss | -0.0603     |\n","|    value_loss           | 3.2         |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 827         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 3           |\n","|    total_timesteps      | 3072        |\n","| train/                  |             |\n","|    approx_kl            | 0.014679079 |\n","|    clip_fraction        | 0.125       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.21       |\n","|    explained_variance   | -0.0407     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.08        |\n","|    n_updates            | 310         |\n","|    policy_gradient_loss | -0.0659     |\n","|    value_loss           | 2.9         |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 813         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.013893325 |\n","|    clip_fraction        | 0.103       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.21       |\n","|    explained_variance   | -0.0285     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.56        |\n","|    n_updates            | 315         |\n","|    policy_gradient_loss | -0.0643     |\n","|    value_loss           | 2.79        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 783         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 6           |\n","|    total_timesteps      | 5120        |\n","| train/                  |             |\n","|    approx_kl            | 0.013288282 |\n","|    clip_fraction        | 0.0984      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.2        |\n","|    explained_variance   | -0.0108     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.12        |\n","|    n_updates            | 320         |\n","|    policy_gradient_loss | -0.06       |\n","|    value_loss           | 3.22        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 787         |\n","|    iterations           | 6           |\n","|    time_elapsed         | 7           |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.014031207 |\n","|    clip_fraction        | 0.115       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.2        |\n","|    explained_variance   | -0.0124     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.99        |\n","|    n_updates            | 325         |\n","|    policy_gradient_loss | -0.0626     |\n","|    value_loss           | 3.06        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 792         |\n","|    iterations           | 7           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 7168        |\n","| train/                  |             |\n","|    approx_kl            | 0.012325618 |\n","|    clip_fraction        | 0.0904      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.19       |\n","|    explained_variance   | -0.0263     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.88        |\n","|    n_updates            | 330         |\n","|    policy_gradient_loss | -0.0564     |\n","|    value_loss           | 4.14        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 742         |\n","|    iterations           | 8           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 8192        |\n","| train/                  |             |\n","|    approx_kl            | 0.011471016 |\n","|    clip_fraction        | 0.0838      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.19       |\n","|    explained_variance   | -0.0435     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.29        |\n","|    n_updates            | 335         |\n","|    policy_gradient_loss | -0.0581     |\n","|    value_loss           | 3.76        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 720         |\n","|    iterations           | 9           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 9216        |\n","| train/                  |             |\n","|    approx_kl            | 0.012706039 |\n","|    clip_fraction        | 0.0959      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.16       |\n","|    explained_variance   | -0.029      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.24        |\n","|    n_updates            | 340         |\n","|    policy_gradient_loss | -0.0608     |\n","|    value_loss           | 3.2         |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 718         |\n","|    iterations           | 10          |\n","|    time_elapsed         | 14          |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.013916779 |\n","|    clip_fraction        | 0.103       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.17       |\n","|    explained_variance   | -0.0283     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.07        |\n","|    n_updates            | 345         |\n","|    policy_gradient_loss | -0.0615     |\n","|    value_loss           | 3.32        |\n","-----------------------------------------\n","-----------------------------\n","| time/              |      |\n","|    fps             | 832  |\n","|    iterations      | 1    |\n","|    time_elapsed    | 1    |\n","|    total_timesteps | 1024 |\n","-----------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 794         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 2           |\n","|    total_timesteps      | 2048        |\n","| train/                  |             |\n","|    approx_kl            | 0.013693678 |\n","|    clip_fraction        | 0.0957      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.16       |\n","|    explained_variance   | -0.0776     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.74        |\n","|    n_updates            | 355         |\n","|    policy_gradient_loss | -0.0605     |\n","|    value_loss           | 3.79        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 794         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 3           |\n","|    total_timesteps      | 3072        |\n","| train/                  |             |\n","|    approx_kl            | 0.013712033 |\n","|    clip_fraction        | 0.107       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.15       |\n","|    explained_variance   | -0.03       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.72        |\n","|    n_updates            | 360         |\n","|    policy_gradient_loss | -0.0626     |\n","|    value_loss           | 3.37        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 750         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.012921149 |\n","|    clip_fraction        | 0.0932      |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.14       |\n","|    explained_variance   | -0.0357     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.14        |\n","|    n_updates            | 365         |\n","|    policy_gradient_loss | -0.0601     |\n","|    value_loss           | 3.52        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 766         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 6           |\n","|    total_timesteps      | 5120        |\n","| train/                  |             |\n","|    approx_kl            | 0.013244097 |\n","|    clip_fraction        | 0.103       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.14       |\n","|    explained_variance   | 0.000656    |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.86        |\n","|    n_updates            | 370         |\n","|    policy_gradient_loss | -0.0615     |\n","|    value_loss           | 4.18        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 776         |\n","|    iterations           | 6           |\n","|    time_elapsed         | 7           |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.012790595 |\n","|    clip_fraction        | 0.106       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.15       |\n","|    explained_variance   | -0.0379     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.2         |\n","|    n_updates            | 375         |\n","|    policy_gradient_loss | -0.0622     |\n","|    value_loss           | 3.98        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 734         |\n","|    iterations           | 7           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 7168        |\n","| train/                  |             |\n","|    approx_kl            | 0.013260039 |\n","|    clip_fraction        | 0.102       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.13       |\n","|    explained_variance   | -0.0505     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.29        |\n","|    n_updates            | 380         |\n","|    policy_gradient_loss | -0.0613     |\n","|    value_loss           | 4.14        |\n","-----------------------------------------\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 713       |\n","|    iterations           | 8         |\n","|    time_elapsed         | 11        |\n","|    total_timesteps      | 8192      |\n","| train/                  |           |\n","|    approx_kl            | 0.0133836 |\n","|    clip_fraction        | 0.103     |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -4.13     |\n","|    explained_variance   | -0.0469   |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | 1.31      |\n","|    n_updates            | 385       |\n","|    policy_gradient_loss | -0.0619   |\n","|    value_loss           | 3.43      |\n","---------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 706         |\n","|    iterations           | 9           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 9216        |\n","| train/                  |             |\n","|    approx_kl            | 0.012806464 |\n","|    clip_fraction        | 0.102       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.12       |\n","|    explained_variance   | -0.0422     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.4         |\n","|    n_updates            | 390         |\n","|    policy_gradient_loss | -0.06       |\n","|    value_loss           | 4.29        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 703         |\n","|    iterations           | 10          |\n","|    time_elapsed         | 14          |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.013235226 |\n","|    clip_fraction        | 0.112       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.11       |\n","|    explained_variance   | 0.0106      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.71        |\n","|    n_updates            | 395         |\n","|    policy_gradient_loss | -0.0622     |\n","|    value_loss           | 4.03        |\n","-----------------------------------------\n","-----------------------------\n","| time/              |      |\n","|    fps             | 1146 |\n","|    iterations      | 1    |\n","|    time_elapsed    | 0    |\n","|    total_timesteps | 1024 |\n","-----------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 976         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 2           |\n","|    total_timesteps      | 2048        |\n","| train/                  |             |\n","|    approx_kl            | 0.014625623 |\n","|    clip_fraction        | 0.111       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.09       |\n","|    explained_variance   | -0.0158     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.1         |\n","|    n_updates            | 405         |\n","|    policy_gradient_loss | -0.0631     |\n","|    value_loss           | 3.84        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 840         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 3           |\n","|    total_timesteps      | 3072        |\n","| train/                  |             |\n","|    approx_kl            | 0.014196806 |\n","|    clip_fraction        | 0.115       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.09       |\n","|    explained_variance   | -0.0392     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.26        |\n","|    n_updates            | 410         |\n","|    policy_gradient_loss | -0.0648     |\n","|    value_loss           | 3.43        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 836         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 4           |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.014277166 |\n","|    clip_fraction        | 0.115       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.08       |\n","|    explained_variance   | -0.0102     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.85        |\n","|    n_updates            | 415         |\n","|    policy_gradient_loss | -0.0633     |\n","|    value_loss           | 3.51        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 797         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 6           |\n","|    total_timesteps      | 5120        |\n","| train/                  |             |\n","|    approx_kl            | 0.014157644 |\n","|    clip_fraction        | 0.117       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.08       |\n","|    explained_variance   | -0.0337     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.22        |\n","|    n_updates            | 420         |\n","|    policy_gradient_loss | -0.0641     |\n","|    value_loss           | 3.96        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 744         |\n","|    iterations           | 6           |\n","|    time_elapsed         | 8           |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.014205288 |\n","|    clip_fraction        | 0.118       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.08       |\n","|    explained_variance   | -0.0345     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.83        |\n","|    n_updates            | 425         |\n","|    policy_gradient_loss | -0.0623     |\n","|    value_loss           | 3.99        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 718         |\n","|    iterations           | 7           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 7168        |\n","| train/                  |             |\n","|    approx_kl            | 0.015113526 |\n","|    clip_fraction        | 0.124       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.07       |\n","|    explained_variance   | -0.0242     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.4         |\n","|    n_updates            | 430         |\n","|    policy_gradient_loss | -0.0654     |\n","|    value_loss           | 4.28        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 692         |\n","|    iterations           | 8           |\n","|    time_elapsed         | 11          |\n","|    total_timesteps      | 8192        |\n","| train/                  |             |\n","|    approx_kl            | 0.015040731 |\n","|    clip_fraction        | 0.131       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.05       |\n","|    explained_variance   | -0.0482     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.62        |\n","|    n_updates            | 435         |\n","|    policy_gradient_loss | -0.0624     |\n","|    value_loss           | 4.69        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 696         |\n","|    iterations           | 9           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 9216        |\n","| train/                  |             |\n","|    approx_kl            | 0.017061776 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.06       |\n","|    explained_variance   | -0.0178     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.78        |\n","|    n_updates            | 440         |\n","|    policy_gradient_loss | -0.0688     |\n","|    value_loss           | 3.1         |\n","-----------------------------------------\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 691        |\n","|    iterations           | 10         |\n","|    time_elapsed         | 14         |\n","|    total_timesteps      | 10240      |\n","| train/                  |            |\n","|    approx_kl            | 0.01747974 |\n","|    clip_fraction        | 0.139      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.05      |\n","|    explained_variance   | -0.0266    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.46       |\n","|    n_updates            | 445        |\n","|    policy_gradient_loss | -0.0691    |\n","|    value_loss           | 3.49       |\n","----------------------------------------\n","-----------------------------\n","| time/              |      |\n","|    fps             | 1100 |\n","|    iterations      | 1    |\n","|    time_elapsed    | 0    |\n","|    total_timesteps | 1024 |\n","-----------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 934         |\n","|    iterations           | 2           |\n","|    time_elapsed         | 2           |\n","|    total_timesteps      | 2048        |\n","| train/                  |             |\n","|    approx_kl            | 0.017947242 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.04       |\n","|    explained_variance   | -0.0403     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.27        |\n","|    n_updates            | 455         |\n","|    policy_gradient_loss | -0.0705     |\n","|    value_loss           | 3.72        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 790         |\n","|    iterations           | 3           |\n","|    time_elapsed         | 3           |\n","|    total_timesteps      | 3072        |\n","| train/                  |             |\n","|    approx_kl            | 0.017292477 |\n","|    clip_fraction        | 0.139       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.04       |\n","|    explained_variance   | 0.0116      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.24        |\n","|    n_updates            | 460         |\n","|    policy_gradient_loss | -0.0676     |\n","|    value_loss           | 3.93        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 773         |\n","|    iterations           | 4           |\n","|    time_elapsed         | 5           |\n","|    total_timesteps      | 4096        |\n","| train/                  |             |\n","|    approx_kl            | 0.016382504 |\n","|    clip_fraction        | 0.133       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.02       |\n","|    explained_variance   | -0.0631     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.46        |\n","|    n_updates            | 465         |\n","|    policy_gradient_loss | -0.0661     |\n","|    value_loss           | 3.92        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 760         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 6           |\n","|    total_timesteps      | 5120        |\n","| train/                  |             |\n","|    approx_kl            | 0.018006708 |\n","|    clip_fraction        | 0.159       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4.01       |\n","|    explained_variance   | -0.053      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.89        |\n","|    n_updates            | 470         |\n","|    policy_gradient_loss | -0.069      |\n","|    value_loss           | 3.78        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 671         |\n","|    iterations           | 6           |\n","|    time_elapsed         | 9           |\n","|    total_timesteps      | 6144        |\n","| train/                  |             |\n","|    approx_kl            | 0.016671207 |\n","|    clip_fraction        | 0.13        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4          |\n","|    explained_variance   | 0.00964     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.8         |\n","|    n_updates            | 475         |\n","|    policy_gradient_loss | -0.0659     |\n","|    value_loss           | 3.67        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 654         |\n","|    iterations           | 7           |\n","|    time_elapsed         | 10          |\n","|    total_timesteps      | 7168        |\n","| train/                  |             |\n","|    approx_kl            | 0.014121338 |\n","|    clip_fraction        | 0.117       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.98       |\n","|    explained_variance   | -0.0232     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.99        |\n","|    n_updates            | 480         |\n","|    policy_gradient_loss | -0.0635     |\n","|    value_loss           | 4.49        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 667         |\n","|    iterations           | 8           |\n","|    time_elapsed         | 12          |\n","|    total_timesteps      | 8192        |\n","| train/                  |             |\n","|    approx_kl            | 0.014083583 |\n","|    clip_fraction        | 0.107       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -4          |\n","|    explained_variance   | 0.00145     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.82        |\n","|    n_updates            | 485         |\n","|    policy_gradient_loss | -0.0628     |\n","|    value_loss           | 4.64        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 662         |\n","|    iterations           | 9           |\n","|    time_elapsed         | 13          |\n","|    total_timesteps      | 9216        |\n","| train/                  |             |\n","|    approx_kl            | 0.018118149 |\n","|    clip_fraction        | 0.151       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.97       |\n","|    explained_variance   | -0.00317    |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.52        |\n","|    n_updates            | 490         |\n","|    policy_gradient_loss | -0.0686     |\n","|    value_loss           | 3.64        |\n","-----------------------------------------\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 673         |\n","|    iterations           | 10          |\n","|    time_elapsed         | 15          |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.017251689 |\n","|    clip_fraction        | 0.146       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.95       |\n","|    explained_variance   | -0.0883     |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.6         |\n","|    n_updates            | 495         |\n","|    policy_gradient_loss | -0.0693     |\n","|    value_loss           | 3.67        |\n","-----------------------------------------\n","\n","Evaluating model performance...\n","Episode 1/10: Success = True, Reward = 3.00\n","Episode 2/10: Success = True, Reward = 2.00\n","Episode 3/10: Success = True, Reward = 9.00\n","Episode 4/10: Success = True, Reward = 2.00\n","Episode 5/10: Success = True, Reward = 4.00\n","Episode 6/10: Success = False, Reward = 0.00\n","Episode 7/10: Success = True, Reward = 1.00\n","Episode 8/10: Success = True, Reward = 3.00\n","Episode 9/10: Success = True, Reward = 2.00\n","Episode 10/10: Success = False, Reward = -1.00\n","\n","Final Results:\n","Success Rate: 80.00%\n","Average Reward: 2.50\n","\n","Model saved as 'ppo_sudoku_final'\n"]}],"execution_count":null},{"cell_type":"markdown","source":"# **DQN model**","metadata":{"id":"9y5EbIHzPvfj"}},{"cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport pandas as pd\nfrom stable_baselines3 import DQN\nfrom stable_baselines3.common.callbacks import BaseCallback\nimport os\nfrom tqdm import tqdm\nimport time\n\n# Define the Sudoku environment class\nclass SudokuEnv(gym.Env):\n    def __init__(self, df):\n        super().__init__()\n        self.puzzles = np.array([list(map(int, quiz)) for quiz in df['quizzes']])\n        self.solutions = np.array([list(map(int, sol)) for sol in df['solutions']])\n        self.action_space = spaces.Discrete(9 * 9 * 9)\n        self.observation_space = spaces.Box(low=0, high=9, shape=(9, 9), dtype=np.float32)\n        self.current_puzzle = None\n        self.current_solution = None\n        self.steps = 0\n        self.max_steps = 100\n        self.total_reward = 0\n\n    def _action_to_coords(self, action):\n        row = action // (9 * 9)\n        col = (action % (9 * 9)) // 9\n        number = (action % 9) + 1\n        return row, col, number\n\n    def reset(self, seed=None):\n        super().reset(seed=seed)\n        idx = np.random.randint(len(self.puzzles))\n        self.current_puzzle = self.puzzles[idx].reshape(9, 9).copy()\n        self.current_solution = self.solutions[idx].reshape(9, 9)\n        self.steps = 0\n        self.total_reward = 0\n        return self.current_puzzle.astype(np.float32), {}\n\n    def step(self, action):\n        row, col, number = self._action_to_coords(action)\n        self.steps += 1\n\n        if self.current_puzzle[row, col] != 0:\n            return self.current_puzzle.astype(np.float32), -1, True, False, {}\n\n        self.current_puzzle[row, col] = number\n\n        reward = 5 if self.current_puzzle[row, col] == self.current_solution[row, col] else -1\n        self.total_reward += reward\n\n        done = np.array_equal(self.current_puzzle, self.current_solution)\n        truncated = self.steps >= self.max_steps\n\n        return self.current_puzzle.astype(np.float32), reward, done, truncated, {}\n\n# Define a callback for training\nclass TrainingCallback(BaseCallback):\n    def __init__(self, total_timesteps, log_interval=100):\n        super().__init__()\n        self.total_timesteps = total_timesteps\n        self.log_interval = log_interval\n\n    def _on_step(self):\n        if self.n_calls % self.log_interval == 0:\n            print(f\"[INFO] Timestep: {self.n_calls}/{self.total_timesteps} | \"\n                  f\"Epsilon: {self.model.exploration_rate:.2f} | \"\n                  f\"Reward: {self.training_env.get_attr('total_reward')[0]:.1f}\")\n        return True\n\n# Function to evaluate the model\ndef evaluate_model(model, env, n_episodes=20):\n    successes = 0\n    total_reward = 0\n\n    for episode in range(1, n_episodes + 1):\n        obs, _ = env.reset()\n        done = False\n        episode_reward = 0\n\n        while not done:\n            action, _ = model.predict(obs, deterministic=True)\n            obs, reward, done, truncated, _ = env.step(action)\n            episode_reward += reward\n            done = done or truncated\n\n        success = episode_reward > 0\n        successes += success\n        total_reward += episode_reward\n\n        print(f\"[INFO] Episode: {episode}/{n_episodes} | \"\n              f\"Success: {success} | Reward: {episode_reward:.1f}\")\n\n    success_rate = (successes / n_episodes) * 100\n    avg_reward = total_reward / n_episodes\n    return success_rate, avg_reward\n\n# Function to solve and display Sudoku puzzles\ndef evaluate_and_solve(model, env, algorithm_name):\n    print(f\"\\nSolving Sudoku with {algorithm_name}...\")\n\n    obs, _ = env.reset()\n    unsolved_board = env.current_puzzle.copy()\n\n    done = False\n    episode_reward = 0\n    start_time = time.time()\n\n    while not done:\n        action, _ = model.predict(obs, deterministic=True)\n        obs, reward, done, truncated, _ = env.step(action)\n        episode_reward += reward\n        done = done or truncated\n\n    end_time = time.time()\n\n    print(\"\\nUnsolved Board:\")\n    print(unsolved_board)\n    print(\"\\nSolved Board:\")\n    print(env.current_puzzle)\n    print(f\"\\nTime Taken by {algorithm_name}: {end_time - start_time:.2f} seconds\")\n    print(f\"Reward Achieved: {episode_reward}\\n\")\n\n# Function to train and save the DQN model\ndef train_and_save_model(model, env, model_type, total_timesteps=10000):\n    print(f\"Training {model_type} model...\")\n    callback = TrainingCallback(total_timesteps)\n    model.learn(total_timesteps=total_timesteps, callback=callback)\n\n    os.makedirs(\"models\", exist_ok=True)\n    model_path = f\"models/{model_type.lower()}_sudoku\"\n    model.save(model_path)\n    print(f\"Model trained successfully and saved at: {model_path}\")\n\n# Main function to run the program\ndef main():\n    df = pd.read_csv('sudoku.csv')\n    env = SudokuEnv(df)\n\n    policy_kwargs = dict(net_arch=[64, 64])\n    dqn_model = DQN(\"MlpPolicy\", env, verbose=0, learning_rate=0.0003, policy_kwargs=policy_kwargs)\n\n    train_and_save_model(dqn_model, env, \"DQN\", total_timesteps=10000)\n\n    print(\"\\nEvaluating DQN model...\")\n    dqn_success_rate, dqn_avg_reward = evaluate_model(dqn_model, env)\n    print(\"DQN Evaluation Completed.\")\n    print(f\"DQN Success Rate: {dqn_success_rate:.1f}%\")\n    print(f\"DQN Average Reward: {dqn_avg_reward:.1f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lca4aPXmkVrh","executionInfo":{"status":"ok","timestamp":1735489977356,"user_tz":-120,"elapsed":58451,"user":{"displayName":"marawan attya (320210295)","userId":"01792541721127694871"}},"outputId":"d38bec5c-49ed-44c0-b7fc-ce70ea0483c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training DQN model...\n","[INFO] Timestep: 100/10000 | Epsilon: 0.91 | Reward: 2.0\n","[INFO] Timestep: 200/10000 | Epsilon: 0.81 | Reward: -2.0\n","[INFO] Timestep: 300/10000 | Epsilon: 0.72 | Reward: -4.0\n","[INFO] Timestep: 400/10000 | Epsilon: 0.62 | Reward: 0.0\n","[INFO] Timestep: 500/10000 | Epsilon: 0.53 | Reward: -1.0\n","[INFO] Timestep: 600/10000 | Epsilon: 0.43 | Reward: -1.0\n","[INFO] Timestep: 700/10000 | Epsilon: 0.34 | Reward: 0.0\n","[INFO] Timestep: 800/10000 | Epsilon: 0.24 | Reward: 0.0\n","[INFO] Timestep: 900/10000 | Epsilon: 0.15 | Reward: 0.0\n","[INFO] Timestep: 1000/10000 | Epsilon: 0.05 | Reward: -2.0\n","[INFO] Timestep: 1100/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 1200/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 1300/10000 | Epsilon: 0.05 | Reward: 5.0\n","[INFO] Timestep: 1400/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 1500/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 1600/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 1700/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 1800/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 1900/10000 | Epsilon: 0.05 | Reward: -2.0\n","[INFO] Timestep: 2000/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 2100/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 2200/10000 | Epsilon: 0.05 | Reward: -3.0\n","[INFO] Timestep: 2300/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 2400/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 2500/10000 | Epsilon: 0.05 | Reward: 1.0\n","[INFO] Timestep: 2600/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 2700/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 2800/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 2900/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 3000/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 3100/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 3200/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 3300/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 3400/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 3500/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 3600/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 3700/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 3800/10000 | Epsilon: 0.05 | Reward: -2.0\n","[INFO] Timestep: 3900/10000 | Epsilon: 0.05 | Reward: 4.0\n","[INFO] Timestep: 4000/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 4100/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 4200/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 4300/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 4400/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 4500/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 4600/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 4700/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 4800/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 4900/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 5000/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 5100/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 5200/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 5300/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 5400/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 5500/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 5600/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 5700/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 5800/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 5900/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 6000/10000 | Epsilon: 0.05 | Reward: 5.0\n","[INFO] Timestep: 6100/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 6200/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 6300/10000 | Epsilon: 0.05 | Reward: 5.0\n","[INFO] Timestep: 6400/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 6500/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 6600/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 6700/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 6800/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 6900/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 7000/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 7100/10000 | Epsilon: 0.05 | Reward: -2.0\n","[INFO] Timestep: 7200/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 7300/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 7400/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 7500/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 7600/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 7700/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 7800/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 7900/10000 | Epsilon: 0.05 | Reward: 4.0\n","[INFO] Timestep: 8000/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 8100/10000 | Epsilon: 0.05 | Reward: 5.0\n","[INFO] Timestep: 8200/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 8300/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 8400/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 8500/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 8600/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 8700/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 8800/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 8900/10000 | Epsilon: 0.05 | Reward: -2.0\n","[INFO] Timestep: 9000/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 9100/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 9200/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 9300/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 9400/10000 | Epsilon: 0.05 | Reward: -1.0\n","[INFO] Timestep: 9500/10000 | Epsilon: 0.05 | Reward: -3.0\n","[INFO] Timestep: 9600/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 9700/10000 | Epsilon: 0.05 | Reward: -2.0\n","[INFO] Timestep: 9800/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 9900/10000 | Epsilon: 0.05 | Reward: 0.0\n","[INFO] Timestep: 10000/10000 | Epsilon: 0.05 | Reward: 3.0\n","Model trained successfully and saved at: models/dqn_sudoku\n","\n","Evaluating DQN model...\n","[INFO] Episode: 1/20 | Success: False | Reward: -2.0\n","[INFO] Episode: 2/20 | Success: True | Reward: 4.0\n","[INFO] Episode: 3/20 | Success: False | Reward: -1.0\n","[INFO] Episode: 4/20 | Success: False | Reward: -3.0\n","[INFO] Episode: 5/20 | Success: False | Reward: -2.0\n","[INFO] Episode: 6/20 | Success: False | Reward: -4.0\n","[INFO] Episode: 7/20 | Success: False | Reward: -3.0\n","[INFO] Episode: 8/20 | Success: False | Reward: -2.0\n","[INFO] Episode: 9/20 | Success: False | Reward: -2.0\n","[INFO] Episode: 10/20 | Success: False | Reward: -1.0\n","[INFO] Episode: 11/20 | Success: False | Reward: -1.0\n","[INFO] Episode: 12/20 | Success: False | Reward: -2.0\n","[INFO] Episode: 13/20 | Success: False | Reward: -1.0\n","[INFO] Episode: 14/20 | Success: False | Reward: -1.0\n","[INFO] Episode: 15/20 | Success: False | Reward: -3.0\n","[INFO] Episode: 16/20 | Success: False | Reward: -3.0\n","[INFO] Episode: 17/20 | Success: False | Reward: -2.0\n","[INFO] Episode: 18/20 | Success: False | Reward: -2.0\n","[INFO] Episode: 19/20 | Success: False | Reward: -2.0\n","[INFO] Episode: 20/20 | Success: False | Reward: -2.0\n","DQN Evaluation Completed.\n","DQN Success Rate: 5.0%\n","DQN Average Reward: -1.8\n"]}],"execution_count":null},{"cell_type":"markdown","source":"# **NeuralLogicMachine**","metadata":{"id":"Qu7gqT0s2itO"}},{"cell_type":"code","source":"url='https://arxiv.org/pdf/2307.00653'","metadata":{"id":"chSV3cpv2iBJ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom stable_baselines3.common.callbacks import BaseCallback\nimport gymnasium as gym\nfrom gymnasium import spaces\n\n# Load and preprocess dataset\ndata = pd.read_csv('/content/sudoku.csv')\npuzzles = np.array([list(map(int, p)) for p in data['quizzes']])\nsolutions = np.array([list(map(int, s)) for s in data['solutions']])\n\n# Custom Gym Environment for Sudoku\nclass SudokuNLMEnv(gym.Env):\n    \"\"\"\n    Custom Gym Environment for solving Sudoku puzzles using Neuro Logic Machines (NLM).\n    \"\"\"\n    def __init__(self, puzzles, solutions):\n        super(SudokuNLMEnv, self).__init__()\n        self.puzzles = puzzles\n        self.solutions = solutions\n\n        # Action space: (row, col, number) encoded as a single integer (0 to 729)\n        self.action_space = spaces.Discrete(9 * 9 * 9)\n\n        # Observation space: predicates for row, column, and submatrix\n        self.observation_space = spaces.Box(low=0, high=1, shape=(9, 9, 3), dtype=np.float32)\n\n        self.current_puzzle = None\n        self.current_solution = None\n        self.steps = 0\n        self.max_steps = 729\n\n    def _action_to_coords(self, action):\n        \"\"\"\n        Convert an action into row, column, and number to place.\n        \"\"\"\n        row = action // (9 * 9)\n        col = (action % (9 * 9)) // 9\n        number = (action % 9) + 1\n        return row, col, number\n\n    def _generate_predicates(self, grid):\n        \"\"\"\n        Generate predicates and summarize over the number dimension.\n        \"\"\"\n        is_row = np.zeros((9, 9), dtype=int)\n        is_col = np.zeros((9, 9), dtype=int)\n        is_submat = np.zeros((9, 9), dtype=int)\n\n        for r in range(9):\n            for num in range(1, 10):\n                is_row[r, num - 1] = (grid[r, :] == num).any()\n\n        for c in range(9):\n            for num in range(1, 10):\n                is_col[c, num - 1] = (grid[:, c] == num).any()\n\n        for sub_r in range(0, 9, 3):\n            for sub_c in range(0, 9, 3):\n                subgrid = grid[sub_r:sub_r+3, sub_c:sub_c+3]\n                for num in range(1, 10):\n                    is_present = (subgrid == num).any()\n                    is_submat[sub_r:sub_r+3, sub_c:sub_c+3] += is_present\n\n        is_submat = np.clip(is_submat, 0, 1)\n        return np.stack([is_row, is_col, is_submat], axis=-1)\n\n    def reset(self, seed=None, options=None):\n        \"\"\"\n        Reset the environment with a random Sudoku puzzle.\n        \"\"\"\n        super().reset(seed=seed)\n        idx = np.random.randint(len(self.puzzles))\n        self.current_puzzle = self.puzzles[idx].reshape(9, 9).copy()\n        self.current_solution = self.solutions[idx].reshape(9, 9).copy()\n        self.steps = 0\n\n        return self._generate_predicates(self.current_puzzle), {}\n\n    def step(self, action):\n        \"\"\"\n        Perform an action in the environment.\n        \"\"\"\n        row, col, number = self._action_to_coords(action)\n        self.steps += 1\n\n        if self.current_puzzle[row, col] != 0:\n            return self._generate_predicates(self.current_puzzle), -5, False, self.steps >= self.max_steps, {}\n\n        valid = (\n            number not in self.current_puzzle[row, :] and\n            number not in self.current_puzzle[:, col] and\n            number not in self.current_puzzle[row//3*3:row//3*3+3, col//3*3:col//3*3+3]\n        )\n        reward = 1 if valid else -5\n\n        if valid:\n            self.current_puzzle[row, col] = number\n            if self.current_puzzle[row, col] == self.current_solution[row, col]:\n                reward += 10\n\n        done = np.array_equal(self.current_puzzle, self.current_solution)\n        truncated = self.steps >= self.max_steps\n\n        return self._generate_predicates(self.current_puzzle), reward, done, truncated, {}\n\n# Custom Callback for Monitoring Training\nclass SudokuCallback(BaseCallback):\n    \"\"\"\n    Custom callback for logging and monitoring training progress.\n    \"\"\"\n    def __init__(self, verbose=1):\n        super(SudokuCallback, self).__init__(verbose)\n\n    def _on_step(self) -> bool:\n        if self.n_calls % 1000 == 0:\n            episode_rewards = self.locals.get('rewards', [])\n            if episode_rewards:\n                avg_reward = np.mean(episode_rewards)\n                print(f\"[INFO] Step {self.n_calls}: Avg Reward = {avg_reward:.2f}\")\n        return True\n\n# Evaluate the Model\ndef evaluate_model(model, env, n_episodes=100):\n    \"\"\"\n    Evaluate the trained model on the environment.\n    \"\"\"\n    success_count = 0\n    total_reward = 0\n\n    for episode in range(n_episodes):\n        obs, _ = env.reset()\n        done = False\n        episode_reward = 0\n\n        while not done:\n            action, _ = model.predict(obs, deterministic=True)\n            obs, reward, done, truncated, _ = env.step(action)\n            episode_reward += reward\n\n            if done:\n                success_count += 1\n                break\n\n            if truncated:\n                break\n\n        total_reward += episode_reward\n\n    success_rate = success_count / n_episodes\n    avg_reward = total_reward / n_episodes\n\n    print(f\"Evaluation Results:\\nSuccess Rate: {success_rate:.2%}\\nAverage Reward: {avg_reward:.2f}\")\n    return success_rate, avg_reward\n\n# Main Script\nif __name__ == \"__main__\":\n    # Train the Model\n    env = DummyVecEnv([lambda: SudokuNLMEnv(puzzles, solutions)])\n    model = PPO(\"MlpPolicy\", env, verbose=1)\n    model.learn(total_timesteps=100_000, callback=SudokuCallback())\n    model.save(\"sudoku_nlm_model\")\n\n    # Evaluate the Model\n    eval_env = SudokuNLMEnv(puzzles, solutions)\n    evaluate_model(model, eval_env, n_episodes=100)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GgXq3_z3dDJ8","executionInfo":{"status":"ok","timestamp":1735431962463,"user_tz":-120,"elapsed":2141089,"user":{"displayName":"marawan attya (320210295)","userId":"01792541721127694871"}},"outputId":"d8ef3a2a-7e4c-4737-97e7-e06cd03943d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","|    loss                 | 2.84        |\n","|    n_updates            | 2390        |\n","|    policy_gradient_loss | -0.0291     |\n","|    value_loss           | 8.5         |\n","-----------------------------------------\n","[INFO] Step 492000: Avg Reward = -5.00\n","[INFO] Step 493000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 226         |\n","|    iterations           | 241         |\n","|    time_elapsed         | 2176        |\n","|    total_timesteps      | 493568      |\n","| train/                  |             |\n","|    approx_kl            | 0.020133015 |\n","|    clip_fraction        | 0.198       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.94       |\n","|    explained_variance   | 0.927       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.06        |\n","|    n_updates            | 2400        |\n","|    policy_gradient_loss | -0.0315     |\n","|    value_loss           | 8.36        |\n","-----------------------------------------\n","[INFO] Step 494000: Avg Reward = -5.00\n","[INFO] Step 495000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 226         |\n","|    iterations           | 242         |\n","|    time_elapsed         | 2186        |\n","|    total_timesteps      | 495616      |\n","| train/                  |             |\n","|    approx_kl            | 0.050286204 |\n","|    clip_fraction        | 0.263       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.29       |\n","|    explained_variance   | 0.895       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.59        |\n","|    n_updates            | 2410        |\n","|    policy_gradient_loss | -0.0291     |\n","|    value_loss           | 3.69        |\n","-----------------------------------------\n","[INFO] Step 496000: Avg Reward = -5.00\n","[INFO] Step 497000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 226         |\n","|    iterations           | 243         |\n","|    time_elapsed         | 2194        |\n","|    total_timesteps      | 497664      |\n","| train/                  |             |\n","|    approx_kl            | 0.014858253 |\n","|    clip_fraction        | 0.139       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.13       |\n","|    explained_variance   | 0.906       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.13        |\n","|    n_updates            | 2420        |\n","|    policy_gradient_loss | -0.0258     |\n","|    value_loss           | 8.03        |\n","-----------------------------------------\n","[INFO] Step 498000: Avg Reward = -5.00\n","[INFO] Step 499000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 226         |\n","|    iterations           | 244         |\n","|    time_elapsed         | 2203        |\n","|    total_timesteps      | 499712      |\n","| train/                  |             |\n","|    approx_kl            | 0.017869085 |\n","|    clip_fraction        | 0.236       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.83       |\n","|    explained_variance   | 0.852       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.96        |\n","|    n_updates            | 2430        |\n","|    policy_gradient_loss | -0.0286     |\n","|    value_loss           | 12.3        |\n","-----------------------------------------\n","[INFO] Step 500000: Avg Reward = -5.00\n","[INFO] Step 501000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 226         |\n","|    iterations           | 245         |\n","|    time_elapsed         | 2212        |\n","|    total_timesteps      | 501760      |\n","| train/                  |             |\n","|    approx_kl            | 0.013805822 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.27       |\n","|    explained_variance   | 0.922       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.35        |\n","|    n_updates            | 2440        |\n","|    policy_gradient_loss | -0.0277     |\n","|    value_loss           | 12.5        |\n","-----------------------------------------\n","[INFO] Step 502000: Avg Reward = -5.00\n","[INFO] Step 503000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 226         |\n","|    iterations           | 246         |\n","|    time_elapsed         | 2220        |\n","|    total_timesteps      | 503808      |\n","| train/                  |             |\n","|    approx_kl            | 0.018565396 |\n","|    clip_fraction        | 0.196       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.9        |\n","|    explained_variance   | 0.923       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.98        |\n","|    n_updates            | 2450        |\n","|    policy_gradient_loss | -0.0292     |\n","|    value_loss           | 6.88        |\n","-----------------------------------------\n","[INFO] Step 504000: Avg Reward = -5.00\n","[INFO] Step 505000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 226        |\n","|    iterations           | 247        |\n","|    time_elapsed         | 2230       |\n","|    total_timesteps      | 505856     |\n","| train/                  |            |\n","|    approx_kl            | 0.01711893 |\n","|    clip_fraction        | 0.137      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.69      |\n","|    explained_variance   | 0.902      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 2.86       |\n","|    n_updates            | 2460       |\n","|    policy_gradient_loss | -0.018     |\n","|    value_loss           | 6.69       |\n","----------------------------------------\n","[INFO] Step 506000: Avg Reward = -5.00\n","[INFO] Step 507000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 226         |\n","|    iterations           | 248         |\n","|    time_elapsed         | 2238        |\n","|    total_timesteps      | 507904      |\n","| train/                  |             |\n","|    approx_kl            | 0.016975552 |\n","|    clip_fraction        | 0.178       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.07       |\n","|    explained_variance   | 0.916       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4           |\n","|    n_updates            | 2470        |\n","|    policy_gradient_loss | -0.0304     |\n","|    value_loss           | 8.28        |\n","-----------------------------------------\n","[INFO] Step 508000: Avg Reward = -5.00\n","[INFO] Step 509000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 226         |\n","|    iterations           | 249         |\n","|    time_elapsed         | 2247        |\n","|    total_timesteps      | 509952      |\n","| train/                  |             |\n","|    approx_kl            | 0.020401608 |\n","|    clip_fraction        | 0.206       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.44       |\n","|    explained_variance   | 0.881       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.98        |\n","|    n_updates            | 2480        |\n","|    policy_gradient_loss | -0.0304     |\n","|    value_loss           | 11.2        |\n","-----------------------------------------\n","[INFO] Step 510000: Avg Reward = -5.00\n","[INFO] Step 511000: Avg Reward = -5.00\n","[INFO] Step 512000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 226         |\n","|    iterations           | 250         |\n","|    time_elapsed         | 2256        |\n","|    total_timesteps      | 512000      |\n","| train/                  |             |\n","|    approx_kl            | 0.016408913 |\n","|    clip_fraction        | 0.148       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.86       |\n","|    explained_variance   | 0.848       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.74        |\n","|    n_updates            | 2490        |\n","|    policy_gradient_loss | -0.027      |\n","|    value_loss           | 11.2        |\n","-----------------------------------------\n","[INFO] Step 513000: Avg Reward = -5.00\n","[INFO] Step 514000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 226         |\n","|    iterations           | 251         |\n","|    time_elapsed         | 2265        |\n","|    total_timesteps      | 514048      |\n","| train/                  |             |\n","|    approx_kl            | 0.022611734 |\n","|    clip_fraction        | 0.226       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.55       |\n","|    explained_variance   | 0.917       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.13        |\n","|    n_updates            | 2500        |\n","|    policy_gradient_loss | -0.0377     |\n","|    value_loss           | 9.31        |\n","-----------------------------------------\n","[INFO] Step 515000: Avg Reward = -5.00\n","[INFO] Step 516000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 226        |\n","|    iterations           | 252        |\n","|    time_elapsed         | 2274       |\n","|    total_timesteps      | 516096     |\n","| train/                  |            |\n","|    approx_kl            | 0.01783053 |\n","|    clip_fraction        | 0.143      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.87      |\n","|    explained_variance   | 0.889      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 4.29       |\n","|    n_updates            | 2510       |\n","|    policy_gradient_loss | -0.0276    |\n","|    value_loss           | 7.98       |\n","----------------------------------------\n","[INFO] Step 517000: Avg Reward = -5.00\n","[INFO] Step 518000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 227        |\n","|    iterations           | 253        |\n","|    time_elapsed         | 2281       |\n","|    total_timesteps      | 518144     |\n","| train/                  |            |\n","|    approx_kl            | 0.03177721 |\n","|    clip_fraction        | 0.207      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -3.04      |\n","|    explained_variance   | 0.946      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 2.26       |\n","|    n_updates            | 2520       |\n","|    policy_gradient_loss | -0.0279    |\n","|    value_loss           | 4.75       |\n","----------------------------------------\n","[INFO] Step 519000: Avg Reward = -5.00\n","[INFO] Step 520000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 254         |\n","|    time_elapsed         | 2291        |\n","|    total_timesteps      | 520192      |\n","| train/                  |             |\n","|    approx_kl            | 0.020731112 |\n","|    clip_fraction        | 0.226       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.33       |\n","|    explained_variance   | 0.95        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.88        |\n","|    n_updates            | 2530        |\n","|    policy_gradient_loss | -0.0287     |\n","|    value_loss           | 6.88        |\n","-----------------------------------------\n","[INFO] Step 521000: Avg Reward = -5.00\n","[INFO] Step 522000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 255         |\n","|    time_elapsed         | 2300        |\n","|    total_timesteps      | 522240      |\n","| train/                  |             |\n","|    approx_kl            | 0.016553193 |\n","|    clip_fraction        | 0.161       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.13       |\n","|    explained_variance   | 0.935       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.31        |\n","|    n_updates            | 2540        |\n","|    policy_gradient_loss | -0.0331     |\n","|    value_loss           | 6.73        |\n","-----------------------------------------\n","[INFO] Step 523000: Avg Reward = -5.00\n","[INFO] Step 524000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 256         |\n","|    time_elapsed         | 2308        |\n","|    total_timesteps      | 524288      |\n","| train/                  |             |\n","|    approx_kl            | 0.016771011 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.09       |\n","|    explained_variance   | 0.905       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.31        |\n","|    n_updates            | 2550        |\n","|    policy_gradient_loss | -0.0288     |\n","|    value_loss           | 6.81        |\n","-----------------------------------------\n","[INFO] Step 525000: Avg Reward = -5.00\n","[INFO] Step 526000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 257         |\n","|    time_elapsed         | 2318        |\n","|    total_timesteps      | 526336      |\n","| train/                  |             |\n","|    approx_kl            | 0.022236718 |\n","|    clip_fraction        | 0.191       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.93       |\n","|    explained_variance   | 0.927       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.32        |\n","|    n_updates            | 2560        |\n","|    policy_gradient_loss | -0.0301     |\n","|    value_loss           | 7.87        |\n","-----------------------------------------\n","[INFO] Step 527000: Avg Reward = -5.00\n","[INFO] Step 528000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 258         |\n","|    time_elapsed         | 2326        |\n","|    total_timesteps      | 528384      |\n","| train/                  |             |\n","|    approx_kl            | 0.025072996 |\n","|    clip_fraction        | 0.208       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.8        |\n","|    explained_variance   | 0.886       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.25        |\n","|    n_updates            | 2570        |\n","|    policy_gradient_loss | -0.0309     |\n","|    value_loss           | 6.65        |\n","-----------------------------------------\n","[INFO] Step 529000: Avg Reward = -5.00\n","[INFO] Step 530000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 259         |\n","|    time_elapsed         | 2335        |\n","|    total_timesteps      | 530432      |\n","| train/                  |             |\n","|    approx_kl            | 0.021223102 |\n","|    clip_fraction        | 0.208       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.03       |\n","|    explained_variance   | 0.875       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.15        |\n","|    n_updates            | 2580        |\n","|    policy_gradient_loss | -0.034      |\n","|    value_loss           | 10.6        |\n","-----------------------------------------\n","[INFO] Step 531000: Avg Reward = -5.00\n","[INFO] Step 532000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 260         |\n","|    time_elapsed         | 2344        |\n","|    total_timesteps      | 532480      |\n","| train/                  |             |\n","|    approx_kl            | 0.018995838 |\n","|    clip_fraction        | 0.161       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.09       |\n","|    explained_variance   | 0.897       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.96        |\n","|    n_updates            | 2590        |\n","|    policy_gradient_loss | -0.0266     |\n","|    value_loss           | 7.25        |\n","-----------------------------------------\n","[INFO] Step 533000: Avg Reward = -5.00\n","[INFO] Step 534000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 261         |\n","|    time_elapsed         | 2352        |\n","|    total_timesteps      | 534528      |\n","| train/                  |             |\n","|    approx_kl            | 0.013999855 |\n","|    clip_fraction        | 0.157       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.94       |\n","|    explained_variance   | 0.823       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.74        |\n","|    n_updates            | 2600        |\n","|    policy_gradient_loss | -0.022      |\n","|    value_loss           | 7.18        |\n","-----------------------------------------\n","[INFO] Step 535000: Avg Reward = -5.00\n","[INFO] Step 536000: Avg Reward = 1.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 262         |\n","|    time_elapsed         | 2362        |\n","|    total_timesteps      | 536576      |\n","| train/                  |             |\n","|    approx_kl            | 0.015731242 |\n","|    clip_fraction        | 0.172       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.83       |\n","|    explained_variance   | 0.951       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.31        |\n","|    n_updates            | 2610        |\n","|    policy_gradient_loss | -0.0352     |\n","|    value_loss           | 6.89        |\n","-----------------------------------------\n","[INFO] Step 537000: Avg Reward = -5.00\n","[INFO] Step 538000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 263         |\n","|    time_elapsed         | 2370        |\n","|    total_timesteps      | 538624      |\n","| train/                  |             |\n","|    approx_kl            | 0.014457824 |\n","|    clip_fraction        | 0.125       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.9        |\n","|    explained_variance   | 0.93        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.05        |\n","|    n_updates            | 2620        |\n","|    policy_gradient_loss | -0.0261     |\n","|    value_loss           | 8.86        |\n","-----------------------------------------\n","[INFO] Step 539000: Avg Reward = -5.00\n","[INFO] Step 540000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 264         |\n","|    time_elapsed         | 2379        |\n","|    total_timesteps      | 540672      |\n","| train/                  |             |\n","|    approx_kl            | 0.017310545 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.08       |\n","|    explained_variance   | 0.951       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.2         |\n","|    n_updates            | 2630        |\n","|    policy_gradient_loss | -0.023      |\n","|    value_loss           | 4.78        |\n","-----------------------------------------\n","[INFO] Step 541000: Avg Reward = -5.00\n","[INFO] Step 542000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 265         |\n","|    time_elapsed         | 2388        |\n","|    total_timesteps      | 542720      |\n","| train/                  |             |\n","|    approx_kl            | 0.017271519 |\n","|    clip_fraction        | 0.169       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.97       |\n","|    explained_variance   | 0.928       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.68        |\n","|    n_updates            | 2640        |\n","|    policy_gradient_loss | -0.0239     |\n","|    value_loss           | 6.71        |\n","-----------------------------------------\n","[INFO] Step 543000: Avg Reward = -5.00\n","[INFO] Step 544000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 266         |\n","|    time_elapsed         | 2396        |\n","|    total_timesteps      | 544768      |\n","| train/                  |             |\n","|    approx_kl            | 0.017208816 |\n","|    clip_fraction        | 0.168       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.09       |\n","|    explained_variance   | 0.925       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.75        |\n","|    n_updates            | 2650        |\n","|    policy_gradient_loss | -0.0255     |\n","|    value_loss           | 9.94        |\n","-----------------------------------------\n","[INFO] Step 545000: Avg Reward = -5.00\n","[INFO] Step 546000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 267         |\n","|    time_elapsed         | 2406        |\n","|    total_timesteps      | 546816      |\n","| train/                  |             |\n","|    approx_kl            | 0.017270371 |\n","|    clip_fraction        | 0.19        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.11       |\n","|    explained_variance   | 0.912       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.89        |\n","|    n_updates            | 2660        |\n","|    policy_gradient_loss | -0.0299     |\n","|    value_loss           | 11.4        |\n","-----------------------------------------\n","[INFO] Step 547000: Avg Reward = -5.00\n","[INFO] Step 548000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 268         |\n","|    time_elapsed         | 2415        |\n","|    total_timesteps      | 548864      |\n","| train/                  |             |\n","|    approx_kl            | 0.019465987 |\n","|    clip_fraction        | 0.165       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.94       |\n","|    explained_variance   | 0.916       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.3         |\n","|    n_updates            | 2670        |\n","|    policy_gradient_loss | -0.0358     |\n","|    value_loss           | 10.8        |\n","-----------------------------------------\n","[INFO] Step 549000: Avg Reward = -5.00\n","[INFO] Step 550000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 269         |\n","|    time_elapsed         | 2423        |\n","|    total_timesteps      | 550912      |\n","| train/                  |             |\n","|    approx_kl            | 0.017510882 |\n","|    clip_fraction        | 0.171       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.95       |\n","|    explained_variance   | 0.883       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.08        |\n","|    n_updates            | 2680        |\n","|    policy_gradient_loss | -0.0251     |\n","|    value_loss           | 7.34        |\n","-----------------------------------------\n","[INFO] Step 551000: Avg Reward = -5.00\n","[INFO] Step 552000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 270         |\n","|    time_elapsed         | 2433        |\n","|    total_timesteps      | 552960      |\n","| train/                  |             |\n","|    approx_kl            | 0.029390387 |\n","|    clip_fraction        | 0.206       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.91       |\n","|    explained_variance   | 0.942       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.11        |\n","|    n_updates            | 2690        |\n","|    policy_gradient_loss | -0.0344     |\n","|    value_loss           | 8.65        |\n","-----------------------------------------\n","[INFO] Step 553000: Avg Reward = -5.00\n","[INFO] Step 554000: Avg Reward = -5.00\n","[INFO] Step 555000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 271         |\n","|    time_elapsed         | 2440        |\n","|    total_timesteps      | 555008      |\n","| train/                  |             |\n","|    approx_kl            | 0.024533778 |\n","|    clip_fraction        | 0.197       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.62       |\n","|    explained_variance   | 0.889       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.16        |\n","|    n_updates            | 2700        |\n","|    policy_gradient_loss | -0.0311     |\n","|    value_loss           | 7.24        |\n","-----------------------------------------\n","[INFO] Step 556000: Avg Reward = -5.00\n","[INFO] Step 557000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 272         |\n","|    time_elapsed         | 2450        |\n","|    total_timesteps      | 557056      |\n","| train/                  |             |\n","|    approx_kl            | 0.016995896 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.02       |\n","|    explained_variance   | 0.923       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.2         |\n","|    n_updates            | 2710        |\n","|    policy_gradient_loss | -0.0252     |\n","|    value_loss           | 9.07        |\n","-----------------------------------------\n","[INFO] Step 558000: Avg Reward = -5.00\n","[INFO] Step 559000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 273         |\n","|    time_elapsed         | 2459        |\n","|    total_timesteps      | 559104      |\n","| train/                  |             |\n","|    approx_kl            | 0.024587598 |\n","|    clip_fraction        | 0.204       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.24       |\n","|    explained_variance   | 0.823       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.56        |\n","|    n_updates            | 2720        |\n","|    policy_gradient_loss | -0.0345     |\n","|    value_loss           | 11.8        |\n","-----------------------------------------\n","[INFO] Step 560000: Avg Reward = -5.00\n","[INFO] Step 561000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 274         |\n","|    time_elapsed         | 2467        |\n","|    total_timesteps      | 561152      |\n","| train/                  |             |\n","|    approx_kl            | 0.016733762 |\n","|    clip_fraction        | 0.216       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.35       |\n","|    explained_variance   | 0.916       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.5         |\n","|    n_updates            | 2730        |\n","|    policy_gradient_loss | -0.0309     |\n","|    value_loss           | 7.84        |\n","-----------------------------------------\n","[INFO] Step 562000: Avg Reward = -5.00\n","[INFO] Step 563000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 227        |\n","|    iterations           | 275        |\n","|    time_elapsed         | 2477       |\n","|    total_timesteps      | 563200     |\n","| train/                  |            |\n","|    approx_kl            | 0.02143531 |\n","|    clip_fraction        | 0.194      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.91      |\n","|    explained_variance   | 0.946      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 3.39       |\n","|    n_updates            | 2740       |\n","|    policy_gradient_loss | -0.0336    |\n","|    value_loss           | 7.07       |\n","----------------------------------------\n","[INFO] Step 564000: Avg Reward = -5.00\n","[INFO] Step 565000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 276         |\n","|    time_elapsed         | 2485        |\n","|    total_timesteps      | 565248      |\n","| train/                  |             |\n","|    approx_kl            | 0.015185633 |\n","|    clip_fraction        | 0.174       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.45       |\n","|    explained_variance   | 0.893       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.02        |\n","|    n_updates            | 2750        |\n","|    policy_gradient_loss | -0.03       |\n","|    value_loss           | 10.9        |\n","-----------------------------------------\n","[INFO] Step 566000: Avg Reward = -5.00\n","[INFO] Step 567000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 277         |\n","|    time_elapsed         | 2494        |\n","|    total_timesteps      | 567296      |\n","| train/                  |             |\n","|    approx_kl            | 0.023094699 |\n","|    clip_fraction        | 0.164       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3          |\n","|    explained_variance   | 0.949       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.61        |\n","|    n_updates            | 2760        |\n","|    policy_gradient_loss | -0.0317     |\n","|    value_loss           | 7.74        |\n","-----------------------------------------\n","[INFO] Step 568000: Avg Reward = -5.00\n","[INFO] Step 569000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 278         |\n","|    time_elapsed         | 2504        |\n","|    total_timesteps      | 569344      |\n","| train/                  |             |\n","|    approx_kl            | 0.020300217 |\n","|    clip_fraction        | 0.192       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.04       |\n","|    explained_variance   | 0.947       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.24        |\n","|    n_updates            | 2770        |\n","|    policy_gradient_loss | -0.033      |\n","|    value_loss           | 6.41        |\n","-----------------------------------------\n","[INFO] Step 570000: Avg Reward = -5.00\n","[INFO] Step 571000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 227        |\n","|    iterations           | 279        |\n","|    time_elapsed         | 2512       |\n","|    total_timesteps      | 571392     |\n","| train/                  |            |\n","|    approx_kl            | 0.01215861 |\n","|    clip_fraction        | 0.132      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.49      |\n","|    explained_variance   | 0.935      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 4.76       |\n","|    n_updates            | 2780       |\n","|    policy_gradient_loss | -0.0275    |\n","|    value_loss           | 10.2       |\n","----------------------------------------\n","[INFO] Step 572000: Avg Reward = -5.00\n","[INFO] Step 573000: Avg Reward = 11.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 280         |\n","|    time_elapsed         | 2521        |\n","|    total_timesteps      | 573440      |\n","| train/                  |             |\n","|    approx_kl            | 0.021503948 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.57       |\n","|    explained_variance   | 0.93        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.34        |\n","|    n_updates            | 2790        |\n","|    policy_gradient_loss | -0.0312     |\n","|    value_loss           | 8.96        |\n","-----------------------------------------\n","[INFO] Step 574000: Avg Reward = -5.00\n","[INFO] Step 575000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 281         |\n","|    time_elapsed         | 2531        |\n","|    total_timesteps      | 575488      |\n","| train/                  |             |\n","|    approx_kl            | 0.016636077 |\n","|    clip_fraction        | 0.178       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.04       |\n","|    explained_variance   | 0.943       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.22        |\n","|    n_updates            | 2800        |\n","|    policy_gradient_loss | -0.0304     |\n","|    value_loss           | 7.92        |\n","-----------------------------------------\n","[INFO] Step 576000: Avg Reward = -5.00\n","[INFO] Step 577000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 282         |\n","|    time_elapsed         | 2539        |\n","|    total_timesteps      | 577536      |\n","| train/                  |             |\n","|    approx_kl            | 0.012178845 |\n","|    clip_fraction        | 0.12        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.4        |\n","|    explained_variance   | 0.926       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.99        |\n","|    n_updates            | 2810        |\n","|    policy_gradient_loss | -0.0239     |\n","|    value_loss           | 8.64        |\n","-----------------------------------------\n","[INFO] Step 578000: Avg Reward = -5.00\n","[INFO] Step 579000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 283         |\n","|    time_elapsed         | 2548        |\n","|    total_timesteps      | 579584      |\n","| train/                  |             |\n","|    approx_kl            | 0.012400652 |\n","|    clip_fraction        | 0.141       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.78       |\n","|    explained_variance   | 0.913       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.92        |\n","|    n_updates            | 2820        |\n","|    policy_gradient_loss | -0.0257     |\n","|    value_loss           | 10.2        |\n","-----------------------------------------\n","[INFO] Step 580000: Avg Reward = -5.00\n","[INFO] Step 581000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 227        |\n","|    iterations           | 284        |\n","|    time_elapsed         | 2556       |\n","|    total_timesteps      | 581632     |\n","| train/                  |            |\n","|    approx_kl            | 0.01753413 |\n","|    clip_fraction        | 0.176      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -3.02      |\n","|    explained_variance   | 0.954      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 2.72       |\n","|    n_updates            | 2830       |\n","|    policy_gradient_loss | -0.0366    |\n","|    value_loss           | 5.12       |\n","----------------------------------------\n","[INFO] Step 582000: Avg Reward = -5.00\n","[INFO] Step 583000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 285         |\n","|    time_elapsed         | 2565        |\n","|    total_timesteps      | 583680      |\n","| train/                  |             |\n","|    approx_kl            | 0.021606252 |\n","|    clip_fraction        | 0.212       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.9        |\n","|    explained_variance   | 0.953       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.46        |\n","|    n_updates            | 2840        |\n","|    policy_gradient_loss | -0.0375     |\n","|    value_loss           | 6.65        |\n","-----------------------------------------\n","[INFO] Step 584000: Avg Reward = -5.00\n","[INFO] Step 585000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 286         |\n","|    time_elapsed         | 2575        |\n","|    total_timesteps      | 585728      |\n","| train/                  |             |\n","|    approx_kl            | 0.016236722 |\n","|    clip_fraction        | 0.163       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.84       |\n","|    explained_variance   | 0.901       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.74        |\n","|    n_updates            | 2850        |\n","|    policy_gradient_loss | -0.0261     |\n","|    value_loss           | 8.53        |\n","-----------------------------------------\n","[INFO] Step 586000: Avg Reward = -5.00\n","[INFO] Step 587000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 287         |\n","|    time_elapsed         | 2583        |\n","|    total_timesteps      | 587776      |\n","| train/                  |             |\n","|    approx_kl            | 0.022961413 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.46       |\n","|    explained_variance   | 0.969       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.01        |\n","|    n_updates            | 2860        |\n","|    policy_gradient_loss | -0.0307     |\n","|    value_loss           | 4.16        |\n","-----------------------------------------\n","[INFO] Step 588000: Avg Reward = -5.00\n","[INFO] Step 589000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 227        |\n","|    iterations           | 288        |\n","|    time_elapsed         | 2592       |\n","|    total_timesteps      | 589824     |\n","| train/                  |            |\n","|    approx_kl            | 0.02545971 |\n","|    clip_fraction        | 0.169      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.43      |\n","|    explained_variance   | 0.93       |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.32       |\n","|    n_updates            | 2870       |\n","|    policy_gradient_loss | -0.0342    |\n","|    value_loss           | 5.26       |\n","----------------------------------------\n","[INFO] Step 590000: Avg Reward = -5.00\n","[INFO] Step 591000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 289         |\n","|    time_elapsed         | 2601        |\n","|    total_timesteps      | 591872      |\n","| train/                  |             |\n","|    approx_kl            | 0.012508171 |\n","|    clip_fraction        | 0.155       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.57       |\n","|    explained_variance   | 0.968       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.96        |\n","|    n_updates            | 2880        |\n","|    policy_gradient_loss | -0.0299     |\n","|    value_loss           | 5.14        |\n","-----------------------------------------\n","[INFO] Step 592000: Avg Reward = -5.00\n","[INFO] Step 593000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 290         |\n","|    time_elapsed         | 2610        |\n","|    total_timesteps      | 593920      |\n","| train/                  |             |\n","|    approx_kl            | 0.017441068 |\n","|    clip_fraction        | 0.144       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.62       |\n","|    explained_variance   | 0.934       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.8         |\n","|    n_updates            | 2890        |\n","|    policy_gradient_loss | -0.0318     |\n","|    value_loss           | 10.6        |\n","-----------------------------------------\n","[INFO] Step 594000: Avg Reward = -5.00\n","[INFO] Step 595000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 291         |\n","|    time_elapsed         | 2619        |\n","|    total_timesteps      | 595968      |\n","| train/                  |             |\n","|    approx_kl            | 0.016948277 |\n","|    clip_fraction        | 0.147       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.68       |\n","|    explained_variance   | 0.946       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.07        |\n","|    n_updates            | 2900        |\n","|    policy_gradient_loss | -0.026      |\n","|    value_loss           | 7.92        |\n","-----------------------------------------\n","[INFO] Step 596000: Avg Reward = -5.00\n","[INFO] Step 597000: Avg Reward = -5.00\n","[INFO] Step 598000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 292         |\n","|    time_elapsed         | 2627        |\n","|    total_timesteps      | 598016      |\n","| train/                  |             |\n","|    approx_kl            | 0.018670753 |\n","|    clip_fraction        | 0.161       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.21       |\n","|    explained_variance   | 0.938       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.04        |\n","|    n_updates            | 2910        |\n","|    policy_gradient_loss | -0.0364     |\n","|    value_loss           | 5.02        |\n","-----------------------------------------\n","[INFO] Step 599000: Avg Reward = -5.00\n","[INFO] Step 600000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 293         |\n","|    time_elapsed         | 2636        |\n","|    total_timesteps      | 600064      |\n","| train/                  |             |\n","|    approx_kl            | 0.023458485 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.12       |\n","|    explained_variance   | 0.951       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.24        |\n","|    n_updates            | 2920        |\n","|    policy_gradient_loss | -0.035      |\n","|    value_loss           | 6.26        |\n","-----------------------------------------\n","[INFO] Step 601000: Avg Reward = -5.00\n","[INFO] Step 602000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 294         |\n","|    time_elapsed         | 2645        |\n","|    total_timesteps      | 602112      |\n","| train/                  |             |\n","|    approx_kl            | 0.027160779 |\n","|    clip_fraction        | 0.248       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.83       |\n","|    explained_variance   | 0.925       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.75        |\n","|    n_updates            | 2930        |\n","|    policy_gradient_loss | -0.0379     |\n","|    value_loss           | 5.14        |\n","-----------------------------------------\n","[INFO] Step 603000: Avg Reward = -5.00\n","[INFO] Step 604000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 295         |\n","|    time_elapsed         | 2654        |\n","|    total_timesteps      | 604160      |\n","| train/                  |             |\n","|    approx_kl            | 0.035128757 |\n","|    clip_fraction        | 0.258       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.94       |\n","|    explained_variance   | 0.951       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.57        |\n","|    n_updates            | 2940        |\n","|    policy_gradient_loss | -0.045      |\n","|    value_loss           | 6.5         |\n","-----------------------------------------\n","[INFO] Step 605000: Avg Reward = -5.00\n","[INFO] Step 606000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 296         |\n","|    time_elapsed         | 2663        |\n","|    total_timesteps      | 606208      |\n","| train/                  |             |\n","|    approx_kl            | 0.019925687 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.79       |\n","|    explained_variance   | 0.931       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.99        |\n","|    n_updates            | 2950        |\n","|    policy_gradient_loss | -0.03       |\n","|    value_loss           | 8.69        |\n","-----------------------------------------\n","[INFO] Step 607000: Avg Reward = -5.00\n","[INFO] Step 608000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 297         |\n","|    time_elapsed         | 2671        |\n","|    total_timesteps      | 608256      |\n","| train/                  |             |\n","|    approx_kl            | 0.018872552 |\n","|    clip_fraction        | 0.168       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.13       |\n","|    explained_variance   | 0.923       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.05        |\n","|    n_updates            | 2960        |\n","|    policy_gradient_loss | -0.0352     |\n","|    value_loss           | 6.61        |\n","-----------------------------------------\n","[INFO] Step 609000: Avg Reward = -5.00\n","[INFO] Step 610000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 298         |\n","|    time_elapsed         | 2680        |\n","|    total_timesteps      | 610304      |\n","| train/                  |             |\n","|    approx_kl            | 0.009506608 |\n","|    clip_fraction        | 0.103       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.37       |\n","|    explained_variance   | 0.927       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.15        |\n","|    n_updates            | 2970        |\n","|    policy_gradient_loss | -0.024      |\n","|    value_loss           | 12.2        |\n","-----------------------------------------\n","[INFO] Step 611000: Avg Reward = -5.00\n","[INFO] Step 612000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 227        |\n","|    iterations           | 299        |\n","|    time_elapsed         | 2689       |\n","|    total_timesteps      | 612352     |\n","| train/                  |            |\n","|    approx_kl            | 0.02119017 |\n","|    clip_fraction        | 0.181      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.87      |\n","|    explained_variance   | 0.952      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 3.7        |\n","|    n_updates            | 2980       |\n","|    policy_gradient_loss | -0.0324    |\n","|    value_loss           | 6.93       |\n","----------------------------------------\n","[INFO] Step 613000: Avg Reward = -5.00\n","[INFO] Step 614000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 300         |\n","|    time_elapsed         | 2697        |\n","|    total_timesteps      | 614400      |\n","| train/                  |             |\n","|    approx_kl            | 0.018865073 |\n","|    clip_fraction        | 0.176       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.76       |\n","|    explained_variance   | 0.914       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.48        |\n","|    n_updates            | 2990        |\n","|    policy_gradient_loss | -0.0327     |\n","|    value_loss           | 9.1         |\n","-----------------------------------------\n","[INFO] Step 615000: Avg Reward = -5.00\n","[INFO] Step 616000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 301         |\n","|    time_elapsed         | 2706        |\n","|    total_timesteps      | 616448      |\n","| train/                  |             |\n","|    approx_kl            | 0.012745952 |\n","|    clip_fraction        | 0.118       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.34       |\n","|    explained_variance   | 0.904       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 7.29        |\n","|    n_updates            | 3000        |\n","|    policy_gradient_loss | -0.025      |\n","|    value_loss           | 15.3        |\n","-----------------------------------------\n","[INFO] Step 617000: Avg Reward = -5.00\n","[INFO] Step 618000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 227        |\n","|    iterations           | 302        |\n","|    time_elapsed         | 2715       |\n","|    total_timesteps      | 618496     |\n","| train/                  |            |\n","|    approx_kl            | 0.02001796 |\n","|    clip_fraction        | 0.189      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.99      |\n","|    explained_variance   | 0.947      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 4.82       |\n","|    n_updates            | 3010       |\n","|    policy_gradient_loss | -0.0368    |\n","|    value_loss           | 6.86       |\n","----------------------------------------\n","[INFO] Step 619000: Avg Reward = -5.00\n","[INFO] Step 620000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 227        |\n","|    iterations           | 303        |\n","|    time_elapsed         | 2725       |\n","|    total_timesteps      | 620544     |\n","| train/                  |            |\n","|    approx_kl            | 0.01669345 |\n","|    clip_fraction        | 0.183      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.96      |\n","|    explained_variance   | 0.954      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 4          |\n","|    n_updates            | 3020       |\n","|    policy_gradient_loss | -0.0351    |\n","|    value_loss           | 7.47       |\n","----------------------------------------\n","[INFO] Step 621000: Avg Reward = -5.00\n","[INFO] Step 622000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 304         |\n","|    time_elapsed         | 2734        |\n","|    total_timesteps      | 622592      |\n","| train/                  |             |\n","|    approx_kl            | 0.017835239 |\n","|    clip_fraction        | 0.172       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.84       |\n","|    explained_variance   | 0.945       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.54        |\n","|    n_updates            | 3030        |\n","|    policy_gradient_loss | -0.0277     |\n","|    value_loss           | 6.55        |\n","-----------------------------------------\n","[INFO] Step 623000: Avg Reward = -5.00\n","[INFO] Step 624000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 305         |\n","|    time_elapsed         | 2742        |\n","|    total_timesteps      | 624640      |\n","| train/                  |             |\n","|    approx_kl            | 0.014240502 |\n","|    clip_fraction        | 0.14        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.55       |\n","|    explained_variance   | 0.906       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.62        |\n","|    n_updates            | 3040        |\n","|    policy_gradient_loss | -0.0285     |\n","|    value_loss           | 9.76        |\n","-----------------------------------------\n","[INFO] Step 625000: Avg Reward = -5.00\n","[INFO] Step 626000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 306         |\n","|    time_elapsed         | 2751        |\n","|    total_timesteps      | 626688      |\n","| train/                  |             |\n","|    approx_kl            | 0.025302857 |\n","|    clip_fraction        | 0.231       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.14       |\n","|    explained_variance   | 0.933       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.48        |\n","|    n_updates            | 3050        |\n","|    policy_gradient_loss | -0.0332     |\n","|    value_loss           | 5.88        |\n","-----------------------------------------\n","[INFO] Step 627000: Avg Reward = -5.00\n","[INFO] Step 628000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 307         |\n","|    time_elapsed         | 2760        |\n","|    total_timesteps      | 628736      |\n","| train/                  |             |\n","|    approx_kl            | 0.020982794 |\n","|    clip_fraction        | 0.214       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.33       |\n","|    explained_variance   | 0.964       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.9         |\n","|    n_updates            | 3060        |\n","|    policy_gradient_loss | -0.0385     |\n","|    value_loss           | 6.57        |\n","-----------------------------------------\n","[INFO] Step 629000: Avg Reward = -5.00\n","[INFO] Step 630000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 308         |\n","|    time_elapsed         | 2769        |\n","|    total_timesteps      | 630784      |\n","| train/                  |             |\n","|    approx_kl            | 0.015597541 |\n","|    clip_fraction        | 0.172       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.33       |\n","|    explained_variance   | 0.944       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.56        |\n","|    n_updates            | 3070        |\n","|    policy_gradient_loss | -0.037      |\n","|    value_loss           | 13.9        |\n","-----------------------------------------\n","[INFO] Step 631000: Avg Reward = -5.00\n","[INFO] Step 632000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 309         |\n","|    time_elapsed         | 2778        |\n","|    total_timesteps      | 632832      |\n","| train/                  |             |\n","|    approx_kl            | 0.020020217 |\n","|    clip_fraction        | 0.202       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.15       |\n","|    explained_variance   | 0.927       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.59        |\n","|    n_updates            | 3080        |\n","|    policy_gradient_loss | -0.0345     |\n","|    value_loss           | 6.93        |\n","-----------------------------------------\n","[INFO] Step 633000: Avg Reward = -5.00\n","[INFO] Step 634000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 310         |\n","|    time_elapsed         | 2786        |\n","|    total_timesteps      | 634880      |\n","| train/                  |             |\n","|    approx_kl            | 0.022932261 |\n","|    clip_fraction        | 0.25        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.37       |\n","|    explained_variance   | 0.957       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.44        |\n","|    n_updates            | 3090        |\n","|    policy_gradient_loss | -0.0415     |\n","|    value_loss           | 7.17        |\n","-----------------------------------------\n","[INFO] Step 635000: Avg Reward = -5.00\n","[INFO] Step 636000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 311         |\n","|    time_elapsed         | 2795        |\n","|    total_timesteps      | 636928      |\n","| train/                  |             |\n","|    approx_kl            | 0.016492587 |\n","|    clip_fraction        | 0.164       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.79       |\n","|    explained_variance   | 0.899       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.46        |\n","|    n_updates            | 3100        |\n","|    policy_gradient_loss | -0.0307     |\n","|    value_loss           | 7.21        |\n","-----------------------------------------\n","[INFO] Step 637000: Avg Reward = -5.00\n","[INFO] Step 638000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 312         |\n","|    time_elapsed         | 2804        |\n","|    total_timesteps      | 638976      |\n","| train/                  |             |\n","|    approx_kl            | 0.018696643 |\n","|    clip_fraction        | 0.173       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.03       |\n","|    explained_variance   | 0.902       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.62        |\n","|    n_updates            | 3110        |\n","|    policy_gradient_loss | -0.0311     |\n","|    value_loss           | 9.47        |\n","-----------------------------------------\n","[INFO] Step 639000: Avg Reward = -5.00\n","[INFO] Step 640000: Avg Reward = -5.00\n","[INFO] Step 641000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 313         |\n","|    time_elapsed         | 2812        |\n","|    total_timesteps      | 641024      |\n","| train/                  |             |\n","|    approx_kl            | 0.014400983 |\n","|    clip_fraction        | 0.171       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.26       |\n","|    explained_variance   | 0.941       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.24        |\n","|    n_updates            | 3120        |\n","|    policy_gradient_loss | -0.0347     |\n","|    value_loss           | 10.6        |\n","-----------------------------------------\n","[INFO] Step 642000: Avg Reward = -5.00\n","[INFO] Step 643000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 314         |\n","|    time_elapsed         | 2821        |\n","|    total_timesteps      | 643072      |\n","| train/                  |             |\n","|    approx_kl            | 0.018336851 |\n","|    clip_fraction        | 0.191       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.03       |\n","|    explained_variance   | 0.918       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.24        |\n","|    n_updates            | 3130        |\n","|    policy_gradient_loss | -0.0259     |\n","|    value_loss           | 12.2        |\n","-----------------------------------------\n","[INFO] Step 644000: Avg Reward = -5.00\n","[INFO] Step 645000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 315         |\n","|    time_elapsed         | 2829        |\n","|    total_timesteps      | 645120      |\n","| train/                  |             |\n","|    approx_kl            | 0.017373096 |\n","|    clip_fraction        | 0.185       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.56       |\n","|    explained_variance   | 0.911       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.67        |\n","|    n_updates            | 3140        |\n","|    policy_gradient_loss | -0.0306     |\n","|    value_loss           | 7.22        |\n","-----------------------------------------\n","[INFO] Step 646000: Avg Reward = -5.00\n","[INFO] Step 647000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 316         |\n","|    time_elapsed         | 2839        |\n","|    total_timesteps      | 647168      |\n","| train/                  |             |\n","|    approx_kl            | 0.023182832 |\n","|    clip_fraction        | 0.207       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.18       |\n","|    explained_variance   | 0.96        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.15        |\n","|    n_updates            | 3150        |\n","|    policy_gradient_loss | -0.0359     |\n","|    value_loss           | 5.7         |\n","-----------------------------------------\n","[INFO] Step 648000: Avg Reward = -5.00\n","[INFO] Step 649000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 317         |\n","|    time_elapsed         | 2848        |\n","|    total_timesteps      | 649216      |\n","| train/                  |             |\n","|    approx_kl            | 0.020452186 |\n","|    clip_fraction        | 0.205       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.41       |\n","|    explained_variance   | 0.915       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.96        |\n","|    n_updates            | 3160        |\n","|    policy_gradient_loss | -0.036      |\n","|    value_loss           | 10.5        |\n","-----------------------------------------\n","[INFO] Step 650000: Avg Reward = -5.00\n","[INFO] Step 651000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 227        |\n","|    iterations           | 318        |\n","|    time_elapsed         | 2856       |\n","|    total_timesteps      | 651264     |\n","| train/                  |            |\n","|    approx_kl            | 0.01822279 |\n","|    clip_fraction        | 0.193      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -3.6       |\n","|    explained_variance   | 0.962      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 2.61       |\n","|    n_updates            | 3170       |\n","|    policy_gradient_loss | -0.0376    |\n","|    value_loss           | 6.09       |\n","----------------------------------------\n","[INFO] Step 652000: Avg Reward = -5.00\n","[INFO] Step 653000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 319         |\n","|    time_elapsed         | 2866        |\n","|    total_timesteps      | 653312      |\n","| train/                  |             |\n","|    approx_kl            | 0.020740319 |\n","|    clip_fraction        | 0.233       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.86       |\n","|    explained_variance   | 0.932       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.44        |\n","|    n_updates            | 3180        |\n","|    policy_gradient_loss | -0.0368     |\n","|    value_loss           | 10.8        |\n","-----------------------------------------\n","[INFO] Step 654000: Avg Reward = -5.00\n","[INFO] Step 655000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 320         |\n","|    time_elapsed         | 2873        |\n","|    total_timesteps      | 655360      |\n","| train/                  |             |\n","|    approx_kl            | 0.020520926 |\n","|    clip_fraction        | 0.245       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.98       |\n","|    explained_variance   | 0.939       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.85        |\n","|    n_updates            | 3190        |\n","|    policy_gradient_loss | -0.0375     |\n","|    value_loss           | 7.09        |\n","-----------------------------------------\n","[INFO] Step 656000: Avg Reward = -5.00\n","[INFO] Step 657000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 321         |\n","|    time_elapsed         | 2883        |\n","|    total_timesteps      | 657408      |\n","| train/                  |             |\n","|    approx_kl            | 0.020592142 |\n","|    clip_fraction        | 0.238       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.86       |\n","|    explained_variance   | 0.896       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.88        |\n","|    n_updates            | 3200        |\n","|    policy_gradient_loss | -0.034      |\n","|    value_loss           | 8.92        |\n","-----------------------------------------\n","[INFO] Step 658000: Avg Reward = -5.00\n","[INFO] Step 659000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 322         |\n","|    time_elapsed         | 2892        |\n","|    total_timesteps      | 659456      |\n","| train/                  |             |\n","|    approx_kl            | 0.024445273 |\n","|    clip_fraction        | 0.229       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.69       |\n","|    explained_variance   | 0.952       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.69        |\n","|    n_updates            | 3210        |\n","|    policy_gradient_loss | -0.0393     |\n","|    value_loss           | 8.08        |\n","-----------------------------------------\n","[INFO] Step 660000: Avg Reward = -5.00\n","[INFO] Step 661000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 323         |\n","|    time_elapsed         | 2900        |\n","|    total_timesteps      | 661504      |\n","| train/                  |             |\n","|    approx_kl            | 0.015777513 |\n","|    clip_fraction        | 0.161       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.47       |\n","|    explained_variance   | 0.955       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.08        |\n","|    n_updates            | 3220        |\n","|    policy_gradient_loss | -0.0297     |\n","|    value_loss           | 8.94        |\n","-----------------------------------------\n","[INFO] Step 662000: Avg Reward = -5.00\n","[INFO] Step 663000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 324         |\n","|    time_elapsed         | 2909        |\n","|    total_timesteps      | 663552      |\n","| train/                  |             |\n","|    approx_kl            | 0.024062237 |\n","|    clip_fraction        | 0.197       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.83       |\n","|    explained_variance   | 0.963       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.16        |\n","|    n_updates            | 3230        |\n","|    policy_gradient_loss | -0.0386     |\n","|    value_loss           | 7.98        |\n","-----------------------------------------\n","[INFO] Step 664000: Avg Reward = -5.00\n","[INFO] Step 665000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 228        |\n","|    iterations           | 325        |\n","|    time_elapsed         | 2918       |\n","|    total_timesteps      | 665600     |\n","| train/                  |            |\n","|    approx_kl            | 0.01829939 |\n","|    clip_fraction        | 0.21       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -3.49      |\n","|    explained_variance   | 0.948      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 5.58       |\n","|    n_updates            | 3240       |\n","|    policy_gradient_loss | -0.0328    |\n","|    value_loss           | 12         |\n","----------------------------------------\n","[INFO] Step 666000: Avg Reward = -5.00\n","[INFO] Step 667000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 326         |\n","|    time_elapsed         | 2926        |\n","|    total_timesteps      | 667648      |\n","| train/                  |             |\n","|    approx_kl            | 0.020581169 |\n","|    clip_fraction        | 0.226       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.12       |\n","|    explained_variance   | 0.978       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.54        |\n","|    n_updates            | 3250        |\n","|    policy_gradient_loss | -0.0309     |\n","|    value_loss           | 4.27        |\n","-----------------------------------------\n","[INFO] Step 668000: Avg Reward = -5.00\n","[INFO] Step 669000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 327         |\n","|    time_elapsed         | 2936        |\n","|    total_timesteps      | 669696      |\n","| train/                  |             |\n","|    approx_kl            | 0.017579684 |\n","|    clip_fraction        | 0.205       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.18       |\n","|    explained_variance   | 0.928       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.02        |\n","|    n_updates            | 3260        |\n","|    policy_gradient_loss | -0.0315     |\n","|    value_loss           | 11.9        |\n","-----------------------------------------\n","[INFO] Step 670000: Avg Reward = -5.00\n","[INFO] Step 671000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 228        |\n","|    iterations           | 328        |\n","|    time_elapsed         | 2944       |\n","|    total_timesteps      | 671744     |\n","| train/                  |            |\n","|    approx_kl            | 0.01739012 |\n","|    clip_fraction        | 0.182      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -3.52      |\n","|    explained_variance   | 0.961      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 4.36       |\n","|    n_updates            | 3270       |\n","|    policy_gradient_loss | -0.0344    |\n","|    value_loss           | 8.98       |\n","----------------------------------------\n","[INFO] Step 672000: Avg Reward = -5.00\n","[INFO] Step 673000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 329         |\n","|    time_elapsed         | 2953        |\n","|    total_timesteps      | 673792      |\n","| train/                  |             |\n","|    approx_kl            | 0.020302078 |\n","|    clip_fraction        | 0.216       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.42       |\n","|    explained_variance   | 0.924       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.85        |\n","|    n_updates            | 3280        |\n","|    policy_gradient_loss | -0.0406     |\n","|    value_loss           | 10.5        |\n","-----------------------------------------\n","[INFO] Step 674000: Avg Reward = -5.00\n","[INFO] Step 675000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 228        |\n","|    iterations           | 330        |\n","|    time_elapsed         | 2962       |\n","|    total_timesteps      | 675840     |\n","| train/                  |            |\n","|    approx_kl            | 0.02048226 |\n","|    clip_fraction        | 0.218      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -3.56      |\n","|    explained_variance   | 0.967      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 4.92       |\n","|    n_updates            | 3290       |\n","|    policy_gradient_loss | -0.0397    |\n","|    value_loss           | 8.8        |\n","----------------------------------------\n","[INFO] Step 676000: Avg Reward = -5.00\n","[INFO] Step 677000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 331         |\n","|    time_elapsed         | 2971        |\n","|    total_timesteps      | 677888      |\n","| train/                  |             |\n","|    approx_kl            | 0.019594874 |\n","|    clip_fraction        | 0.193       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.49       |\n","|    explained_variance   | 0.949       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.72        |\n","|    n_updates            | 3300        |\n","|    policy_gradient_loss | -0.0367     |\n","|    value_loss           | 8.71        |\n","-----------------------------------------\n","[INFO] Step 678000: Avg Reward = -5.00\n","[INFO] Step 679000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 332         |\n","|    time_elapsed         | 2980        |\n","|    total_timesteps      | 679936      |\n","| train/                  |             |\n","|    approx_kl            | 0.018250674 |\n","|    clip_fraction        | 0.196       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.91       |\n","|    explained_variance   | 0.938       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.14        |\n","|    n_updates            | 3310        |\n","|    policy_gradient_loss | -0.0318     |\n","|    value_loss           | 7.55        |\n","-----------------------------------------\n","[INFO] Step 680000: Avg Reward = -5.00\n","[INFO] Step 681000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 333         |\n","|    time_elapsed         | 2988        |\n","|    total_timesteps      | 681984      |\n","| train/                  |             |\n","|    approx_kl            | 0.026393589 |\n","|    clip_fraction        | 0.236       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.94       |\n","|    explained_variance   | 0.949       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.01        |\n","|    n_updates            | 3320        |\n","|    policy_gradient_loss | -0.0351     |\n","|    value_loss           | 5.12        |\n","-----------------------------------------\n","[INFO] Step 682000: Avg Reward = -5.00\n","[INFO] Step 683000: Avg Reward = -5.00\n","[INFO] Step 684000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 334         |\n","|    time_elapsed         | 2997        |\n","|    total_timesteps      | 684032      |\n","| train/                  |             |\n","|    approx_kl            | 0.017425627 |\n","|    clip_fraction        | 0.2         |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.51       |\n","|    explained_variance   | 0.953       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.22        |\n","|    n_updates            | 3330        |\n","|    policy_gradient_loss | -0.0342     |\n","|    value_loss           | 8.61        |\n","-----------------------------------------\n","[INFO] Step 685000: Avg Reward = -5.00\n","[INFO] Step 686000: Avg Reward = 11.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 335         |\n","|    time_elapsed         | 3007        |\n","|    total_timesteps      | 686080      |\n","| train/                  |             |\n","|    approx_kl            | 0.020999335 |\n","|    clip_fraction        | 0.203       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.27       |\n","|    explained_variance   | 0.957       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.19        |\n","|    n_updates            | 3340        |\n","|    policy_gradient_loss | -0.0348     |\n","|    value_loss           | 4.97        |\n","-----------------------------------------\n","[INFO] Step 687000: Avg Reward = -5.00\n","[INFO] Step 688000: Avg Reward = -5.00\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 228          |\n","|    iterations           | 336          |\n","|    time_elapsed         | 3015         |\n","|    total_timesteps      | 688128       |\n","| train/                  |              |\n","|    approx_kl            | 0.0129524395 |\n","|    clip_fraction        | 0.119        |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -2.49        |\n","|    explained_variance   | 0.932        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 7.11         |\n","|    n_updates            | 3350         |\n","|    policy_gradient_loss | -0.0292      |\n","|    value_loss           | 13.4         |\n","------------------------------------------\n","[INFO] Step 689000: Avg Reward = -5.00\n","[INFO] Step 690000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 228        |\n","|    iterations           | 337        |\n","|    time_elapsed         | 3025       |\n","|    total_timesteps      | 690176     |\n","| train/                  |            |\n","|    approx_kl            | 0.01626017 |\n","|    clip_fraction        | 0.199      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -3.37      |\n","|    explained_variance   | 0.92       |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 5.84       |\n","|    n_updates            | 3360       |\n","|    policy_gradient_loss | -0.0313    |\n","|    value_loss           | 8.06       |\n","----------------------------------------\n","[INFO] Step 691000: Avg Reward = -5.00\n","[INFO] Step 692000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 338         |\n","|    time_elapsed         | 3034        |\n","|    total_timesteps      | 692224      |\n","| train/                  |             |\n","|    approx_kl            | 0.021948416 |\n","|    clip_fraction        | 0.228       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.62       |\n","|    explained_variance   | 0.936       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.56        |\n","|    n_updates            | 3370        |\n","|    policy_gradient_loss | -0.0342     |\n","|    value_loss           | 7.32        |\n","-----------------------------------------\n","[INFO] Step 693000: Avg Reward = -5.00\n","[INFO] Step 694000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 339         |\n","|    time_elapsed         | 3043        |\n","|    total_timesteps      | 694272      |\n","| train/                  |             |\n","|    approx_kl            | 0.014440273 |\n","|    clip_fraction        | 0.172       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.13       |\n","|    explained_variance   | 0.91        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.34        |\n","|    n_updates            | 3380        |\n","|    policy_gradient_loss | -0.0311     |\n","|    value_loss           | 9.84        |\n","-----------------------------------------\n","[INFO] Step 695000: Avg Reward = -5.00\n","[INFO] Step 696000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 340         |\n","|    time_elapsed         | 3053        |\n","|    total_timesteps      | 696320      |\n","| train/                  |             |\n","|    approx_kl            | 0.020553898 |\n","|    clip_fraction        | 0.185       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.14       |\n","|    explained_variance   | 0.924       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.31        |\n","|    n_updates            | 3390        |\n","|    policy_gradient_loss | -0.0382     |\n","|    value_loss           | 11.6        |\n","-----------------------------------------\n","[INFO] Step 697000: Avg Reward = -5.00\n","[INFO] Step 698000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 341         |\n","|    time_elapsed         | 3061        |\n","|    total_timesteps      | 698368      |\n","| train/                  |             |\n","|    approx_kl            | 0.018341582 |\n","|    clip_fraction        | 0.187       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.13       |\n","|    explained_variance   | 0.937       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.93        |\n","|    n_updates            | 3400        |\n","|    policy_gradient_loss | -0.0329     |\n","|    value_loss           | 8.6         |\n","-----------------------------------------\n","[INFO] Step 699000: Avg Reward = -5.00\n","[INFO] Step 700000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 342         |\n","|    time_elapsed         | 3070        |\n","|    total_timesteps      | 700416      |\n","| train/                  |             |\n","|    approx_kl            | 0.020089377 |\n","|    clip_fraction        | 0.223       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.52       |\n","|    explained_variance   | 0.915       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.56        |\n","|    n_updates            | 3410        |\n","|    policy_gradient_loss | -0.0366     |\n","|    value_loss           | 9.46        |\n","-----------------------------------------\n","[INFO] Step 701000: Avg Reward = -5.00\n","[INFO] Step 702000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 343         |\n","|    time_elapsed         | 3080        |\n","|    total_timesteps      | 702464      |\n","| train/                  |             |\n","|    approx_kl            | 0.015809357 |\n","|    clip_fraction        | 0.192       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.42       |\n","|    explained_variance   | 0.948       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.16        |\n","|    n_updates            | 3420        |\n","|    policy_gradient_loss | -0.0307     |\n","|    value_loss           | 7.89        |\n","-----------------------------------------\n","[INFO] Step 703000: Avg Reward = -5.00\n","[INFO] Step 704000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 344         |\n","|    time_elapsed         | 3088        |\n","|    total_timesteps      | 704512      |\n","| train/                  |             |\n","|    approx_kl            | 0.021515878 |\n","|    clip_fraction        | 0.213       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.19       |\n","|    explained_variance   | 0.947       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.96        |\n","|    n_updates            | 3430        |\n","|    policy_gradient_loss | -0.0396     |\n","|    value_loss           | 6.04        |\n","-----------------------------------------\n","[INFO] Step 705000: Avg Reward = -5.00\n","[INFO] Step 706000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 345         |\n","|    time_elapsed         | 3098        |\n","|    total_timesteps      | 706560      |\n","| train/                  |             |\n","|    approx_kl            | 0.015697641 |\n","|    clip_fraction        | 0.166       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.23       |\n","|    explained_variance   | 0.95        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.74        |\n","|    n_updates            | 3440        |\n","|    policy_gradient_loss | -0.0346     |\n","|    value_loss           | 7.66        |\n","-----------------------------------------\n","[INFO] Step 707000: Avg Reward = -5.00\n","[INFO] Step 708000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 346         |\n","|    time_elapsed         | 3106        |\n","|    total_timesteps      | 708608      |\n","| train/                  |             |\n","|    approx_kl            | 0.020224597 |\n","|    clip_fraction        | 0.18        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.84       |\n","|    explained_variance   | 0.927       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.58        |\n","|    n_updates            | 3450        |\n","|    policy_gradient_loss | -0.038      |\n","|    value_loss           | 7.92        |\n","-----------------------------------------\n","[INFO] Step 709000: Avg Reward = -5.00\n","[INFO] Step 710000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 347         |\n","|    time_elapsed         | 3115        |\n","|    total_timesteps      | 710656      |\n","| train/                  |             |\n","|    approx_kl            | 0.018050127 |\n","|    clip_fraction        | 0.213       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.43       |\n","|    explained_variance   | 0.89        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.06        |\n","|    n_updates            | 3460        |\n","|    policy_gradient_loss | -0.0316     |\n","|    value_loss           | 9.67        |\n","-----------------------------------------\n","[INFO] Step 711000: Avg Reward = -5.00\n","[INFO] Step 712000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 228        |\n","|    iterations           | 348        |\n","|    time_elapsed         | 3125       |\n","|    total_timesteps      | 712704     |\n","| train/                  |            |\n","|    approx_kl            | 0.01828068 |\n","|    clip_fraction        | 0.19       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -3.22      |\n","|    explained_variance   | 0.888      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 4.56       |\n","|    n_updates            | 3470       |\n","|    policy_gradient_loss | -0.0281    |\n","|    value_loss           | 10.3       |\n","----------------------------------------\n","[INFO] Step 713000: Avg Reward = -5.00\n","[INFO] Step 714000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 349         |\n","|    time_elapsed         | 3133        |\n","|    total_timesteps      | 714752      |\n","| train/                  |             |\n","|    approx_kl            | 0.019019453 |\n","|    clip_fraction        | 0.206       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.55       |\n","|    explained_variance   | 0.946       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.17        |\n","|    n_updates            | 3480        |\n","|    policy_gradient_loss | -0.0412     |\n","|    value_loss           | 8.05        |\n","-----------------------------------------\n","[INFO] Step 715000: Avg Reward = -5.00\n","[INFO] Step 716000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 350         |\n","|    time_elapsed         | 3142        |\n","|    total_timesteps      | 716800      |\n","| train/                  |             |\n","|    approx_kl            | 0.018180477 |\n","|    clip_fraction        | 0.163       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.86       |\n","|    explained_variance   | 0.942       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.51        |\n","|    n_updates            | 3490        |\n","|    policy_gradient_loss | -0.0304     |\n","|    value_loss           | 7.6         |\n","-----------------------------------------\n","[INFO] Step 717000: Avg Reward = -5.00\n","[INFO] Step 718000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 351         |\n","|    time_elapsed         | 3152        |\n","|    total_timesteps      | 718848      |\n","| train/                  |             |\n","|    approx_kl            | 0.022982474 |\n","|    clip_fraction        | 0.178       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.76       |\n","|    explained_variance   | 0.95        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.84        |\n","|    n_updates            | 3500        |\n","|    policy_gradient_loss | -0.0322     |\n","|    value_loss           | 6.53        |\n","-----------------------------------------\n","[INFO] Step 719000: Avg Reward = -5.00\n","[INFO] Step 720000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 352         |\n","|    time_elapsed         | 3160        |\n","|    total_timesteps      | 720896      |\n","| train/                  |             |\n","|    approx_kl            | 0.013988249 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.6        |\n","|    explained_variance   | 0.908       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.27        |\n","|    n_updates            | 3510        |\n","|    policy_gradient_loss | -0.0307     |\n","|    value_loss           | 12.7        |\n","-----------------------------------------\n","[INFO] Step 721000: Avg Reward = -5.00\n","[INFO] Step 722000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 353         |\n","|    time_elapsed         | 3170        |\n","|    total_timesteps      | 722944      |\n","| train/                  |             |\n","|    approx_kl            | 0.020481676 |\n","|    clip_fraction        | 0.172       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.85       |\n","|    explained_variance   | 0.939       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.98        |\n","|    n_updates            | 3520        |\n","|    policy_gradient_loss | -0.0273     |\n","|    value_loss           | 9.37        |\n","-----------------------------------------\n","[INFO] Step 723000: Avg Reward = -5.00\n","[INFO] Step 724000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 354         |\n","|    time_elapsed         | 3178        |\n","|    total_timesteps      | 724992      |\n","| train/                  |             |\n","|    approx_kl            | 0.012128118 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.42       |\n","|    explained_variance   | 0.922       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.25        |\n","|    n_updates            | 3530        |\n","|    policy_gradient_loss | -0.027      |\n","|    value_loss           | 9.49        |\n","-----------------------------------------\n","[INFO] Step 725000: Avg Reward = -5.00\n","[INFO] Step 726000: Avg Reward = -5.00\n","[INFO] Step 727000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 355         |\n","|    time_elapsed         | 3187        |\n","|    total_timesteps      | 727040      |\n","| train/                  |             |\n","|    approx_kl            | 0.018117901 |\n","|    clip_fraction        | 0.183       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.08       |\n","|    explained_variance   | 0.944       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.61        |\n","|    n_updates            | 3540        |\n","|    policy_gradient_loss | -0.0285     |\n","|    value_loss           | 6.66        |\n","-----------------------------------------\n","[INFO] Step 728000: Avg Reward = -5.00\n","[INFO] Step 729000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 356         |\n","|    time_elapsed         | 3196        |\n","|    total_timesteps      | 729088      |\n","| train/                  |             |\n","|    approx_kl            | 0.016557656 |\n","|    clip_fraction        | 0.184       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.93       |\n","|    explained_variance   | 0.879       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.32        |\n","|    n_updates            | 3550        |\n","|    policy_gradient_loss | -0.0297     |\n","|    value_loss           | 11.5        |\n","-----------------------------------------\n","[INFO] Step 730000: Avg Reward = -5.00\n","[INFO] Step 731000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 357         |\n","|    time_elapsed         | 3204        |\n","|    total_timesteps      | 731136      |\n","| train/                  |             |\n","|    approx_kl            | 0.027642706 |\n","|    clip_fraction        | 0.247       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.91       |\n","|    explained_variance   | 0.95        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.97        |\n","|    n_updates            | 3560        |\n","|    policy_gradient_loss | -0.0386     |\n","|    value_loss           | 5.44        |\n","-----------------------------------------\n","[INFO] Step 732000: Avg Reward = -5.00\n","[INFO] Step 733000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 358         |\n","|    time_elapsed         | 3213        |\n","|    total_timesteps      | 733184      |\n","| train/                  |             |\n","|    approx_kl            | 0.012968312 |\n","|    clip_fraction        | 0.124       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.51       |\n","|    explained_variance   | 0.927       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.78        |\n","|    n_updates            | 3570        |\n","|    policy_gradient_loss | -0.0257     |\n","|    value_loss           | 4.96        |\n","-----------------------------------------\n","[INFO] Step 734000: Avg Reward = -5.00\n","[INFO] Step 735000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 359         |\n","|    time_elapsed         | 3222        |\n","|    total_timesteps      | 735232      |\n","| train/                  |             |\n","|    approx_kl            | 0.013330584 |\n","|    clip_fraction        | 0.147       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.42       |\n","|    explained_variance   | 0.953       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.8         |\n","|    n_updates            | 3580        |\n","|    policy_gradient_loss | -0.0322     |\n","|    value_loss           | 6.62        |\n","-----------------------------------------\n","[INFO] Step 736000: Avg Reward = -5.00\n","[INFO] Step 737000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 360         |\n","|    time_elapsed         | 3231        |\n","|    total_timesteps      | 737280      |\n","| train/                  |             |\n","|    approx_kl            | 0.018828232 |\n","|    clip_fraction        | 0.195       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.24       |\n","|    explained_variance   | 0.949       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.72        |\n","|    n_updates            | 3590        |\n","|    policy_gradient_loss | -0.0357     |\n","|    value_loss           | 9.5         |\n","-----------------------------------------\n","[INFO] Step 738000: Avg Reward = -5.00\n","[INFO] Step 739000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 361         |\n","|    time_elapsed         | 3240        |\n","|    total_timesteps      | 739328      |\n","| train/                  |             |\n","|    approx_kl            | 0.015252892 |\n","|    clip_fraction        | 0.171       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.03       |\n","|    explained_variance   | 0.941       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.34        |\n","|    n_updates            | 3600        |\n","|    policy_gradient_loss | -0.0322     |\n","|    value_loss           | 8.62        |\n","-----------------------------------------\n","[INFO] Step 740000: Avg Reward = -5.00\n","[INFO] Step 741000: Avg Reward = 11.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 362         |\n","|    time_elapsed         | 3248        |\n","|    total_timesteps      | 741376      |\n","| train/                  |             |\n","|    approx_kl            | 0.018360095 |\n","|    clip_fraction        | 0.161       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.54       |\n","|    explained_variance   | 0.942       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.37        |\n","|    n_updates            | 3610        |\n","|    policy_gradient_loss | -0.0314     |\n","|    value_loss           | 5.87        |\n","-----------------------------------------\n","[INFO] Step 742000: Avg Reward = -5.00\n","[INFO] Step 743000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 363         |\n","|    time_elapsed         | 3257        |\n","|    total_timesteps      | 743424      |\n","| train/                  |             |\n","|    approx_kl            | 0.017804872 |\n","|    clip_fraction        | 0.21        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.88       |\n","|    explained_variance   | 0.915       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.61        |\n","|    n_updates            | 3620        |\n","|    policy_gradient_loss | -0.0275     |\n","|    value_loss           | 4.72        |\n","-----------------------------------------\n","[INFO] Step 744000: Avg Reward = -5.00\n","[INFO] Step 745000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 364         |\n","|    time_elapsed         | 3266        |\n","|    total_timesteps      | 745472      |\n","| train/                  |             |\n","|    approx_kl            | 0.012498199 |\n","|    clip_fraction        | 0.131       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.52       |\n","|    explained_variance   | 0.957       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.52        |\n","|    n_updates            | 3630        |\n","|    policy_gradient_loss | -0.0304     |\n","|    value_loss           | 8.08        |\n","-----------------------------------------\n","[INFO] Step 746000: Avg Reward = -5.00\n","[INFO] Step 747000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 365         |\n","|    time_elapsed         | 3275        |\n","|    total_timesteps      | 747520      |\n","| train/                  |             |\n","|    approx_kl            | 0.012769771 |\n","|    clip_fraction        | 0.144       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.63       |\n","|    explained_variance   | 0.941       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.04        |\n","|    n_updates            | 3640        |\n","|    policy_gradient_loss | -0.033      |\n","|    value_loss           | 7.92        |\n","-----------------------------------------\n","[INFO] Step 748000: Avg Reward = -5.00\n","[INFO] Step 749000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 366         |\n","|    time_elapsed         | 3284        |\n","|    total_timesteps      | 749568      |\n","| train/                  |             |\n","|    approx_kl            | 0.016019065 |\n","|    clip_fraction        | 0.135       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.04       |\n","|    explained_variance   | 0.946       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.856       |\n","|    n_updates            | 3650        |\n","|    policy_gradient_loss | -0.0333     |\n","|    value_loss           | 6.2         |\n","-----------------------------------------\n","[INFO] Step 750000: Avg Reward = -5.00\n","[INFO] Step 751000: Avg Reward = -5.00\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 228       |\n","|    iterations           | 367       |\n","|    time_elapsed         | 3292      |\n","|    total_timesteps      | 751616    |\n","| train/                  |           |\n","|    approx_kl            | 0.0203726 |\n","|    clip_fraction        | 0.181     |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -2.63     |\n","|    explained_variance   | 0.964     |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | 2.98      |\n","|    n_updates            | 3660      |\n","|    policy_gradient_loss | -0.0382   |\n","|    value_loss           | 5.3       |\n","---------------------------------------\n","[INFO] Step 752000: Avg Reward = -5.00\n","[INFO] Step 753000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 368         |\n","|    time_elapsed         | 3302        |\n","|    total_timesteps      | 753664      |\n","| train/                  |             |\n","|    approx_kl            | 0.029058542 |\n","|    clip_fraction        | 0.212       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.22       |\n","|    explained_variance   | 0.917       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.31        |\n","|    n_updates            | 3670        |\n","|    policy_gradient_loss | -0.0298     |\n","|    value_loss           | 7.74        |\n","-----------------------------------------\n","[INFO] Step 754000: Avg Reward = -5.00\n","[INFO] Step 755000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 369         |\n","|    time_elapsed         | 3311        |\n","|    total_timesteps      | 755712      |\n","| train/                  |             |\n","|    approx_kl            | 0.013005972 |\n","|    clip_fraction        | 0.186       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.94       |\n","|    explained_variance   | 0.947       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.86        |\n","|    n_updates            | 3680        |\n","|    policy_gradient_loss | -0.028      |\n","|    value_loss           | 7.81        |\n","-----------------------------------------\n","[INFO] Step 756000: Avg Reward = -5.00\n","[INFO] Step 757000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 228        |\n","|    iterations           | 370        |\n","|    time_elapsed         | 3319       |\n","|    total_timesteps      | 757760     |\n","| train/                  |            |\n","|    approx_kl            | 0.01996126 |\n","|    clip_fraction        | 0.203      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.82      |\n","|    explained_variance   | 0.955      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.41       |\n","|    n_updates            | 3690       |\n","|    policy_gradient_loss | -0.038     |\n","|    value_loss           | 5.44       |\n","----------------------------------------\n","[INFO] Step 758000: Avg Reward = -5.00\n","[INFO] Step 759000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 371         |\n","|    time_elapsed         | 3329        |\n","|    total_timesteps      | 759808      |\n","| train/                  |             |\n","|    approx_kl            | 0.037577555 |\n","|    clip_fraction        | 0.27        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.61       |\n","|    explained_variance   | 0.956       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.34        |\n","|    n_updates            | 3700        |\n","|    policy_gradient_loss | -0.0417     |\n","|    value_loss           | 3.77        |\n","-----------------------------------------\n","[INFO] Step 760000: Avg Reward = -5.00\n","[INFO] Step 761000: Avg Reward = -5.00\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 228       |\n","|    iterations           | 372       |\n","|    time_elapsed         | 3338      |\n","|    total_timesteps      | 761856    |\n","| train/                  |           |\n","|    approx_kl            | 0.0188451 |\n","|    clip_fraction        | 0.145     |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -1.98     |\n","|    explained_variance   | 0.917     |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | 3.61      |\n","|    n_updates            | 3710      |\n","|    policy_gradient_loss | -0.0279   |\n","|    value_loss           | 6.03      |\n","---------------------------------------\n","[INFO] Step 762000: Avg Reward = -5.00\n","[INFO] Step 763000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 373         |\n","|    time_elapsed         | 3346        |\n","|    total_timesteps      | 763904      |\n","| train/                  |             |\n","|    approx_kl            | 0.026395332 |\n","|    clip_fraction        | 0.155       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.15       |\n","|    explained_variance   | 0.957       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.87        |\n","|    n_updates            | 3720        |\n","|    policy_gradient_loss | -0.0299     |\n","|    value_loss           | 4.53        |\n","-----------------------------------------\n","[INFO] Step 764000: Avg Reward = -5.00\n","[INFO] Step 765000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 374         |\n","|    time_elapsed         | 3356        |\n","|    total_timesteps      | 765952      |\n","| train/                  |             |\n","|    approx_kl            | 0.016503498 |\n","|    clip_fraction        | 0.185       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.21       |\n","|    explained_variance   | 0.949       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.909       |\n","|    n_updates            | 3730        |\n","|    policy_gradient_loss | -0.0249     |\n","|    value_loss           | 5.06        |\n","-----------------------------------------\n","[INFO] Step 766000: Avg Reward = -5.00\n","[INFO] Step 767000: Avg Reward = -5.00\n","[INFO] Step 768000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 375         |\n","|    time_elapsed         | 3363        |\n","|    total_timesteps      | 768000      |\n","| train/                  |             |\n","|    approx_kl            | 0.022084804 |\n","|    clip_fraction        | 0.199       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -3.19       |\n","|    explained_variance   | 0.936       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.04        |\n","|    n_updates            | 3740        |\n","|    policy_gradient_loss | -0.0326     |\n","|    value_loss           | 9.03        |\n","-----------------------------------------\n","[INFO] Step 769000: Avg Reward = -5.00\n","[INFO] Step 770000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 376         |\n","|    time_elapsed         | 3373        |\n","|    total_timesteps      | 770048      |\n","| train/                  |             |\n","|    approx_kl            | 0.018358508 |\n","|    clip_fraction        | 0.177       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.75       |\n","|    explained_variance   | 0.941       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.5         |\n","|    n_updates            | 3750        |\n","|    policy_gradient_loss | -0.0333     |\n","|    value_loss           | 7.13        |\n","-----------------------------------------\n","[INFO] Step 771000: Avg Reward = -5.00\n","[INFO] Step 772000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 228        |\n","|    iterations           | 377        |\n","|    time_elapsed         | 3382       |\n","|    total_timesteps      | 772096     |\n","| train/                  |            |\n","|    approx_kl            | 0.01643387 |\n","|    clip_fraction        | 0.164      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.48      |\n","|    explained_variance   | 0.924      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 4.25       |\n","|    n_updates            | 3760       |\n","|    policy_gradient_loss | -0.0269    |\n","|    value_loss           | 8.35       |\n","----------------------------------------\n","[INFO] Step 773000: Avg Reward = -5.00\n","[INFO] Step 774000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 378         |\n","|    time_elapsed         | 3390        |\n","|    total_timesteps      | 774144      |\n","| train/                  |             |\n","|    approx_kl            | 0.016119514 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.41       |\n","|    explained_variance   | 0.924       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.57        |\n","|    n_updates            | 3770        |\n","|    policy_gradient_loss | -0.0281     |\n","|    value_loss           | 6.2         |\n","-----------------------------------------\n","[INFO] Step 775000: Avg Reward = -5.00\n","[INFO] Step 776000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 379         |\n","|    time_elapsed         | 3400        |\n","|    total_timesteps      | 776192      |\n","| train/                  |             |\n","|    approx_kl            | 0.027014678 |\n","|    clip_fraction        | 0.198       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.38       |\n","|    explained_variance   | 0.937       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.39        |\n","|    n_updates            | 3780        |\n","|    policy_gradient_loss | -0.0229     |\n","|    value_loss           | 3.79        |\n","-----------------------------------------\n","[INFO] Step 777000: Avg Reward = -5.00\n","[INFO] Step 778000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 380         |\n","|    time_elapsed         | 3407        |\n","|    total_timesteps      | 778240      |\n","| train/                  |             |\n","|    approx_kl            | 0.013552038 |\n","|    clip_fraction        | 0.156       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.44       |\n","|    explained_variance   | 0.901       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.59        |\n","|    n_updates            | 3790        |\n","|    policy_gradient_loss | -0.0248     |\n","|    value_loss           | 9.31        |\n","-----------------------------------------\n","[INFO] Step 779000: Avg Reward = -5.00\n","[INFO] Step 780000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 381         |\n","|    time_elapsed         | 3416        |\n","|    total_timesteps      | 780288      |\n","| train/                  |             |\n","|    approx_kl            | 0.027402826 |\n","|    clip_fraction        | 0.186       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.22       |\n","|    explained_variance   | 0.909       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.52        |\n","|    n_updates            | 3800        |\n","|    policy_gradient_loss | -0.0345     |\n","|    value_loss           | 10.6        |\n","-----------------------------------------\n","[INFO] Step 781000: Avg Reward = -5.00\n","[INFO] Step 782000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 382         |\n","|    time_elapsed         | 3426        |\n","|    total_timesteps      | 782336      |\n","| train/                  |             |\n","|    approx_kl            | 0.022727411 |\n","|    clip_fraction        | 0.216       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.67       |\n","|    explained_variance   | 0.956       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.46        |\n","|    n_updates            | 3810        |\n","|    policy_gradient_loss | -0.0352     |\n","|    value_loss           | 7.13        |\n","-----------------------------------------\n","[INFO] Step 783000: Avg Reward = -5.00\n","[INFO] Step 784000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 383         |\n","|    time_elapsed         | 3434        |\n","|    total_timesteps      | 784384      |\n","| train/                  |             |\n","|    approx_kl            | 0.016499745 |\n","|    clip_fraction        | 0.14        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.69       |\n","|    explained_variance   | 0.895       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.95        |\n","|    n_updates            | 3820        |\n","|    policy_gradient_loss | -0.0292     |\n","|    value_loss           | 9.78        |\n","-----------------------------------------\n","[INFO] Step 785000: Avg Reward = -5.00\n","[INFO] Step 786000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 384         |\n","|    time_elapsed         | 3443        |\n","|    total_timesteps      | 786432      |\n","| train/                  |             |\n","|    approx_kl            | 0.022538349 |\n","|    clip_fraction        | 0.205       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.77       |\n","|    explained_variance   | 0.914       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.261       |\n","|    n_updates            | 3830        |\n","|    policy_gradient_loss | -0.0351     |\n","|    value_loss           | 5.37        |\n","-----------------------------------------\n","[INFO] Step 787000: Avg Reward = -5.00\n","[INFO] Step 788000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 385         |\n","|    time_elapsed         | 3452        |\n","|    total_timesteps      | 788480      |\n","| train/                  |             |\n","|    approx_kl            | 0.025215473 |\n","|    clip_fraction        | 0.194       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.85       |\n","|    explained_variance   | 0.913       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.68        |\n","|    n_updates            | 3840        |\n","|    policy_gradient_loss | -0.0299     |\n","|    value_loss           | 6.34        |\n","-----------------------------------------\n","[INFO] Step 789000: Avg Reward = -5.00\n","[INFO] Step 790000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 386         |\n","|    time_elapsed         | 3461        |\n","|    total_timesteps      | 790528      |\n","| train/                  |             |\n","|    approx_kl            | 0.017451013 |\n","|    clip_fraction        | 0.156       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.48       |\n","|    explained_variance   | 0.924       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.71        |\n","|    n_updates            | 3850        |\n","|    policy_gradient_loss | -0.0258     |\n","|    value_loss           | 7.59        |\n","-----------------------------------------\n","[INFO] Step 791000: Avg Reward = -5.00\n","[INFO] Step 792000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 387         |\n","|    time_elapsed         | 3471        |\n","|    total_timesteps      | 792576      |\n","| train/                  |             |\n","|    approx_kl            | 0.017564334 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.63       |\n","|    explained_variance   | 0.936       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.07        |\n","|    n_updates            | 3860        |\n","|    policy_gradient_loss | -0.025      |\n","|    value_loss           | 5.88        |\n","-----------------------------------------\n","[INFO] Step 793000: Avg Reward = -5.00\n","[INFO] Step 794000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 388         |\n","|    time_elapsed         | 3479        |\n","|    total_timesteps      | 794624      |\n","| train/                  |             |\n","|    approx_kl            | 0.021624364 |\n","|    clip_fraction        | 0.21        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.9        |\n","|    explained_variance   | 0.949       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.84        |\n","|    n_updates            | 3870        |\n","|    policy_gradient_loss | -0.0347     |\n","|    value_loss           | 7.98        |\n","-----------------------------------------\n","[INFO] Step 795000: Avg Reward = -5.00\n","[INFO] Step 796000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 389         |\n","|    time_elapsed         | 3488        |\n","|    total_timesteps      | 796672      |\n","| train/                  |             |\n","|    approx_kl            | 0.028455174 |\n","|    clip_fraction        | 0.203       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.68       |\n","|    explained_variance   | 0.944       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.19        |\n","|    n_updates            | 3880        |\n","|    policy_gradient_loss | -0.034      |\n","|    value_loss           | 4.77        |\n","-----------------------------------------\n","[INFO] Step 797000: Avg Reward = -5.00\n","[INFO] Step 798000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 390         |\n","|    time_elapsed         | 3498        |\n","|    total_timesteps      | 798720      |\n","| train/                  |             |\n","|    approx_kl            | 0.024073925 |\n","|    clip_fraction        | 0.221       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.84       |\n","|    explained_variance   | 0.926       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.09        |\n","|    n_updates            | 3890        |\n","|    policy_gradient_loss | -0.036      |\n","|    value_loss           | 7.05        |\n","-----------------------------------------\n","[INFO] Step 799000: Avg Reward = -5.00\n","[INFO] Step 800000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 391         |\n","|    time_elapsed         | 3506        |\n","|    total_timesteps      | 800768      |\n","| train/                  |             |\n","|    approx_kl            | 0.013480366 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.38       |\n","|    explained_variance   | 0.918       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.79        |\n","|    n_updates            | 3900        |\n","|    policy_gradient_loss | -0.0223     |\n","|    value_loss           | 8.71        |\n","-----------------------------------------\n","[INFO] Step 801000: Avg Reward = -5.00\n","[INFO] Step 802000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 392         |\n","|    time_elapsed         | 3515        |\n","|    total_timesteps      | 802816      |\n","| train/                  |             |\n","|    approx_kl            | 0.023625897 |\n","|    clip_fraction        | 0.151       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.3        |\n","|    explained_variance   | 0.89        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.93        |\n","|    n_updates            | 3910        |\n","|    policy_gradient_loss | -0.0328     |\n","|    value_loss           | 5.86        |\n","-----------------------------------------\n","[INFO] Step 803000: Avg Reward = -5.00\n","[INFO] Step 804000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 393         |\n","|    time_elapsed         | 3524        |\n","|    total_timesteps      | 804864      |\n","| train/                  |             |\n","|    approx_kl            | 0.011576086 |\n","|    clip_fraction        | 0.129       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.25       |\n","|    explained_variance   | 0.955       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.37        |\n","|    n_updates            | 3920        |\n","|    policy_gradient_loss | -0.026      |\n","|    value_loss           | 8.16        |\n","-----------------------------------------\n","[INFO] Step 805000: Avg Reward = -5.00\n","[INFO] Step 806000: Avg Reward = -5.00\n","---------------------------------------\n","| time/                   |           |\n","|    fps                  | 228       |\n","|    iterations           | 394       |\n","|    time_elapsed         | 3533      |\n","|    total_timesteps      | 806912    |\n","| train/                  |           |\n","|    approx_kl            | 0.0244871 |\n","|    clip_fraction        | 0.175     |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -2.4      |\n","|    explained_variance   | 0.944     |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | 1.67      |\n","|    n_updates            | 3930      |\n","|    policy_gradient_loss | -0.0321   |\n","|    value_loss           | 5.47      |\n","---------------------------------------\n","[INFO] Step 807000: Avg Reward = -5.00\n","[INFO] Step 808000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 395         |\n","|    time_elapsed         | 3543        |\n","|    total_timesteps      | 808960      |\n","| train/                  |             |\n","|    approx_kl            | 0.016655311 |\n","|    clip_fraction        | 0.13        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.36       |\n","|    explained_variance   | 0.918       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.28        |\n","|    n_updates            | 3940        |\n","|    policy_gradient_loss | -0.023      |\n","|    value_loss           | 8.93        |\n","-----------------------------------------\n","[INFO] Step 809000: Avg Reward = -5.00\n","[INFO] Step 810000: Avg Reward = -5.00\n","[INFO] Step 811000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 396         |\n","|    time_elapsed         | 3551        |\n","|    total_timesteps      | 811008      |\n","| train/                  |             |\n","|    approx_kl            | 0.019933064 |\n","|    clip_fraction        | 0.155       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.79       |\n","|    explained_variance   | 0.945       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.77        |\n","|    n_updates            | 3950        |\n","|    policy_gradient_loss | -0.0237     |\n","|    value_loss           | 6.03        |\n","-----------------------------------------\n","[INFO] Step 812000: Avg Reward = -5.00\n","[INFO] Step 813000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 228        |\n","|    iterations           | 397        |\n","|    time_elapsed         | 3560       |\n","|    total_timesteps      | 813056     |\n","| train/                  |            |\n","|    approx_kl            | 0.01873321 |\n","|    clip_fraction        | 0.166      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.43      |\n","|    explained_variance   | 0.932      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 4.47       |\n","|    n_updates            | 3960       |\n","|    policy_gradient_loss | -0.0268    |\n","|    value_loss           | 5.01       |\n","----------------------------------------\n","[INFO] Step 814000: Avg Reward = -5.00\n","[INFO] Step 815000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 398         |\n","|    time_elapsed         | 3570        |\n","|    total_timesteps      | 815104      |\n","| train/                  |             |\n","|    approx_kl            | 0.011814378 |\n","|    clip_fraction        | 0.11        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.97       |\n","|    explained_variance   | 0.923       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.24        |\n","|    n_updates            | 3970        |\n","|    policy_gradient_loss | -0.0285     |\n","|    value_loss           | 6.02        |\n","-----------------------------------------\n","[INFO] Step 816000: Avg Reward = -5.00\n","[INFO] Step 817000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 399         |\n","|    time_elapsed         | 3578        |\n","|    total_timesteps      | 817152      |\n","| train/                  |             |\n","|    approx_kl            | 0.017627306 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.3        |\n","|    explained_variance   | 0.96        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.64        |\n","|    n_updates            | 3980        |\n","|    policy_gradient_loss | -0.0259     |\n","|    value_loss           | 5.52        |\n","-----------------------------------------\n","[INFO] Step 818000: Avg Reward = -5.00\n","[INFO] Step 819000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 400         |\n","|    time_elapsed         | 3588        |\n","|    total_timesteps      | 819200      |\n","| train/                  |             |\n","|    approx_kl            | 0.030584777 |\n","|    clip_fraction        | 0.25        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.47       |\n","|    explained_variance   | 0.972       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.64        |\n","|    n_updates            | 3990        |\n","|    policy_gradient_loss | -0.0336     |\n","|    value_loss           | 4.77        |\n","-----------------------------------------\n","[INFO] Step 820000: Avg Reward = -5.00\n","[INFO] Step 821000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 401         |\n","|    time_elapsed         | 3597        |\n","|    total_timesteps      | 821248      |\n","| train/                  |             |\n","|    approx_kl            | 0.020450056 |\n","|    clip_fraction        | 0.156       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.98       |\n","|    explained_variance   | 0.957       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.09        |\n","|    n_updates            | 4000        |\n","|    policy_gradient_loss | -0.0273     |\n","|    value_loss           | 4.83        |\n","-----------------------------------------\n","[INFO] Step 822000: Avg Reward = -5.00\n","[INFO] Step 823000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 402         |\n","|    time_elapsed         | 3606        |\n","|    total_timesteps      | 823296      |\n","| train/                  |             |\n","|    approx_kl            | 0.016679052 |\n","|    clip_fraction        | 0.156       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.38       |\n","|    explained_variance   | 0.917       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.51        |\n","|    n_updates            | 4010        |\n","|    policy_gradient_loss | -0.03       |\n","|    value_loss           | 7.99        |\n","-----------------------------------------\n","[INFO] Step 824000: Avg Reward = -5.00\n","[INFO] Step 825000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 403         |\n","|    time_elapsed         | 3616        |\n","|    total_timesteps      | 825344      |\n","| train/                  |             |\n","|    approx_kl            | 0.017597977 |\n","|    clip_fraction        | 0.181       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.52       |\n","|    explained_variance   | 0.921       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.01        |\n","|    n_updates            | 4020        |\n","|    policy_gradient_loss | -0.0295     |\n","|    value_loss           | 6.73        |\n","-----------------------------------------\n","[INFO] Step 826000: Avg Reward = -5.00\n","[INFO] Step 827000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 404         |\n","|    time_elapsed         | 3624        |\n","|    total_timesteps      | 827392      |\n","| train/                  |             |\n","|    approx_kl            | 0.012463178 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.44       |\n","|    explained_variance   | 0.94        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.66        |\n","|    n_updates            | 4030        |\n","|    policy_gradient_loss | -0.0334     |\n","|    value_loss           | 6.25        |\n","-----------------------------------------\n","[INFO] Step 828000: Avg Reward = -5.00\n","[INFO] Step 829000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 405         |\n","|    time_elapsed         | 3634        |\n","|    total_timesteps      | 829440      |\n","| train/                  |             |\n","|    approx_kl            | 0.010036287 |\n","|    clip_fraction        | 0.104       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.74       |\n","|    explained_variance   | 0.913       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.07        |\n","|    n_updates            | 4040        |\n","|    policy_gradient_loss | -0.0238     |\n","|    value_loss           | 6.69        |\n","-----------------------------------------\n","[INFO] Step 830000: Avg Reward = -5.00\n","[INFO] Step 831000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 406         |\n","|    time_elapsed         | 3643        |\n","|    total_timesteps      | 831488      |\n","| train/                  |             |\n","|    approx_kl            | 0.022909133 |\n","|    clip_fraction        | 0.171       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.98       |\n","|    explained_variance   | 0.963       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.15        |\n","|    n_updates            | 4050        |\n","|    policy_gradient_loss | -0.0329     |\n","|    value_loss           | 3.24        |\n","-----------------------------------------\n","[INFO] Step 832000: Avg Reward = -5.00\n","[INFO] Step 833000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 407         |\n","|    time_elapsed         | 3651        |\n","|    total_timesteps      | 833536      |\n","| train/                  |             |\n","|    approx_kl            | 0.015542554 |\n","|    clip_fraction        | 0.16        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.47       |\n","|    explained_variance   | 0.953       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.696       |\n","|    n_updates            | 4060        |\n","|    policy_gradient_loss | -0.0411     |\n","|    value_loss           | 4.03        |\n","-----------------------------------------\n","[INFO] Step 834000: Avg Reward = -5.00\n","[INFO] Step 835000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 408         |\n","|    time_elapsed         | 3661        |\n","|    total_timesteps      | 835584      |\n","| train/                  |             |\n","|    approx_kl            | 0.011625541 |\n","|    clip_fraction        | 0.119       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.79       |\n","|    explained_variance   | 0.896       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.41        |\n","|    n_updates            | 4070        |\n","|    policy_gradient_loss | -0.0248     |\n","|    value_loss           | 9.62        |\n","-----------------------------------------\n","[INFO] Step 836000: Avg Reward = -5.00\n","[INFO] Step 837000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 409         |\n","|    time_elapsed         | 3670        |\n","|    total_timesteps      | 837632      |\n","| train/                  |             |\n","|    approx_kl            | 0.027345533 |\n","|    clip_fraction        | 0.209       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.32       |\n","|    explained_variance   | 0.893       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.71        |\n","|    n_updates            | 4080        |\n","|    policy_gradient_loss | -0.0323     |\n","|    value_loss           | 5.59        |\n","-----------------------------------------\n","[INFO] Step 838000: Avg Reward = -5.00\n","[INFO] Step 839000: Avg Reward = 11.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 228        |\n","|    iterations           | 410        |\n","|    time_elapsed         | 3679       |\n","|    total_timesteps      | 839680     |\n","| train/                  |            |\n","|    approx_kl            | 0.01421377 |\n","|    clip_fraction        | 0.144      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.85      |\n","|    explained_variance   | 0.942      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.62       |\n","|    n_updates            | 4090       |\n","|    policy_gradient_loss | -0.0241    |\n","|    value_loss           | 3.8        |\n","----------------------------------------\n","[INFO] Step 840000: Avg Reward = -5.00\n","[INFO] Step 841000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 411         |\n","|    time_elapsed         | 3690        |\n","|    total_timesteps      | 841728      |\n","| train/                  |             |\n","|    approx_kl            | 0.014284564 |\n","|    clip_fraction        | 0.174       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.42       |\n","|    explained_variance   | 0.895       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.08        |\n","|    n_updates            | 4100        |\n","|    policy_gradient_loss | -0.0296     |\n","|    value_loss           | 8.21        |\n","-----------------------------------------\n","[INFO] Step 842000: Avg Reward = -5.00\n","[INFO] Step 843000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 412         |\n","|    time_elapsed         | 3699        |\n","|    total_timesteps      | 843776      |\n","| train/                  |             |\n","|    approx_kl            | 0.014705401 |\n","|    clip_fraction        | 0.131       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.25       |\n","|    explained_variance   | 0.93        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.1         |\n","|    n_updates            | 4110        |\n","|    policy_gradient_loss | -0.0274     |\n","|    value_loss           | 7.61        |\n","-----------------------------------------\n","[INFO] Step 844000: Avg Reward = -5.00\n","[INFO] Step 845000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 413         |\n","|    time_elapsed         | 3708        |\n","|    total_timesteps      | 845824      |\n","| train/                  |             |\n","|    approx_kl            | 0.011681169 |\n","|    clip_fraction        | 0.135       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.24       |\n","|    explained_variance   | 0.946       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.797       |\n","|    n_updates            | 4120        |\n","|    policy_gradient_loss | -0.037      |\n","|    value_loss           | 8.72        |\n","-----------------------------------------\n","[INFO] Step 846000: Avg Reward = -5.00\n","[INFO] Step 847000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 414         |\n","|    time_elapsed         | 3718        |\n","|    total_timesteps      | 847872      |\n","| train/                  |             |\n","|    approx_kl            | 0.010781568 |\n","|    clip_fraction        | 0.142       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.03       |\n","|    explained_variance   | 0.955       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.03        |\n","|    n_updates            | 4130        |\n","|    policy_gradient_loss | -0.0325     |\n","|    value_loss           | 7.71        |\n","-----------------------------------------\n","[INFO] Step 848000: Avg Reward = -5.00\n","[INFO] Step 849000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 415         |\n","|    time_elapsed         | 3726        |\n","|    total_timesteps      | 849920      |\n","| train/                  |             |\n","|    approx_kl            | 0.014335605 |\n","|    clip_fraction        | 0.167       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.28       |\n","|    explained_variance   | 0.953       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.02        |\n","|    n_updates            | 4140        |\n","|    policy_gradient_loss | -0.0283     |\n","|    value_loss           | 7.7         |\n","-----------------------------------------\n","[INFO] Step 850000: Avg Reward = -5.00\n","[INFO] Step 851000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 416         |\n","|    time_elapsed         | 3736        |\n","|    total_timesteps      | 851968      |\n","| train/                  |             |\n","|    approx_kl            | 0.015824992 |\n","|    clip_fraction        | 0.16        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.31       |\n","|    explained_variance   | 0.907       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.75        |\n","|    n_updates            | 4150        |\n","|    policy_gradient_loss | -0.0288     |\n","|    value_loss           | 9.39        |\n","-----------------------------------------\n","[INFO] Step 852000: Avg Reward = -5.00\n","[INFO] Step 853000: Avg Reward = -5.00\n","[INFO] Step 854000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 417         |\n","|    time_elapsed         | 3745        |\n","|    total_timesteps      | 854016      |\n","| train/                  |             |\n","|    approx_kl            | 0.023800086 |\n","|    clip_fraction        | 0.178       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.64       |\n","|    explained_variance   | 0.949       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.92        |\n","|    n_updates            | 4160        |\n","|    policy_gradient_loss | -0.0335     |\n","|    value_loss           | 4.4         |\n","-----------------------------------------\n","[INFO] Step 855000: Avg Reward = -5.00\n","[INFO] Step 856000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 418         |\n","|    time_elapsed         | 3753        |\n","|    total_timesteps      | 856064      |\n","| train/                  |             |\n","|    approx_kl            | 0.023655064 |\n","|    clip_fraction        | 0.187       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.46       |\n","|    explained_variance   | 0.96        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.33        |\n","|    n_updates            | 4170        |\n","|    policy_gradient_loss | -0.0349     |\n","|    value_loss           | 6.22        |\n","-----------------------------------------\n","[INFO] Step 857000: Avg Reward = -5.00\n","[INFO] Step 858000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 228        |\n","|    iterations           | 419        |\n","|    time_elapsed         | 3763       |\n","|    total_timesteps      | 858112     |\n","| train/                  |            |\n","|    approx_kl            | 0.02325962 |\n","|    clip_fraction        | 0.168      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.32      |\n","|    explained_variance   | 0.951      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 2.9        |\n","|    n_updates            | 4180       |\n","|    policy_gradient_loss | -0.0343    |\n","|    value_loss           | 5.38       |\n","----------------------------------------\n","[INFO] Step 859000: Avg Reward = -5.00\n","[INFO] Step 860000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 420         |\n","|    time_elapsed         | 3772        |\n","|    total_timesteps      | 860160      |\n","| train/                  |             |\n","|    approx_kl            | 0.012680001 |\n","|    clip_fraction        | 0.155       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.99       |\n","|    explained_variance   | 0.968       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.7         |\n","|    n_updates            | 4190        |\n","|    policy_gradient_loss | -0.0316     |\n","|    value_loss           | 5.56        |\n","-----------------------------------------\n","[INFO] Step 861000: Avg Reward = -5.00\n","[INFO] Step 862000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 421         |\n","|    time_elapsed         | 3780        |\n","|    total_timesteps      | 862208      |\n","| train/                  |             |\n","|    approx_kl            | 0.021822823 |\n","|    clip_fraction        | 0.173       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.16       |\n","|    explained_variance   | 0.938       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.12        |\n","|    n_updates            | 4200        |\n","|    policy_gradient_loss | -0.0297     |\n","|    value_loss           | 6.47        |\n","-----------------------------------------\n","[INFO] Step 863000: Avg Reward = -5.00\n","[INFO] Step 864000: Avg Reward = 1.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 422         |\n","|    time_elapsed         | 3790        |\n","|    total_timesteps      | 864256      |\n","| train/                  |             |\n","|    approx_kl            | 0.020136245 |\n","|    clip_fraction        | 0.184       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.52       |\n","|    explained_variance   | 0.957       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.64        |\n","|    n_updates            | 4210        |\n","|    policy_gradient_loss | -0.0327     |\n","|    value_loss           | 5.84        |\n","-----------------------------------------\n","[INFO] Step 865000: Avg Reward = -5.00\n","[INFO] Step 866000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 423         |\n","|    time_elapsed         | 3799        |\n","|    total_timesteps      | 866304      |\n","| train/                  |             |\n","|    approx_kl            | 0.018143924 |\n","|    clip_fraction        | 0.156       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.07       |\n","|    explained_variance   | 0.956       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.933       |\n","|    n_updates            | 4220        |\n","|    policy_gradient_loss | -0.0319     |\n","|    value_loss           | 4.64        |\n","-----------------------------------------\n","[INFO] Step 867000: Avg Reward = -5.00\n","[INFO] Step 868000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 424         |\n","|    time_elapsed         | 3808        |\n","|    total_timesteps      | 868352      |\n","| train/                  |             |\n","|    approx_kl            | 0.013706177 |\n","|    clip_fraction        | 0.118       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.83       |\n","|    explained_variance   | 0.951       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.11        |\n","|    n_updates            | 4230        |\n","|    policy_gradient_loss | -0.0267     |\n","|    value_loss           | 5.62        |\n","-----------------------------------------\n","[INFO] Step 869000: Avg Reward = -5.00\n","[INFO] Step 870000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 425         |\n","|    time_elapsed         | 3817        |\n","|    total_timesteps      | 870400      |\n","| train/                  |             |\n","|    approx_kl            | 0.017100384 |\n","|    clip_fraction        | 0.196       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.51       |\n","|    explained_variance   | 0.94        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.77        |\n","|    n_updates            | 4240        |\n","|    policy_gradient_loss | -0.0358     |\n","|    value_loss           | 6.33        |\n","-----------------------------------------\n","[INFO] Step 871000: Avg Reward = -5.00\n","[INFO] Step 872000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 426         |\n","|    time_elapsed         | 3825        |\n","|    total_timesteps      | 872448      |\n","| train/                  |             |\n","|    approx_kl            | 0.019474663 |\n","|    clip_fraction        | 0.167       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.11       |\n","|    explained_variance   | 0.961       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.645       |\n","|    n_updates            | 4250        |\n","|    policy_gradient_loss | -0.0347     |\n","|    value_loss           | 3.31        |\n","-----------------------------------------\n","[INFO] Step 873000: Avg Reward = -5.00\n","[INFO] Step 874000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 427         |\n","|    time_elapsed         | 3835        |\n","|    total_timesteps      | 874496      |\n","| train/                  |             |\n","|    approx_kl            | 0.009004112 |\n","|    clip_fraction        | 0.1         |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.98       |\n","|    explained_variance   | 0.931       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.869       |\n","|    n_updates            | 4260        |\n","|    policy_gradient_loss | -0.0301     |\n","|    value_loss           | 6.37        |\n","-----------------------------------------\n","[INFO] Step 875000: Avg Reward = -5.00\n","[INFO] Step 876000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 428         |\n","|    time_elapsed         | 3845        |\n","|    total_timesteps      | 876544      |\n","| train/                  |             |\n","|    approx_kl            | 0.020171123 |\n","|    clip_fraction        | 0.131       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.71       |\n","|    explained_variance   | 0.974       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.29        |\n","|    n_updates            | 4270        |\n","|    policy_gradient_loss | -0.0291     |\n","|    value_loss           | 3.07        |\n","-----------------------------------------\n","[INFO] Step 877000: Avg Reward = -5.00\n","[INFO] Step 878000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 429         |\n","|    time_elapsed         | 3853        |\n","|    total_timesteps      | 878592      |\n","| train/                  |             |\n","|    approx_kl            | 0.018639758 |\n","|    clip_fraction        | 0.143       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2          |\n","|    explained_variance   | 0.959       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.6         |\n","|    n_updates            | 4280        |\n","|    policy_gradient_loss | -0.0309     |\n","|    value_loss           | 4.36        |\n","-----------------------------------------\n","[INFO] Step 879000: Avg Reward = -5.00\n","[INFO] Step 880000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 430         |\n","|    time_elapsed         | 3863        |\n","|    total_timesteps      | 880640      |\n","| train/                  |             |\n","|    approx_kl            | 0.015555499 |\n","|    clip_fraction        | 0.114       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.72       |\n","|    explained_variance   | 0.973       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2           |\n","|    n_updates            | 4290        |\n","|    policy_gradient_loss | -0.0298     |\n","|    value_loss           | 3.55        |\n","-----------------------------------------\n","[INFO] Step 881000: Avg Reward = -5.00\n","[INFO] Step 882000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 431         |\n","|    time_elapsed         | 3871        |\n","|    total_timesteps      | 882688      |\n","| train/                  |             |\n","|    approx_kl            | 0.022986759 |\n","|    clip_fraction        | 0.197       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.62       |\n","|    explained_variance   | 0.957       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.7         |\n","|    n_updates            | 4300        |\n","|    policy_gradient_loss | -0.0377     |\n","|    value_loss           | 4.95        |\n","-----------------------------------------\n","[INFO] Step 883000: Avg Reward = -5.00\n","[INFO] Step 884000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 432         |\n","|    time_elapsed         | 3880        |\n","|    total_timesteps      | 884736      |\n","| train/                  |             |\n","|    approx_kl            | 0.025868103 |\n","|    clip_fraction        | 0.203       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.52       |\n","|    explained_variance   | 0.941       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.52        |\n","|    n_updates            | 4310        |\n","|    policy_gradient_loss | -0.0347     |\n","|    value_loss           | 6.23        |\n","-----------------------------------------\n","[INFO] Step 885000: Avg Reward = -5.00\n","[INFO] Step 886000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 433         |\n","|    time_elapsed         | 3890        |\n","|    total_timesteps      | 886784      |\n","| train/                  |             |\n","|    approx_kl            | 0.016951123 |\n","|    clip_fraction        | 0.156       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.52       |\n","|    explained_variance   | 0.943       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.61        |\n","|    n_updates            | 4320        |\n","|    policy_gradient_loss | -0.0324     |\n","|    value_loss           | 7.23        |\n","-----------------------------------------\n","[INFO] Step 887000: Avg Reward = -5.00\n","[INFO] Step 888000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 434         |\n","|    time_elapsed         | 3898        |\n","|    total_timesteps      | 888832      |\n","| train/                  |             |\n","|    approx_kl            | 0.024991568 |\n","|    clip_fraction        | 0.207       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.56       |\n","|    explained_variance   | 0.89        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.65        |\n","|    n_updates            | 4330        |\n","|    policy_gradient_loss | -0.0413     |\n","|    value_loss           | 7.72        |\n","-----------------------------------------\n","[INFO] Step 889000: Avg Reward = -5.00\n","[INFO] Step 890000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 435         |\n","|    time_elapsed         | 3908        |\n","|    total_timesteps      | 890880      |\n","| train/                  |             |\n","|    approx_kl            | 0.016190214 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.23       |\n","|    explained_variance   | 0.949       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.26        |\n","|    n_updates            | 4340        |\n","|    policy_gradient_loss | -0.0326     |\n","|    value_loss           | 5.94        |\n","-----------------------------------------\n","[INFO] Step 891000: Avg Reward = -5.00\n","[INFO] Step 892000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 436         |\n","|    time_elapsed         | 3917        |\n","|    total_timesteps      | 892928      |\n","| train/                  |             |\n","|    approx_kl            | 0.025347345 |\n","|    clip_fraction        | 0.23        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.61       |\n","|    explained_variance   | 0.963       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.01        |\n","|    n_updates            | 4350        |\n","|    policy_gradient_loss | -0.0382     |\n","|    value_loss           | 3.78        |\n","-----------------------------------------\n","[INFO] Step 893000: Avg Reward = -5.00\n","[INFO] Step 894000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 437         |\n","|    time_elapsed         | 3926        |\n","|    total_timesteps      | 894976      |\n","| train/                  |             |\n","|    approx_kl            | 0.024151752 |\n","|    clip_fraction        | 0.183       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.66       |\n","|    explained_variance   | 0.952       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.8         |\n","|    n_updates            | 4360        |\n","|    policy_gradient_loss | -0.035      |\n","|    value_loss           | 8.28        |\n","-----------------------------------------\n","[INFO] Step 895000: Avg Reward = -5.00\n","[INFO] Step 896000: Avg Reward = -5.00\n","[INFO] Step 897000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 438         |\n","|    time_elapsed         | 3935        |\n","|    total_timesteps      | 897024      |\n","| train/                  |             |\n","|    approx_kl            | 0.023114353 |\n","|    clip_fraction        | 0.182       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.76       |\n","|    explained_variance   | 0.955       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.26        |\n","|    n_updates            | 4370        |\n","|    policy_gradient_loss | -0.0393     |\n","|    value_loss           | 4.53        |\n","-----------------------------------------\n","[INFO] Step 898000: Avg Reward = -5.00\n","[INFO] Step 899000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 439         |\n","|    time_elapsed         | 3944        |\n","|    total_timesteps      | 899072      |\n","| train/                  |             |\n","|    approx_kl            | 0.012107162 |\n","|    clip_fraction        | 0.108       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.68       |\n","|    explained_variance   | 0.904       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.19        |\n","|    n_updates            | 4380        |\n","|    policy_gradient_loss | -0.0253     |\n","|    value_loss           | 7.04        |\n","-----------------------------------------\n","[INFO] Step 900000: Avg Reward = -5.00\n","[INFO] Step 901000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 440         |\n","|    time_elapsed         | 3953        |\n","|    total_timesteps      | 901120      |\n","| train/                  |             |\n","|    approx_kl            | 0.019509029 |\n","|    clip_fraction        | 0.168       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.23       |\n","|    explained_variance   | 0.952       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.29        |\n","|    n_updates            | 4390        |\n","|    policy_gradient_loss | -0.0358     |\n","|    value_loss           | 5.69        |\n","-----------------------------------------\n","[INFO] Step 902000: Avg Reward = -5.00\n","[INFO] Step 903000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 441         |\n","|    time_elapsed         | 3962        |\n","|    total_timesteps      | 903168      |\n","| train/                  |             |\n","|    approx_kl            | 0.023136538 |\n","|    clip_fraction        | 0.118       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.02       |\n","|    explained_variance   | 0.944       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.648       |\n","|    n_updates            | 4400        |\n","|    policy_gradient_loss | -0.0268     |\n","|    value_loss           | 3.62        |\n","-----------------------------------------\n","[INFO] Step 904000: Avg Reward = -5.00\n","[INFO] Step 905000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 442         |\n","|    time_elapsed         | 3971        |\n","|    total_timesteps      | 905216      |\n","| train/                  |             |\n","|    approx_kl            | 0.014604261 |\n","|    clip_fraction        | 0.139       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.91       |\n","|    explained_variance   | 0.927       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.12        |\n","|    n_updates            | 4410        |\n","|    policy_gradient_loss | -0.031      |\n","|    value_loss           | 6.46        |\n","-----------------------------------------\n","[INFO] Step 906000: Avg Reward = -5.00\n","[INFO] Step 907000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 443         |\n","|    time_elapsed         | 3980        |\n","|    total_timesteps      | 907264      |\n","| train/                  |             |\n","|    approx_kl            | 0.014103152 |\n","|    clip_fraction        | 0.123       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.24       |\n","|    explained_variance   | 0.95        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.53        |\n","|    n_updates            | 4420        |\n","|    policy_gradient_loss | -0.0342     |\n","|    value_loss           | 6.33        |\n","-----------------------------------------\n","[INFO] Step 908000: Avg Reward = -5.00\n","[INFO] Step 909000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 444         |\n","|    time_elapsed         | 3989        |\n","|    total_timesteps      | 909312      |\n","| train/                  |             |\n","|    approx_kl            | 0.015187159 |\n","|    clip_fraction        | 0.138       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.22       |\n","|    explained_variance   | 0.926       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 6.91        |\n","|    n_updates            | 4430        |\n","|    policy_gradient_loss | -0.0316     |\n","|    value_loss           | 7.28        |\n","-----------------------------------------\n","[INFO] Step 910000: Avg Reward = -5.00\n","[INFO] Step 911000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 445         |\n","|    time_elapsed         | 3997        |\n","|    total_timesteps      | 911360      |\n","| train/                  |             |\n","|    approx_kl            | 0.014733366 |\n","|    clip_fraction        | 0.159       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.59       |\n","|    explained_variance   | 0.946       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.07        |\n","|    n_updates            | 4440        |\n","|    policy_gradient_loss | -0.0337     |\n","|    value_loss           | 6.97        |\n","-----------------------------------------\n","[INFO] Step 912000: Avg Reward = -5.00\n","[INFO] Step 913000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 446         |\n","|    time_elapsed         | 4007        |\n","|    total_timesteps      | 913408      |\n","| train/                  |             |\n","|    approx_kl            | 0.014909995 |\n","|    clip_fraction        | 0.116       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.85       |\n","|    explained_variance   | 0.965       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.95        |\n","|    n_updates            | 4450        |\n","|    policy_gradient_loss | -0.0331     |\n","|    value_loss           | 6.16        |\n","-----------------------------------------\n","[INFO] Step 914000: Avg Reward = -5.00\n","[INFO] Step 915000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 447         |\n","|    time_elapsed         | 4016        |\n","|    total_timesteps      | 915456      |\n","| train/                  |             |\n","|    approx_kl            | 0.029050171 |\n","|    clip_fraction        | 0.137       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.94       |\n","|    explained_variance   | 0.936       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.46        |\n","|    n_updates            | 4460        |\n","|    policy_gradient_loss | -0.0304     |\n","|    value_loss           | 5.63        |\n","-----------------------------------------\n","[INFO] Step 916000: Avg Reward = -5.00\n","[INFO] Step 917000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 448         |\n","|    time_elapsed         | 4025        |\n","|    total_timesteps      | 917504      |\n","| train/                  |             |\n","|    approx_kl            | 0.027865125 |\n","|    clip_fraction        | 0.17        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.24       |\n","|    explained_variance   | 0.944       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.01        |\n","|    n_updates            | 4470        |\n","|    policy_gradient_loss | -0.0367     |\n","|    value_loss           | 6.86        |\n","-----------------------------------------\n","[INFO] Step 918000: Avg Reward = -5.00\n","[INFO] Step 919000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 449         |\n","|    time_elapsed         | 4034        |\n","|    total_timesteps      | 919552      |\n","| train/                  |             |\n","|    approx_kl            | 0.021965861 |\n","|    clip_fraction        | 0.165       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.19       |\n","|    explained_variance   | 0.957       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.93        |\n","|    n_updates            | 4480        |\n","|    policy_gradient_loss | -0.0334     |\n","|    value_loss           | 5.67        |\n","-----------------------------------------\n","[INFO] Step 920000: Avg Reward = -5.00\n","[INFO] Step 921000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 450         |\n","|    time_elapsed         | 4042        |\n","|    total_timesteps      | 921600      |\n","| train/                  |             |\n","|    approx_kl            | 0.013842978 |\n","|    clip_fraction        | 0.127       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.93       |\n","|    explained_variance   | 0.93        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.83        |\n","|    n_updates            | 4490        |\n","|    policy_gradient_loss | -0.0236     |\n","|    value_loss           | 5.27        |\n","-----------------------------------------\n","[INFO] Step 922000: Avg Reward = -5.00\n","[INFO] Step 923000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 451         |\n","|    time_elapsed         | 4052        |\n","|    total_timesteps      | 923648      |\n","| train/                  |             |\n","|    approx_kl            | 0.018308844 |\n","|    clip_fraction        | 0.144       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.73       |\n","|    explained_variance   | 0.96        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.75        |\n","|    n_updates            | 4500        |\n","|    policy_gradient_loss | -0.039      |\n","|    value_loss           | 7.72        |\n","-----------------------------------------\n","[INFO] Step 924000: Avg Reward = -5.00\n","[INFO] Step 925000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 452         |\n","|    time_elapsed         | 4061        |\n","|    total_timesteps      | 925696      |\n","| train/                  |             |\n","|    approx_kl            | 0.015433008 |\n","|    clip_fraction        | 0.148       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.93       |\n","|    explained_variance   | 0.895       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 5.37        |\n","|    n_updates            | 4510        |\n","|    policy_gradient_loss | -0.0279     |\n","|    value_loss           | 7.76        |\n","-----------------------------------------\n","[INFO] Step 926000: Avg Reward = -5.00\n","[INFO] Step 927000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 453         |\n","|    time_elapsed         | 4069        |\n","|    total_timesteps      | 927744      |\n","| train/                  |             |\n","|    approx_kl            | 0.012286544 |\n","|    clip_fraction        | 0.13        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.83       |\n","|    explained_variance   | 0.959       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.68        |\n","|    n_updates            | 4520        |\n","|    policy_gradient_loss | -0.0302     |\n","|    value_loss           | 7.18        |\n","-----------------------------------------\n","[INFO] Step 928000: Avg Reward = -5.00\n","[INFO] Step 929000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 454         |\n","|    time_elapsed         | 4079        |\n","|    total_timesteps      | 929792      |\n","| train/                  |             |\n","|    approx_kl            | 0.015209831 |\n","|    clip_fraction        | 0.126       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.81       |\n","|    explained_variance   | 0.951       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.04        |\n","|    n_updates            | 4530        |\n","|    policy_gradient_loss | -0.0301     |\n","|    value_loss           | 6.38        |\n","-----------------------------------------\n","[INFO] Step 930000: Avg Reward = -5.00\n","[INFO] Step 931000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 455         |\n","|    time_elapsed         | 4087        |\n","|    total_timesteps      | 931840      |\n","| train/                  |             |\n","|    approx_kl            | 0.014392629 |\n","|    clip_fraction        | 0.114       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.89       |\n","|    explained_variance   | 0.909       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.97        |\n","|    n_updates            | 4540        |\n","|    policy_gradient_loss | -0.0255     |\n","|    value_loss           | 9.92        |\n","-----------------------------------------\n","[INFO] Step 932000: Avg Reward = -5.00\n","[INFO] Step 933000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 227        |\n","|    iterations           | 456        |\n","|    time_elapsed         | 4096       |\n","|    total_timesteps      | 933888     |\n","| train/                  |            |\n","|    approx_kl            | 0.03168017 |\n","|    clip_fraction        | 0.24       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.02      |\n","|    explained_variance   | 0.931      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.35       |\n","|    n_updates            | 4550       |\n","|    policy_gradient_loss | -0.0453    |\n","|    value_loss           | 6.62       |\n","----------------------------------------\n","[INFO] Step 934000: Avg Reward = -5.00\n","[INFO] Step 935000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 227        |\n","|    iterations           | 457        |\n","|    time_elapsed         | 4106       |\n","|    total_timesteps      | 935936     |\n","| train/                  |            |\n","|    approx_kl            | 0.03478502 |\n","|    clip_fraction        | 0.194      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.96      |\n","|    explained_variance   | 0.964      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.949      |\n","|    n_updates            | 4560       |\n","|    policy_gradient_loss | -0.0331    |\n","|    value_loss           | 2.53       |\n","----------------------------------------\n","[INFO] Step 936000: Avg Reward = -5.00\n","[INFO] Step 937000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 458         |\n","|    time_elapsed         | 4113        |\n","|    total_timesteps      | 937984      |\n","| train/                  |             |\n","|    approx_kl            | 0.024375983 |\n","|    clip_fraction        | 0.161       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.42       |\n","|    explained_variance   | 0.93        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.23        |\n","|    n_updates            | 4570        |\n","|    policy_gradient_loss | -0.0316     |\n","|    value_loss           | 5.74        |\n","-----------------------------------------\n","[INFO] Step 938000: Avg Reward = -5.00\n","[INFO] Step 939000: Avg Reward = -5.00\n","[INFO] Step 940000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 227        |\n","|    iterations           | 459        |\n","|    time_elapsed         | 4123       |\n","|    total_timesteps      | 940032     |\n","| train/                  |            |\n","|    approx_kl            | 0.01913217 |\n","|    clip_fraction        | 0.102      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.66      |\n","|    explained_variance   | 0.948      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.97       |\n","|    n_updates            | 4580       |\n","|    policy_gradient_loss | -0.0226    |\n","|    value_loss           | 4.84       |\n","----------------------------------------\n","[INFO] Step 941000: Avg Reward = -5.00\n","[INFO] Step 942000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 460         |\n","|    time_elapsed         | 4132        |\n","|    total_timesteps      | 942080      |\n","| train/                  |             |\n","|    approx_kl            | 0.017339602 |\n","|    clip_fraction        | 0.121       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.77       |\n","|    explained_variance   | 0.965       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.1         |\n","|    n_updates            | 4590        |\n","|    policy_gradient_loss | -0.0237     |\n","|    value_loss           | 5.09        |\n","-----------------------------------------\n","[INFO] Step 943000: Avg Reward = -5.00\n","[INFO] Step 944000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 461         |\n","|    time_elapsed         | 4140        |\n","|    total_timesteps      | 944128      |\n","| train/                  |             |\n","|    approx_kl            | 0.028312922 |\n","|    clip_fraction        | 0.145       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2          |\n","|    explained_variance   | 0.892       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.81        |\n","|    n_updates            | 4600        |\n","|    policy_gradient_loss | -0.0297     |\n","|    value_loss           | 7.45        |\n","-----------------------------------------\n","[INFO] Step 945000: Avg Reward = -5.00\n","[INFO] Step 946000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 462         |\n","|    time_elapsed         | 4150        |\n","|    total_timesteps      | 946176      |\n","| train/                  |             |\n","|    approx_kl            | 0.012569133 |\n","|    clip_fraction        | 0.113       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.62       |\n","|    explained_variance   | 0.924       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.46        |\n","|    n_updates            | 4610        |\n","|    policy_gradient_loss | -0.0229     |\n","|    value_loss           | 11.6        |\n","-----------------------------------------\n","[INFO] Step 947000: Avg Reward = -5.00\n","[INFO] Step 948000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 463         |\n","|    time_elapsed         | 4158        |\n","|    total_timesteps      | 948224      |\n","| train/                  |             |\n","|    approx_kl            | 0.019092709 |\n","|    clip_fraction        | 0.132       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.5        |\n","|    explained_variance   | 0.97        |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.75        |\n","|    n_updates            | 4620        |\n","|    policy_gradient_loss | -0.0314     |\n","|    value_loss           | 4.02        |\n","-----------------------------------------\n","[INFO] Step 949000: Avg Reward = -5.00\n","[INFO] Step 950000: Avg Reward = -5.00\n","------------------------------------------\n","| time/                   |              |\n","|    fps                  | 228          |\n","|    iterations           | 464          |\n","|    time_elapsed         | 4167         |\n","|    total_timesteps      | 950272       |\n","| train/                  |              |\n","|    approx_kl            | 0.0097382665 |\n","|    clip_fraction        | 0.0957       |\n","|    clip_range           | 0.2          |\n","|    entropy_loss         | -1.76        |\n","|    explained_variance   | 0.925        |\n","|    learning_rate        | 0.0003       |\n","|    loss                 | 1.12         |\n","|    n_updates            | 4630         |\n","|    policy_gradient_loss | -0.0292      |\n","|    value_loss           | 10.3         |\n","------------------------------------------\n","[INFO] Step 951000: Avg Reward = -5.00\n","[INFO] Step 952000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 227         |\n","|    iterations           | 465         |\n","|    time_elapsed         | 4177        |\n","|    total_timesteps      | 952320      |\n","| train/                  |             |\n","|    approx_kl            | 0.032232814 |\n","|    clip_fraction        | 0.119       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.48       |\n","|    explained_variance   | 0.954       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.925       |\n","|    n_updates            | 4640        |\n","|    policy_gradient_loss | -0.0294     |\n","|    value_loss           | 3.54        |\n","-----------------------------------------\n","[INFO] Step 953000: Avg Reward = -5.00\n","[INFO] Step 954000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 466         |\n","|    time_elapsed         | 4185        |\n","|    total_timesteps      | 954368      |\n","| train/                  |             |\n","|    approx_kl            | 0.014437134 |\n","|    clip_fraction        | 0.131       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.99       |\n","|    explained_variance   | 0.962       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.71        |\n","|    n_updates            | 4650        |\n","|    policy_gradient_loss | -0.0322     |\n","|    value_loss           | 5.54        |\n","-----------------------------------------\n","[INFO] Step 955000: Avg Reward = 11.00\n","[INFO] Step 956000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 467         |\n","|    time_elapsed         | 4194        |\n","|    total_timesteps      | 956416      |\n","| train/                  |             |\n","|    approx_kl            | 0.025844928 |\n","|    clip_fraction        | 0.206       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.07       |\n","|    explained_variance   | 0.963       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.26        |\n","|    n_updates            | 4660        |\n","|    policy_gradient_loss | -0.047      |\n","|    value_loss           | 4.78        |\n","-----------------------------------------\n","[INFO] Step 957000: Avg Reward = -5.00\n","[INFO] Step 958000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 468         |\n","|    time_elapsed         | 4203        |\n","|    total_timesteps      | 958464      |\n","| train/                  |             |\n","|    approx_kl            | 0.017939663 |\n","|    clip_fraction        | 0.149       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.87       |\n","|    explained_variance   | 0.951       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.91        |\n","|    n_updates            | 4670        |\n","|    policy_gradient_loss | -0.0317     |\n","|    value_loss           | 4.73        |\n","-----------------------------------------\n","[INFO] Step 959000: Avg Reward = -5.00\n","[INFO] Step 960000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 469         |\n","|    time_elapsed         | 4211        |\n","|    total_timesteps      | 960512      |\n","| train/                  |             |\n","|    approx_kl            | 0.024628736 |\n","|    clip_fraction        | 0.175       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.16       |\n","|    explained_variance   | 0.919       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.1         |\n","|    n_updates            | 4680        |\n","|    policy_gradient_loss | -0.03       |\n","|    value_loss           | 5.44        |\n","-----------------------------------------\n","[INFO] Step 961000: Avg Reward = -5.00\n","[INFO] Step 962000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 228        |\n","|    iterations           | 470        |\n","|    time_elapsed         | 4221       |\n","|    total_timesteps      | 962560     |\n","| train/                  |            |\n","|    approx_kl            | 0.02481661 |\n","|    clip_fraction        | 0.188      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.57      |\n","|    explained_variance   | 0.949      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.31       |\n","|    n_updates            | 4690       |\n","|    policy_gradient_loss | -0.0396    |\n","|    value_loss           | 5.7        |\n","----------------------------------------\n","[INFO] Step 963000: Avg Reward = -5.00\n","[INFO] Step 964000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 471         |\n","|    time_elapsed         | 4229        |\n","|    total_timesteps      | 964608      |\n","| train/                  |             |\n","|    approx_kl            | 0.017483547 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2          |\n","|    explained_variance   | 0.966       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.28        |\n","|    n_updates            | 4700        |\n","|    policy_gradient_loss | -0.0293     |\n","|    value_loss           | 3.87        |\n","-----------------------------------------\n","[INFO] Step 965000: Avg Reward = -5.00\n","[INFO] Step 966000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 472         |\n","|    time_elapsed         | 4238        |\n","|    total_timesteps      | 966656      |\n","| train/                  |             |\n","|    approx_kl            | 0.018197834 |\n","|    clip_fraction        | 0.137       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.04       |\n","|    explained_variance   | 0.904       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.03        |\n","|    n_updates            | 4710        |\n","|    policy_gradient_loss | -0.0285     |\n","|    value_loss           | 6.62        |\n","-----------------------------------------\n","[INFO] Step 967000: Avg Reward = -5.00\n","[INFO] Step 968000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 473         |\n","|    time_elapsed         | 4248        |\n","|    total_timesteps      | 968704      |\n","| train/                  |             |\n","|    approx_kl            | 0.021505335 |\n","|    clip_fraction        | 0.11        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.59       |\n","|    explained_variance   | 0.955       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.22        |\n","|    n_updates            | 4720        |\n","|    policy_gradient_loss | -0.0223     |\n","|    value_loss           | 6.04        |\n","-----------------------------------------\n","[INFO] Step 969000: Avg Reward = -5.00\n","[INFO] Step 970000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 474         |\n","|    time_elapsed         | 4256        |\n","|    total_timesteps      | 970752      |\n","| train/                  |             |\n","|    approx_kl            | 0.012710434 |\n","|    clip_fraction        | 0.135       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.24       |\n","|    explained_variance   | 0.977       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.57        |\n","|    n_updates            | 4730        |\n","|    policy_gradient_loss | -0.034      |\n","|    value_loss           | 6.28        |\n","-----------------------------------------\n","[INFO] Step 971000: Avg Reward = -5.00\n","[INFO] Step 972000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 475         |\n","|    time_elapsed         | 4265        |\n","|    total_timesteps      | 972800      |\n","| train/                  |             |\n","|    approx_kl            | 0.012361862 |\n","|    clip_fraction        | 0.112       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.67       |\n","|    explained_variance   | 0.955       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.87        |\n","|    n_updates            | 4740        |\n","|    policy_gradient_loss | -0.027      |\n","|    value_loss           | 7.1         |\n","-----------------------------------------\n","[INFO] Step 973000: Avg Reward = -5.00\n","[INFO] Step 974000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 476         |\n","|    time_elapsed         | 4274        |\n","|    total_timesteps      | 974848      |\n","| train/                  |             |\n","|    approx_kl            | 0.018032506 |\n","|    clip_fraction        | 0.159       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.38       |\n","|    explained_variance   | 0.952       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.04        |\n","|    n_updates            | 4750        |\n","|    policy_gradient_loss | -0.0389     |\n","|    value_loss           | 5.16        |\n","-----------------------------------------\n","[INFO] Step 975000: Avg Reward = -5.00\n","[INFO] Step 976000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 477         |\n","|    time_elapsed         | 4283        |\n","|    total_timesteps      | 976896      |\n","| train/                  |             |\n","|    approx_kl            | 0.018119719 |\n","|    clip_fraction        | 0.154       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.29       |\n","|    explained_variance   | 0.962       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.96        |\n","|    n_updates            | 4760        |\n","|    policy_gradient_loss | -0.0315     |\n","|    value_loss           | 5.61        |\n","-----------------------------------------\n","[INFO] Step 977000: Avg Reward = -5.00\n","[INFO] Step 978000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 478         |\n","|    time_elapsed         | 4292        |\n","|    total_timesteps      | 978944      |\n","| train/                  |             |\n","|    approx_kl            | 0.013283858 |\n","|    clip_fraction        | 0.15        |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.02       |\n","|    explained_variance   | 0.951       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.5         |\n","|    n_updates            | 4770        |\n","|    policy_gradient_loss | -0.0307     |\n","|    value_loss           | 4.87        |\n","-----------------------------------------\n","[INFO] Step 979000: Avg Reward = -5.00\n","[INFO] Step 980000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 479         |\n","|    time_elapsed         | 4300        |\n","|    total_timesteps      | 980992      |\n","| train/                  |             |\n","|    approx_kl            | 0.023941595 |\n","|    clip_fraction        | 0.216       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.39       |\n","|    explained_variance   | 0.925       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.89        |\n","|    n_updates            | 4780        |\n","|    policy_gradient_loss | -0.0344     |\n","|    value_loss           | 8.02        |\n","-----------------------------------------\n","[INFO] Step 981000: Avg Reward = -5.00\n","[INFO] Step 982000: Avg Reward = 1.00\n","[INFO] Step 983000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 480         |\n","|    time_elapsed         | 4310        |\n","|    total_timesteps      | 983040      |\n","| train/                  |             |\n","|    approx_kl            | 0.023625469 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.16       |\n","|    explained_variance   | 0.899       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.9         |\n","|    n_updates            | 4790        |\n","|    policy_gradient_loss | -0.0324     |\n","|    value_loss           | 9.27        |\n","-----------------------------------------\n","[INFO] Step 984000: Avg Reward = -5.00\n","[INFO] Step 985000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 481         |\n","|    time_elapsed         | 4318        |\n","|    total_timesteps      | 985088      |\n","| train/                  |             |\n","|    approx_kl            | 0.020787839 |\n","|    clip_fraction        | 0.153       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.14       |\n","|    explained_variance   | 0.948       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 1.63        |\n","|    n_updates            | 4800        |\n","|    policy_gradient_loss | -0.0302     |\n","|    value_loss           | 4.23        |\n","-----------------------------------------\n","[INFO] Step 986000: Avg Reward = -5.00\n","[INFO] Step 987000: Avg Reward = -5.00\n","--------------------------------------\n","| time/                   |          |\n","|    fps                  | 228      |\n","|    iterations           | 482      |\n","|    time_elapsed         | 4327     |\n","|    total_timesteps      | 987136   |\n","| train/                  |          |\n","|    approx_kl            | 0.031364 |\n","|    clip_fraction        | 0.182    |\n","|    clip_range           | 0.2      |\n","|    entropy_loss         | -2.61    |\n","|    explained_variance   | 0.959    |\n","|    learning_rate        | 0.0003   |\n","|    loss                 | 0.574    |\n","|    n_updates            | 4810     |\n","|    policy_gradient_loss | -0.0422  |\n","|    value_loss           | 2.87     |\n","--------------------------------------\n","[INFO] Step 988000: Avg Reward = -5.00\n","[INFO] Step 989000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 483         |\n","|    time_elapsed         | 4336        |\n","|    total_timesteps      | 989184      |\n","| train/                  |             |\n","|    approx_kl            | 0.025210805 |\n","|    clip_fraction        | 0.163       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.4        |\n","|    explained_variance   | 0.965       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.458       |\n","|    n_updates            | 4820        |\n","|    policy_gradient_loss | -0.0325     |\n","|    value_loss           | 2.27        |\n","-----------------------------------------\n","[INFO] Step 990000: Avg Reward = -5.00\n","[INFO] Step 991000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 228        |\n","|    iterations           | 484        |\n","|    time_elapsed         | 4344       |\n","|    total_timesteps      | 991232     |\n","| train/                  |            |\n","|    approx_kl            | 0.01591321 |\n","|    clip_fraction        | 0.148      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.89      |\n","|    explained_variance   | 0.829      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 5.08       |\n","|    n_updates            | 4830       |\n","|    policy_gradient_loss | -0.0233    |\n","|    value_loss           | 10.7       |\n","----------------------------------------\n","[INFO] Step 992000: Avg Reward = -5.00\n","[INFO] Step 993000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 485         |\n","|    time_elapsed         | 4353        |\n","|    total_timesteps      | 993280      |\n","| train/                  |             |\n","|    approx_kl            | 0.013966692 |\n","|    clip_fraction        | 0.126       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.98       |\n","|    explained_variance   | 0.932       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 2.9         |\n","|    n_updates            | 4840        |\n","|    policy_gradient_loss | -0.0305     |\n","|    value_loss           | 7.5         |\n","-----------------------------------------\n","[INFO] Step 994000: Avg Reward = -5.00\n","[INFO] Step 995000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 486         |\n","|    time_elapsed         | 4362        |\n","|    total_timesteps      | 995328      |\n","| train/                  |             |\n","|    approx_kl            | 0.015737586 |\n","|    clip_fraction        | 0.161       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -2.19       |\n","|    explained_variance   | 0.913       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 4.14        |\n","|    n_updates            | 4850        |\n","|    policy_gradient_loss | -0.0324     |\n","|    value_loss           | 6.57        |\n","-----------------------------------------\n","[INFO] Step 996000: Avg Reward = -5.00\n","[INFO] Step 997000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 228        |\n","|    iterations           | 487        |\n","|    time_elapsed         | 4370       |\n","|    total_timesteps      | 997376     |\n","| train/                  |            |\n","|    approx_kl            | 0.01900605 |\n","|    clip_fraction        | 0.117      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -1.77      |\n","|    explained_variance   | 0.93       |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 4.52       |\n","|    n_updates            | 4860       |\n","|    policy_gradient_loss | -0.0321    |\n","|    value_loss           | 8.17       |\n","----------------------------------------\n","[INFO] Step 998000: Avg Reward = -5.00\n","[INFO] Step 999000: Avg Reward = -5.00\n","----------------------------------------\n","| time/                   |            |\n","|    fps                  | 228        |\n","|    iterations           | 488        |\n","|    time_elapsed         | 4380       |\n","|    total_timesteps      | 999424     |\n","| train/                  |            |\n","|    approx_kl            | 0.02426353 |\n","|    clip_fraction        | 0.16       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -2.16      |\n","|    explained_variance   | 0.931      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 1.42       |\n","|    n_updates            | 4870       |\n","|    policy_gradient_loss | -0.0338    |\n","|    value_loss           | 5.3        |\n","----------------------------------------\n","[INFO] Step 1000000: Avg Reward = -5.00\n","[INFO] Step 1001000: Avg Reward = -5.00\n","-----------------------------------------\n","| time/                   |             |\n","|    fps                  | 228         |\n","|    iterations           | 489         |\n","|    time_elapsed         | 4387        |\n","|    total_timesteps      | 1001472     |\n","| train/                  |             |\n","|    approx_kl            | 0.012620617 |\n","|    clip_fraction        | 0.126       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -1.65       |\n","|    explained_variance   | 0.943       |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 3.01        |\n","|    n_updates            | 4880        |\n","|    policy_gradient_loss | -0.0285     |\n","|    value_loss           | 4.47        |\n","-----------------------------------------\n","Evaluation Results:\n","Success Rate: 0.00%\n","Average Reward: -3643.30\n"]}],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport gymnasium as gym\nfrom gymnasium import spaces\nfrom stable_baselines3 import PPO\n\n# Step 1: Load and preprocess the dataset\ndata = pd.read_csv('/content/sudoku.csv')\ndata = data.sample(n=1000000, random_state=42)\n\n# Convert puzzles and solutions into numpy arrays\npuzzles = np.array([list(map(int, puzzle)) for puzzle in data['quizzes']])\nsolutions = np.array([list(map(int, solution)) for solution in data['solutions']])\n\n# Step 2: Define the custom Sudoku environment\nclass SudokuEnv(gym.Env):\n    def __init__(self, puzzles, solutions):\n        super(SudokuEnv, self).__init__()\n        self.puzzles = puzzles\n        self.solutions = solutions\n        self.action_space = spaces.Discrete(81 * 9)  # 81 cells × 9 numbers\n        self.observation_space = spaces.Box(\n            low=0, high=9, shape=(9, 9), dtype=np.float32\n        )\n        self.current_puzzle = None\n        self.current_solution = None\n        self.steps = 0\n        self.max_steps = 100\n\n    def _action_to_coords(self, action):\n        \"\"\"Convert a discrete action into row, column, and number.\"\"\"\n        cell = action // 9\n        number = (action % 9) + 1\n        row, col = divmod(cell, 9)\n        return row, col, number\n\n    def reset(self, seed=None):\n        super().reset(seed=seed)\n        idx = np.random.randint(len(self.puzzles))\n        self.current_puzzle = self.puzzles[idx].reshape(9, 9).copy()\n        self.current_solution = self.solutions[idx].reshape(9, 9)\n        self.steps = 0\n        return self.current_puzzle.astype(np.float32), {}\n\n    def step(self, action):\n        row, col, number = self._action_to_coords(action)\n        self.steps += 1\n\n        # Check if the cell is prefilled\n        if self.puzzles[0].reshape(9, 9)[row, col] != 0:\n            return self.current_puzzle.astype(np.float32), -1, True, False, {}\n\n        # Apply the action\n        self.current_puzzle[row, col] = number\n\n        # Calculate reward\n        if self.current_puzzle[row, col] == self.current_solution[row, col]:\n            reward = 5  # Positive reward for correct placement\n        else:\n            reward = -1  # Negative reward for incorrect placement\n\n        # Check if puzzle is solved or max steps are reached\n        done = np.array_equal(self.current_puzzle, self.current_solution)\n        truncated = self.steps >= self.max_steps\n\n        return self.current_puzzle.astype(np.float32), reward, done, truncated, {}\n\n# Step 3: Initialize the environment\nenv = SudokuEnv(puzzles, solutions)\n\n# Step 4: Define the PPO model\npolicy_kwargs = dict(net_arch=[256, 256])  # Neural network architecture\nmodel = PPO(\n    \"MlpPolicy\",\n    env,\n    verbose=1,\n    learning_rate=3e-4,\n    n_steps=2048,\n    batch_size=64,\n    n_epochs=10,\n    gamma=0.99,\n    gae_lambda=0.95,\n    clip_range=0.2,\n    policy_kwargs=policy_kwargs,\n)\n\n# Step 5: Train the model\ntotal_timesteps = 100_000\nmodel.learn(total_timesteps=total_timesteps)\n\n# Step 6: Evaluate the model\ndef evaluate_model(model, env, n_episodes=10):\n    successes = 0\n    total_reward = 0\n\n    for episode in range(n_episodes):\n        obs, _ = env.reset()\n        done = False\n        episode_reward = 0\n\n        while not done:\n            action, _ = model.predict(obs, deterministic=True)\n            obs, reward, done, truncated, _ = env.step(action)\n            episode_reward += reward\n            done = done or truncated\n\n        success = episode_reward > 0\n        successes += success\n        total_reward += episode_reward\n\n        print(f\"Episode {episode + 1}/{n_episodes}: Success = {success}, Reward = {episode_reward}\")\n\n    success_rate = (successes / n_episodes) * 100\n    avg_reward = total_reward / n_episodes\n    return success_rate, avg_reward\n\nsuccess_rate, avg_reward = evaluate_model(model, env)\nprint(f\"Success Rate: {success_rate:.2f}%\")\nprint(f\"Average Reward: {avg_reward:.2f}\")\n\n# Save the model\nmodel.save(\"ppo_sudoku\")\nprint(\"Model saved as 'ppo_sudoku.zip'\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aIUSPhpIkb5J","executionInfo":{"status":"ok","timestamp":1735479531365,"user_tz":-120,"elapsed":684004,"user":{"displayName":"marawan attya (320210295)","userId":"01792541721127694871"}},"outputId":"b796c62a-bb65-4838-8cca-1b0e9f6cf9d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu device\n","Wrapping the env with a `Monitor` wrapper\n","Wrapping the env in a DummyVecEnv.\n","---------------------------------\n","| rollout/           |          |\n","|    ep_len_mean     | 2.38     |\n","|    ep_rew_mean     | -1.6     |\n","| time/              |          |\n","|    fps             | 371      |\n","|    iterations      | 1        |\n","|    time_elapsed    | 5        |\n","|    total_timesteps | 2048     |\n","---------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 2.31       |\n","|    ep_rew_mean          | -1.29      |\n","| time/                   |            |\n","|    fps                  | 224        |\n","|    iterations           | 2          |\n","|    time_elapsed         | 18         |\n","|    total_timesteps      | 4096       |\n","| train/                  |            |\n","|    approx_kl            | 0.05113909 |\n","|    clip_fraction        | 0.722      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -6.59      |\n","|    explained_variance   | -0.0563    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.245      |\n","|    n_updates            | 10         |\n","|    policy_gradient_loss | -0.117     |\n","|    value_loss           | 2.6        |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 2          |\n","|    ep_rew_mean          | -1.22      |\n","| time/                   |            |\n","|    fps                  | 196        |\n","|    iterations           | 3          |\n","|    time_elapsed         | 31         |\n","|    total_timesteps      | 6144       |\n","| train/                  |            |\n","|    approx_kl            | 0.08245779 |\n","|    clip_fraction        | 0.77       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -6.57      |\n","|    explained_variance   | -0.102     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.323      |\n","|    n_updates            | 20         |\n","|    policy_gradient_loss | -0.119     |\n","|    value_loss           | 2.38       |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.85       |\n","|    ep_rew_mean          | -1.31      |\n","| time/                   |            |\n","|    fps                  | 184        |\n","|    iterations           | 4          |\n","|    time_elapsed         | 44         |\n","|    total_timesteps      | 8192       |\n","| train/                  |            |\n","|    approx_kl            | 0.09752311 |\n","|    clip_fraction        | 0.772      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -6.55      |\n","|    explained_variance   | -0.217     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.412      |\n","|    n_updates            | 30         |\n","|    policy_gradient_loss | -0.122     |\n","|    value_loss           | 2          |\n","----------------------------------------\n","-----------------------------------------\n","| rollout/                |             |\n","|    ep_len_mean          | 1.77        |\n","|    ep_rew_mean          | -1.29       |\n","| time/                   |             |\n","|    fps                  | 176         |\n","|    iterations           | 5           |\n","|    time_elapsed         | 58          |\n","|    total_timesteps      | 10240       |\n","| train/                  |             |\n","|    approx_kl            | 0.113614395 |\n","|    clip_fraction        | 0.783       |\n","|    clip_range           | 0.2         |\n","|    entropy_loss         | -6.52       |\n","|    explained_variance   | -0.135      |\n","|    learning_rate        | 0.0003      |\n","|    loss                 | 0.606       |\n","|    n_updates            | 40          |\n","|    policy_gradient_loss | -0.121      |\n","|    value_loss           | 1.9         |\n","-----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.65       |\n","|    ep_rew_mean          | -1.05      |\n","| time/                   |            |\n","|    fps                  | 172        |\n","|    iterations           | 6          |\n","|    time_elapsed         | 71         |\n","|    total_timesteps      | 12288      |\n","| train/                  |            |\n","|    approx_kl            | 0.12990603 |\n","|    clip_fraction        | 0.794      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -6.46      |\n","|    explained_variance   | -0.371     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.278      |\n","|    n_updates            | 50         |\n","|    policy_gradient_loss | -0.119     |\n","|    value_loss           | 1.56       |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.51       |\n","|    ep_rew_mean          | -1.21      |\n","| time/                   |            |\n","|    fps                  | 171        |\n","|    iterations           | 7          |\n","|    time_elapsed         | 83         |\n","|    total_timesteps      | 14336      |\n","| train/                  |            |\n","|    approx_kl            | 0.14246058 |\n","|    clip_fraction        | 0.792      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -6.4       |\n","|    explained_variance   | -0.213     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.319      |\n","|    n_updates            | 60         |\n","|    policy_gradient_loss | -0.116     |\n","|    value_loss           | 1.51       |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.35       |\n","|    ep_rew_mean          | -1.11      |\n","| time/                   |            |\n","|    fps                  | 170        |\n","|    iterations           | 8          |\n","|    time_elapsed         | 95         |\n","|    total_timesteps      | 16384      |\n","| train/                  |            |\n","|    approx_kl            | 0.16553253 |\n","|    clip_fraction        | 0.809      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -6.31      |\n","|    explained_variance   | -0.272     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.191      |\n","|    n_updates            | 70         |\n","|    policy_gradient_loss | -0.12      |\n","|    value_loss           | 1.11       |\n","----------------------------------------\n","---------------------------------------\n","| rollout/                |           |\n","|    ep_len_mean          | 1.41      |\n","|    ep_rew_mean          | -1.17     |\n","| time/                   |           |\n","|    fps                  | 170       |\n","|    iterations           | 9         |\n","|    time_elapsed         | 107       |\n","|    total_timesteps      | 18432     |\n","| train/                  |           |\n","|    approx_kl            | 0.1777163 |\n","|    clip_fraction        | 0.786     |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -6.22     |\n","|    explained_variance   | -0.277    |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | 0.23      |\n","|    n_updates            | 80        |\n","|    policy_gradient_loss | -0.113    |\n","|    value_loss           | 1.1       |\n","---------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.17       |\n","|    ep_rew_mean          | -0.99      |\n","| time/                   |            |\n","|    fps                  | 170        |\n","|    iterations           | 10         |\n","|    time_elapsed         | 120        |\n","|    total_timesteps      | 20480      |\n","| train/                  |            |\n","|    approx_kl            | 0.19435826 |\n","|    clip_fraction        | 0.778      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -6.13      |\n","|    explained_variance   | -0.233     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.192      |\n","|    n_updates            | 90         |\n","|    policy_gradient_loss | -0.106     |\n","|    value_loss           | 0.986      |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.22       |\n","|    ep_rew_mean          | -0.98      |\n","| time/                   |            |\n","|    fps                  | 168        |\n","|    iterations           | 11         |\n","|    time_elapsed         | 133        |\n","|    total_timesteps      | 22528      |\n","| train/                  |            |\n","|    approx_kl            | 0.23234582 |\n","|    clip_fraction        | 0.819      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5.98      |\n","|    explained_variance   | -0.421     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.0712     |\n","|    n_updates            | 100        |\n","|    policy_gradient_loss | -0.112     |\n","|    value_loss           | 0.505      |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.1        |\n","|    ep_rew_mean          | -1.04      |\n","| time/                   |            |\n","|    fps                  | 166        |\n","|    iterations           | 12         |\n","|    time_elapsed         | 147        |\n","|    total_timesteps      | 24576      |\n","| train/                  |            |\n","|    approx_kl            | 0.24928069 |\n","|    clip_fraction        | 0.765      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5.85      |\n","|    explained_variance   | -0.245     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.00294    |\n","|    n_updates            | 110        |\n","|    policy_gradient_loss | -0.102     |\n","|    value_loss           | 0.435      |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.16       |\n","|    ep_rew_mean          | -1.1       |\n","| time/                   |            |\n","|    fps                  | 165        |\n","|    iterations           | 13         |\n","|    time_elapsed         | 160        |\n","|    total_timesteps      | 26624      |\n","| train/                  |            |\n","|    approx_kl            | 0.23721382 |\n","|    clip_fraction        | 0.75       |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5.73      |\n","|    explained_variance   | -0.318     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.112      |\n","|    n_updates            | 120        |\n","|    policy_gradient_loss | -0.102     |\n","|    value_loss           | 0.328      |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.07       |\n","|    ep_rew_mean          | -1.07      |\n","| time/                   |            |\n","|    fps                  | 164        |\n","|    iterations           | 14         |\n","|    time_elapsed         | 174        |\n","|    total_timesteps      | 28672      |\n","| train/                  |            |\n","|    approx_kl            | 0.21003562 |\n","|    clip_fraction        | 0.733      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5.62      |\n","|    explained_variance   | -0.307     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.0777    |\n","|    n_updates            | 130        |\n","|    policy_gradient_loss | -0.0994    |\n","|    value_loss           | 0.268      |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.07       |\n","|    ep_rew_mean          | -0.95      |\n","| time/                   |            |\n","|    fps                  | 163        |\n","|    iterations           | 15         |\n","|    time_elapsed         | 188        |\n","|    total_timesteps      | 30720      |\n","| train/                  |            |\n","|    approx_kl            | 0.21200156 |\n","|    clip_fraction        | 0.742      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5.53      |\n","|    explained_variance   | -0.339     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.121     |\n","|    n_updates            | 140        |\n","|    policy_gradient_loss | -0.103     |\n","|    value_loss           | 0.205      |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.02       |\n","|    ep_rew_mean          | -1.02      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 16         |\n","|    time_elapsed         | 201        |\n","|    total_timesteps      | 32768      |\n","| train/                  |            |\n","|    approx_kl            | 0.20619355 |\n","|    clip_fraction        | 0.735      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5.46      |\n","|    explained_variance   | -0.362     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.0636     |\n","|    n_updates            | 150        |\n","|    policy_gradient_loss | -0.103     |\n","|    value_loss           | 0.146      |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.02       |\n","|    ep_rew_mean          | -0.96      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 17         |\n","|    time_elapsed         | 214        |\n","|    total_timesteps      | 34816      |\n","| train/                  |            |\n","|    approx_kl            | 0.19657788 |\n","|    clip_fraction        | 0.731      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5.41      |\n","|    explained_variance   | -0.468     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.131     |\n","|    n_updates            | 160        |\n","|    policy_gradient_loss | -0.108     |\n","|    value_loss           | 0.099      |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.04       |\n","|    ep_rew_mean          | -1.04      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 18         |\n","|    time_elapsed         | 226        |\n","|    total_timesteps      | 36864      |\n","| train/                  |            |\n","|    approx_kl            | 0.17344648 |\n","|    clip_fraction        | 0.713      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5.32      |\n","|    explained_variance   | -0.338     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.104     |\n","|    n_updates            | 170        |\n","|    policy_gradient_loss | -0.105     |\n","|    value_loss           | 0.0788     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.02       |\n","|    ep_rew_mean          | -1.02      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 19         |\n","|    time_elapsed         | 238        |\n","|    total_timesteps      | 38912      |\n","| train/                  |            |\n","|    approx_kl            | 0.18139213 |\n","|    clip_fraction        | 0.701      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5.27      |\n","|    explained_variance   | -0.563     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.123     |\n","|    n_updates            | 180        |\n","|    policy_gradient_loss | -0.108     |\n","|    value_loss           | 0.0601     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.02       |\n","|    ep_rew_mean          | -1.02      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 20         |\n","|    time_elapsed         | 251        |\n","|    total_timesteps      | 40960      |\n","| train/                  |            |\n","|    approx_kl            | 0.17523602 |\n","|    clip_fraction        | 0.677      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5.23      |\n","|    explained_variance   | -0.353     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.102     |\n","|    n_updates            | 190        |\n","|    policy_gradient_loss | -0.103     |\n","|    value_loss           | 0.0714     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.02       |\n","|    ep_rew_mean          | -1.02      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 21         |\n","|    time_elapsed         | 264        |\n","|    total_timesteps      | 43008      |\n","| train/                  |            |\n","|    approx_kl            | 0.15108128 |\n","|    clip_fraction        | 0.666      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5.19      |\n","|    explained_variance   | -0.204     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.0768     |\n","|    n_updates            | 200        |\n","|    policy_gradient_loss | -0.0972    |\n","|    value_loss           | 0.0715     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.02       |\n","|    ep_rew_mean          | -1.02      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 22         |\n","|    time_elapsed         | 277        |\n","|    total_timesteps      | 45056      |\n","| train/                  |            |\n","|    approx_kl            | 0.15960471 |\n","|    clip_fraction        | 0.683      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5.17      |\n","|    explained_variance   | -0.22      |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.109     |\n","|    n_updates            | 210        |\n","|    policy_gradient_loss | -0.103     |\n","|    value_loss           | 0.0696     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.02       |\n","|    ep_rew_mean          | -0.96      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 23         |\n","|    time_elapsed         | 290        |\n","|    total_timesteps      | 47104      |\n","| train/                  |            |\n","|    approx_kl            | 0.15736865 |\n","|    clip_fraction        | 0.696      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5.09      |\n","|    explained_variance   | -0.379     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.105     |\n","|    n_updates            | 220        |\n","|    policy_gradient_loss | -0.106     |\n","|    value_loss           | 0.0378     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1          |\n","|    ep_rew_mean          | -1         |\n","| time/                   |            |\n","|    fps                  | 161        |\n","|    iterations           | 24         |\n","|    time_elapsed         | 303        |\n","|    total_timesteps      | 49152      |\n","| train/                  |            |\n","|    approx_kl            | 0.14781529 |\n","|    clip_fraction        | 0.675      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5.05      |\n","|    explained_variance   | -0.194     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.148     |\n","|    n_updates            | 230        |\n","|    policy_gradient_loss | -0.1       |\n","|    value_loss           | 0.0357     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.02       |\n","|    ep_rew_mean          | -1.02      |\n","| time/                   |            |\n","|    fps                  | 161        |\n","|    iterations           | 25         |\n","|    time_elapsed         | 316        |\n","|    total_timesteps      | 51200      |\n","| train/                  |            |\n","|    approx_kl            | 0.14005284 |\n","|    clip_fraction        | 0.674      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -5         |\n","|    explained_variance   | -0.187     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.161     |\n","|    n_updates            | 240        |\n","|    policy_gradient_loss | -0.099     |\n","|    value_loss           | 0.0335     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.01       |\n","|    ep_rew_mean          | -1.01      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 26         |\n","|    time_elapsed         | 328        |\n","|    total_timesteps      | 53248      |\n","| train/                  |            |\n","|    approx_kl            | 0.14185414 |\n","|    clip_fraction        | 0.653      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.97      |\n","|    explained_variance   | -0.174     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.149     |\n","|    n_updates            | 250        |\n","|    policy_gradient_loss | -0.0997    |\n","|    value_loss           | 0.031      |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.01       |\n","|    ep_rew_mean          | -1.01      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 27         |\n","|    time_elapsed         | 340        |\n","|    total_timesteps      | 55296      |\n","| train/                  |            |\n","|    approx_kl            | 0.15004532 |\n","|    clip_fraction        | 0.673      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.94      |\n","|    explained_variance   | -0.111     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.104     |\n","|    n_updates            | 260        |\n","|    policy_gradient_loss | -0.101     |\n","|    value_loss           | 0.0321     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1          |\n","|    ep_rew_mean          | -1         |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 28         |\n","|    time_elapsed         | 352        |\n","|    total_timesteps      | 57344      |\n","| train/                  |            |\n","|    approx_kl            | 0.14089733 |\n","|    clip_fraction        | 0.656      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.91      |\n","|    explained_variance   | -0.127     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.153     |\n","|    n_updates            | 270        |\n","|    policy_gradient_loss | -0.101     |\n","|    value_loss           | 0.0301     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.01       |\n","|    ep_rew_mean          | -1.01      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 29         |\n","|    time_elapsed         | 366        |\n","|    total_timesteps      | 59392      |\n","| train/                  |            |\n","|    approx_kl            | 0.15386873 |\n","|    clip_fraction        | 0.689      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.89      |\n","|    explained_variance   | -0.524     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.162     |\n","|    n_updates            | 280        |\n","|    policy_gradient_loss | -0.107     |\n","|    value_loss           | 0.00702    |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1          |\n","|    ep_rew_mean          | -1         |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 30         |\n","|    time_elapsed         | 378        |\n","|    total_timesteps      | 61440      |\n","| train/                  |            |\n","|    approx_kl            | 0.13286583 |\n","|    clip_fraction        | 0.662      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.85      |\n","|    explained_variance   | -0.114     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.154     |\n","|    n_updates            | 290        |\n","|    policy_gradient_loss | -0.095     |\n","|    value_loss           | 0.0187     |\n","----------------------------------------\n","---------------------------------------\n","| rollout/                |           |\n","|    ep_len_mean          | 1         |\n","|    ep_rew_mean          | -1        |\n","| time/                   |           |\n","|    fps                  | 161       |\n","|    iterations           | 31        |\n","|    time_elapsed         | 391       |\n","|    total_timesteps      | 63488     |\n","| train/                  |           |\n","|    approx_kl            | 0.1483184 |\n","|    clip_fraction        | 0.668     |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -4.81     |\n","|    explained_variance   | -0.124    |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | -0.155    |\n","|    n_updates            | 300       |\n","|    policy_gradient_loss | -0.099    |\n","|    value_loss           | 0.0175    |\n","---------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.02       |\n","|    ep_rew_mean          | -1.02      |\n","| time/                   |            |\n","|    fps                  | 161        |\n","|    iterations           | 32         |\n","|    time_elapsed         | 404        |\n","|    total_timesteps      | 65536      |\n","| train/                  |            |\n","|    approx_kl            | 0.13919461 |\n","|    clip_fraction        | 0.642      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.79      |\n","|    explained_variance   | -0.0509    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.0523    |\n","|    n_updates            | 310        |\n","|    policy_gradient_loss | -0.0917    |\n","|    value_loss           | 0.0365     |\n","----------------------------------------\n","---------------------------------------\n","| rollout/                |           |\n","|    ep_len_mean          | 1.01      |\n","|    ep_rew_mean          | -1.01     |\n","| time/                   |           |\n","|    fps                  | 161       |\n","|    iterations           | 33        |\n","|    time_elapsed         | 417       |\n","|    total_timesteps      | 67584     |\n","| train/                  |           |\n","|    approx_kl            | 0.1401361 |\n","|    clip_fraction        | 0.651     |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -4.78     |\n","|    explained_variance   | -0.0816   |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | -0.153    |\n","|    n_updates            | 320       |\n","|    policy_gradient_loss | -0.0876   |\n","|    value_loss           | 0.0328    |\n","---------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.02       |\n","|    ep_rew_mean          | -1.02      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 34         |\n","|    time_elapsed         | 429        |\n","|    total_timesteps      | 69632      |\n","| train/                  |            |\n","|    approx_kl            | 0.15102878 |\n","|    clip_fraction        | 0.686      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.73      |\n","|    explained_variance   | -0.436     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.153     |\n","|    n_updates            | 330        |\n","|    policy_gradient_loss | -0.11      |\n","|    value_loss           | 0.00452    |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1          |\n","|    ep_rew_mean          | -1         |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 35         |\n","|    time_elapsed         | 441        |\n","|    total_timesteps      | 71680      |\n","| train/                  |            |\n","|    approx_kl            | 0.14942698 |\n","|    clip_fraction        | 0.648      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.7       |\n","|    explained_variance   | -0.0328    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.16      |\n","|    n_updates            | 340        |\n","|    policy_gradient_loss | -0.0915    |\n","|    value_loss           | 0.0204     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1          |\n","|    ep_rew_mean          | -1         |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 36         |\n","|    time_elapsed         | 453        |\n","|    total_timesteps      | 73728      |\n","| train/                  |            |\n","|    approx_kl            | 0.14855225 |\n","|    clip_fraction        | 0.658      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.65      |\n","|    explained_variance   | -0.257     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.164     |\n","|    n_updates            | 350        |\n","|    policy_gradient_loss | -0.0996    |\n","|    value_loss           | 0.00571    |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.03       |\n","|    ep_rew_mean          | -1.03      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 37         |\n","|    time_elapsed         | 467        |\n","|    total_timesteps      | 75776      |\n","| train/                  |            |\n","|    approx_kl            | 0.14407033 |\n","|    clip_fraction        | 0.669      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.63      |\n","|    explained_variance   | -0.236     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.0573    |\n","|    n_updates            | 360        |\n","|    policy_gradient_loss | -0.107     |\n","|    value_loss           | 0.00378    |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.01       |\n","|    ep_rew_mean          | -1.01      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 38         |\n","|    time_elapsed         | 479        |\n","|    total_timesteps      | 77824      |\n","| train/                  |            |\n","|    approx_kl            | 0.14320758 |\n","|    clip_fraction        | 0.643      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.61      |\n","|    explained_variance   | -0.0232    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.163     |\n","|    n_updates            | 370        |\n","|    policy_gradient_loss | -0.0894    |\n","|    value_loss           | 0.0172     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1          |\n","|    ep_rew_mean          | -1         |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 39         |\n","|    time_elapsed         | 492        |\n","|    total_timesteps      | 79872      |\n","| train/                  |            |\n","|    approx_kl            | 0.14049828 |\n","|    clip_fraction        | 0.654      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.59      |\n","|    explained_variance   | -0.0144    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.159     |\n","|    n_updates            | 380        |\n","|    policy_gradient_loss | -0.0994    |\n","|    value_loss           | 0.0257     |\n","----------------------------------------\n","---------------------------------------\n","| rollout/                |           |\n","|    ep_len_mean          | 1         |\n","|    ep_rew_mean          | -1        |\n","| time/                   |           |\n","|    fps                  | 162       |\n","|    iterations           | 40        |\n","|    time_elapsed         | 505       |\n","|    total_timesteps      | 81920     |\n","| train/                  |           |\n","|    approx_kl            | 0.1610101 |\n","|    clip_fraction        | 0.684     |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -4.55     |\n","|    explained_variance   | -0.461    |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | -0.158    |\n","|    n_updates            | 390       |\n","|    policy_gradient_loss | -0.107    |\n","|    value_loss           | 0.00361   |\n","---------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1          |\n","|    ep_rew_mean          | -1         |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 41         |\n","|    time_elapsed         | 518        |\n","|    total_timesteps      | 83968      |\n","| train/                  |            |\n","|    approx_kl            | 0.14988293 |\n","|    clip_fraction        | 0.647      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.53      |\n","|    explained_variance   | -0.0195    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.0435    |\n","|    n_updates            | 400        |\n","|    policy_gradient_loss | -0.0969    |\n","|    value_loss           | 0.0155     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1          |\n","|    ep_rew_mean          | -1         |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 42         |\n","|    time_elapsed         | 530        |\n","|    total_timesteps      | 86016      |\n","| train/                  |            |\n","|    approx_kl            | 0.14908244 |\n","|    clip_fraction        | 0.647      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.49      |\n","|    explained_variance   | -0.107     |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.149     |\n","|    n_updates            | 410        |\n","|    policy_gradient_loss | -0.0957    |\n","|    value_loss           | 0.00579    |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1          |\n","|    ep_rew_mean          | -1         |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 43         |\n","|    time_elapsed         | 542        |\n","|    total_timesteps      | 88064      |\n","| train/                  |            |\n","|    approx_kl            | 0.15849112 |\n","|    clip_fraction        | 0.662      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.5       |\n","|    explained_variance   | -0.0145    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | 0.143      |\n","|    n_updates            | 420        |\n","|    policy_gradient_loss | -0.105     |\n","|    value_loss           | 0.026      |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.01       |\n","|    ep_rew_mean          | -1.01      |\n","| time/                   |            |\n","|    fps                  | 162        |\n","|    iterations           | 44         |\n","|    time_elapsed         | 554        |\n","|    total_timesteps      | 90112      |\n","| train/                  |            |\n","|    approx_kl            | 0.15772495 |\n","|    clip_fraction        | 0.673      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.48      |\n","|    explained_variance   | -0.0905    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.151     |\n","|    n_updates            | 430        |\n","|    policy_gradient_loss | -0.105     |\n","|    value_loss           | 0.00371    |\n","----------------------------------------\n","---------------------------------------\n","| rollout/                |           |\n","|    ep_len_mean          | 1         |\n","|    ep_rew_mean          | -1        |\n","| time/                   |           |\n","|    fps                  | 161       |\n","|    iterations           | 45        |\n","|    time_elapsed         | 570       |\n","|    total_timesteps      | 92160     |\n","| train/                  |           |\n","|    approx_kl            | 0.1294833 |\n","|    clip_fraction        | 0.63      |\n","|    clip_range           | 0.2       |\n","|    entropy_loss         | -4.44     |\n","|    explained_variance   | -0.023    |\n","|    learning_rate        | 0.0003    |\n","|    loss                 | -0.142    |\n","|    n_updates            | 440       |\n","|    policy_gradient_loss | -0.0867   |\n","|    value_loss           | 0.018     |\n","---------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1          |\n","|    ep_rew_mean          | -1         |\n","| time/                   |            |\n","|    fps                  | 161        |\n","|    iterations           | 46         |\n","|    time_elapsed         | 583        |\n","|    total_timesteps      | 94208      |\n","| train/                  |            |\n","|    approx_kl            | 0.14898902 |\n","|    clip_fraction        | 0.665      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.41      |\n","|    explained_variance   | -0.0294    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.0488    |\n","|    n_updates            | 450        |\n","|    policy_gradient_loss | -0.104     |\n","|    value_loss           | 0.0143     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.01       |\n","|    ep_rew_mean          | -1.01      |\n","| time/                   |            |\n","|    fps                  | 161        |\n","|    iterations           | 47         |\n","|    time_elapsed         | 596        |\n","|    total_timesteps      | 96256      |\n","| train/                  |            |\n","|    approx_kl            | 0.15610184 |\n","|    clip_fraction        | 0.651      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.4       |\n","|    explained_variance   | -0.00697   |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.129     |\n","|    n_updates            | 460        |\n","|    policy_gradient_loss | -0.0983    |\n","|    value_loss           | 0.0369     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.02       |\n","|    ep_rew_mean          | -1.02      |\n","| time/                   |            |\n","|    fps                  | 161        |\n","|    iterations           | 48         |\n","|    time_elapsed         | 609        |\n","|    total_timesteps      | 98304      |\n","| train/                  |            |\n","|    approx_kl            | 0.16970319 |\n","|    clip_fraction        | 0.664      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.38      |\n","|    explained_variance   | -0.0157    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.156     |\n","|    n_updates            | 470        |\n","|    policy_gradient_loss | -0.102     |\n","|    value_loss           | 0.0255     |\n","----------------------------------------\n","----------------------------------------\n","| rollout/                |            |\n","|    ep_len_mean          | 1.02       |\n","|    ep_rew_mean          | -1.02      |\n","| time/                   |            |\n","|    fps                  | 161        |\n","|    iterations           | 49         |\n","|    time_elapsed         | 622        |\n","|    total_timesteps      | 100352     |\n","| train/                  |            |\n","|    approx_kl            | 0.16649967 |\n","|    clip_fraction        | 0.652      |\n","|    clip_range           | 0.2        |\n","|    entropy_loss         | -4.36      |\n","|    explained_variance   | -0.0412    |\n","|    learning_rate        | 0.0003     |\n","|    loss                 | -0.155     |\n","|    n_updates            | 480        |\n","|    policy_gradient_loss | -0.0945    |\n","|    value_loss           | 0.0288     |\n","----------------------------------------\n","Episode 1/10: Success = False, Reward = -1\n","Episode 2/10: Success = False, Reward = -1\n","Episode 3/10: Success = False, Reward = -1\n","Episode 4/10: Success = False, Reward = -1\n","Episode 5/10: Success = False, Reward = -1\n","Episode 6/10: Success = False, Reward = -1\n","Episode 7/10: Success = False, Reward = -1\n","Episode 8/10: Success = False, Reward = -1\n","Episode 9/10: Success = False, Reward = -1\n","Episode 10/10: Success = False, Reward = -1\n","Success Rate: 0.00%\n","Average Reward: -1.00\n","Model saved as 'ppo_sudoku.zip'\n"]}],"execution_count":null}]}